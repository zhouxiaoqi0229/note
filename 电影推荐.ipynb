{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U9nfyusqdnVjNC3BtvK3a2MWxWd4MciR",
      "authorship_tag": "ABX9TyNsTkdmr/4t1BQbt+qYbaqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouxiaoqi0229/note/blob/main/%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYp8dcOWp9X5",
        "outputId": "c4a0f9f0-b813-4b60-ab12-d4f265269dbc"
      },
      "source": [
        " !pwd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RUGxoDZqECn"
      },
      "source": [
        "import os\n",
        "os.chdir('./drive/MyDrive/Colab Notebooks/movie_recommender')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjJHmPTq6_w"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split#数据集划分\n",
        "import numpy as np\n",
        "from collections import Counter#用于统计字符出现的次数\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle #提供保存数据在本地的方法\n",
        "import re #正则表达式\n",
        "from tensorflow.python.ops import math_ops \n",
        "from urllib.request import urlretrieve#将url表示的网络对象复制到本地\n",
        "from os.path import isfile,isdir#判断是否存在文件file 文件夹dir\n",
        "from tqdm import tqdm  #Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)\n",
        "import zipfile #用来做zip格式的压缩与解压缩\n",
        "import hashlib #hash 或者MD5加密\n",
        "pd.set_option('display.max_rows',None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "N5dSKsHxhAJ6",
        "outputId": "d6dcdb49-6a47-4ac8-b60f-427499914d6b"
      },
      "source": [
        "#数据查看\n",
        "\n",
        "#用户数据\n",
        "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
        "users = pd.read_csv('./users.dat',sep='::', header=None, names=users_title, engine = 'python')\n",
        "users.head(5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>OccupationID</th>\n",
              "      <th>Zip-code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>M</td>\n",
              "      <td>56</td>\n",
              "      <td>16</td>\n",
              "      <td>70072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>M</td>\n",
              "      <td>25</td>\n",
              "      <td>15</td>\n",
              "      <td>55117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>M</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>02460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>M</td>\n",
              "      <td>25</td>\n",
              "      <td>20</td>\n",
              "      <td>55455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID Gender  Age  OccupationID Zip-code\n",
              "0       1      F    1            10    48067\n",
              "1       2      M   56            16    70072\n",
              "2       3      M   25            15    55117\n",
              "3       4      M   45             7    02460\n",
              "4       5      M   25            20    55455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehhfojiqobYl",
        "outputId": "f03e5e29-1e6e-4ff9-9dbc-5afb635e9250"
      },
      "source": [
        "#电影数据\n",
        "movies_title = ['MovieID', 'Title', 'Genres']\n",
        "movies = pd.read_csv('./movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
        "movies.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 'Toy Story (1995)', \"Animation|Children's|Comedy\"],\n",
              "       [2, 'Jumanji (1995)', \"Adventure|Children's|Fantasy\"],\n",
              "       [3, 'Grumpier Old Men (1995)', 'Comedy|Romance'],\n",
              "       ...,\n",
              "       [3950, 'Tigerland (2000)', 'Drama'],\n",
              "       [3951, 'Two Family House (2000)', 'Drama'],\n",
              "       [3952, 'Contender, The (2000)', 'Drama|Thriller']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X7wrcMzQooJU",
        "outputId": "c7144d3a-46e9-45fa-e370-1c75eaa9a7c7"
      },
      "source": [
        "#评分数据\n",
        "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
        "ratings = pd.read_csv('./ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>timestamps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  MovieID  Rating  timestamps\n",
              "0       1     1193       5   978300760\n",
              "1       1      661       3   978302109\n",
              "2       1      914       3   978301968\n",
              "3       1     3408       4   978300275\n",
              "4       1     2355       5   978824291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QvfhpFNo2Ss"
      },
      "source": [
        "**处理数据**\n",
        "\n",
        "\n",
        "*   userID movieId occuption不变\n",
        "*   性别 0,1 \n",
        "\n",
        "*   年龄 0-6\n",
        "*   类别;是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
        "\n",
        "\n",
        "*   Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉\n",
        "*   Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlo4z-bAoyHc"
      },
      "source": [
        "#数据预处理\n",
        "\n",
        "def load_data():\n",
        "  #修改user数据集,不要邮编\n",
        "  users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
        "  users = pd.read_csv('./users.dat',sep='::', header=None, names=users_title, engine = 'python')\n",
        "  users = users.filter(regex='UserID|Gender|Age|JobID')\n",
        "  users_orig = users.values\n",
        "  users['Gender'] = users['Gender'].map({'F':0, 'M':1})#性别映射\n",
        "  age_map = {val:ii for ii, val in enumerate(set(users['Age']))}\n",
        "  users['Age'] = users['Age'].map(age_map)#年龄映射为0-6\n",
        "  \n",
        "  #修改movie数据集\n",
        "  movies_title = ['MovieID', 'Title', 'Genres']\n",
        "  movies = pd.read_csv('./movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
        "  movies_orig = movies.values\n",
        "\n",
        "  #去掉标题中的年份\n",
        "  pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
        "  title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
        "  movies['Title'] = movies['Title'].map(title_map)\n",
        "\n",
        "  #电影类型转换为数字字典\n",
        "  genres_set = set()\n",
        "  for val in movies['Genres'].str.split('|'):\n",
        "    genres_set.update(val)\n",
        "  genres_set.add('<PAD>')\n",
        "\n",
        "  genres2int = {val:ii for ii, val in enumerate(genres_set)} #每种类型到一个数字的映射\n",
        "  #eg: a|b|c:1|2|3这种类型的一个映射\n",
        "  genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))} \n",
        "  #将类型补齐\n",
        "  for key in genres_map:\n",
        "    for cnt in range(max(genres2int.values())-len(genres_map[key])):\n",
        "      genres_map[key].insert(len(genres_map[key])+cnt,genres2int['<PAD>'])\n",
        "  movies['Genres'] = movies['Genres'].map(genres_map)\n",
        "  movies.head(10)\n",
        "\n",
        "  #电影Title转数字字典\n",
        "  title_set = set()#所有电影名称涉及到的单词\n",
        "  for val in movies['Title'].str.split():\n",
        "    title_set.update(val)\n",
        "  title_set.add('<PAD>')\n",
        "\n",
        "  title_count = 15\n",
        "  title2int = {val:ii for ii, val in enumerate(title_set)}#每个单词映射成一个数字\n",
        "  title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
        "  for key in title_map:\n",
        "    for cnt in range(title_count - len(title_map[key])):\n",
        "      title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
        "    \n",
        "  movies['Title'] = movies['Title'].map(title_map)\n",
        "\n",
        "  #读取评分数据集\n",
        "  ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
        "  ratings = pd.read_csv('./ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "  ratings = ratings.filter(regex='UserID|MovieID|ratings')#不要时间戳\n",
        "\n",
        "  #合并三个表\n",
        "  data = pd.merge(pd.merge(ratings, users), movies)#1000209 rows × 8 columns\n",
        "  \n",
        "  #分成两张表\n",
        "  target_fields = ['ratings']\n",
        "\n",
        "  features_pd,targets_pd = data.drop(target_fields,axis=1),data[target_fields]# 一个表没有评分 另一个表只有评分\n",
        "  features = features_pd.values #将dataframe转变为array\n",
        "  targets_values = targets_pd.values\n",
        "\n",
        "  return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZbf4uicMTDK"
      },
      "source": [
        "\n",
        "\n",
        "*   title_count:title字段的长度（15）\n",
        "\n",
        "*   title_set：title文本的集合\n",
        "\n",
        "*   genres2int：电影类型转数字的字典\n",
        "\n",
        "*   features：是输入X\n",
        "*   ratings：评分数据集的Pandas对象\n",
        "\n",
        "\n",
        "*   movies_orig：没有做数据处理的原始电影数据\n",
        "\n",
        "\n",
        "*   users_orig：没有做数据处理的原始用户数据\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsFFMB9yI6HI",
        "outputId": "67cfd4f4-9af4-4705-a020-65ce0ff71178"
      },
      "source": [
        "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
        "\n",
        "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))\n",
        "genres2int"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<PAD>': 8,\n",
              " 'Action': 4,\n",
              " 'Adventure': 6,\n",
              " 'Animation': 14,\n",
              " \"Children's\": 3,\n",
              " 'Comedy': 17,\n",
              " 'Crime': 15,\n",
              " 'Documentary': 12,\n",
              " 'Drama': 18,\n",
              " 'Fantasy': 7,\n",
              " 'Film-Noir': 1,\n",
              " 'Horror': 0,\n",
              " 'Musical': 13,\n",
              " 'Mystery': 11,\n",
              " 'Romance': 16,\n",
              " 'Sci-Fi': 9,\n",
              " 'Thriller': 2,\n",
              " 'War': 10,\n",
              " 'Western': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aajMPrEAPHFC"
      },
      "source": [
        "#辅助函数\n",
        "def save_params(params):\n",
        "  '''\n",
        "  save params to filr\n",
        "  '''\n",
        "  pickle.dump(params,open('params.p','wb'))\n",
        "def load_params():\n",
        "  \"\"\"\n",
        "  Load parameters from file\n",
        "  \"\"\"\n",
        "  return pickle.load(open('params.p', mode='rb'))\n",
        "\n",
        "    \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ3TZ9yLcZnP"
      },
      "source": [
        "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))\n",
        "movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxHFKIWeSNTk",
        "outputId": "f434f6d3-c2eb-4af4-d435-9e4d918d08fb"
      },
      "source": [
        "import numpy as np\n",
        "student = np.array([('name','S20'), ('age', 'i1'), ('marks', 'f4')]) \n",
        "student.take(1,0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['age', 'i1'], dtype='<U5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2wXPJNZQDJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedbf500-2f36-489d-e7b6-ce3fb4bdd1b2"
      },
      "source": [
        "#嵌入矩阵的维度\n",
        "embed_dim = 32\n",
        "#用户ID个数\n",
        "uid_max = max(features.take(0,1)) + 1 # 6040+1\n",
        "#性别个数\n",
        "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
        "#年龄类别个数\n",
        "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
        "#职业个数\n",
        "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
        "\n",
        "#电影ID个数\n",
        "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
        "#电影类型个数\n",
        "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
        "#电影名单词个数\n",
        "movie_title_max = len(title_set) # 5216\n",
        "#电影名长度\n",
        "sentences_size = title_count # = 15\n",
        "\n",
        "#对电影类型嵌入向量做加和操作的标志\n",
        "combiner = \"sum\"\n",
        "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
        "window_sizes = {2, 3, 4, 5}\n",
        "#文本卷积核数量\n",
        "filter_num = 8\n",
        "\n",
        "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5,id:下标\n",
        "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}\n",
        "movieid2idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0,\n",
              " 2: 1,\n",
              " 3: 2,\n",
              " 4: 3,\n",
              " 5: 4,\n",
              " 6: 5,\n",
              " 7: 6,\n",
              " 8: 7,\n",
              " 9: 8,\n",
              " 10: 9,\n",
              " 11: 10,\n",
              " 12: 11,\n",
              " 13: 12,\n",
              " 14: 13,\n",
              " 15: 14,\n",
              " 16: 15,\n",
              " 17: 16,\n",
              " 18: 17,\n",
              " 19: 18,\n",
              " 20: 19,\n",
              " 21: 20,\n",
              " 22: 21,\n",
              " 23: 22,\n",
              " 24: 23,\n",
              " 25: 24,\n",
              " 26: 25,\n",
              " 27: 26,\n",
              " 28: 27,\n",
              " 29: 28,\n",
              " 30: 29,\n",
              " 31: 30,\n",
              " 32: 31,\n",
              " 33: 32,\n",
              " 34: 33,\n",
              " 35: 34,\n",
              " 36: 35,\n",
              " 37: 36,\n",
              " 38: 37,\n",
              " 39: 38,\n",
              " 40: 39,\n",
              " 41: 40,\n",
              " 42: 41,\n",
              " 43: 42,\n",
              " 44: 43,\n",
              " 45: 44,\n",
              " 46: 45,\n",
              " 47: 46,\n",
              " 48: 47,\n",
              " 49: 48,\n",
              " 50: 49,\n",
              " 51: 50,\n",
              " 52: 51,\n",
              " 53: 52,\n",
              " 54: 53,\n",
              " 55: 54,\n",
              " 56: 55,\n",
              " 57: 56,\n",
              " 58: 57,\n",
              " 59: 58,\n",
              " 60: 59,\n",
              " 61: 60,\n",
              " 62: 61,\n",
              " 63: 62,\n",
              " 64: 63,\n",
              " 65: 64,\n",
              " 66: 65,\n",
              " 67: 66,\n",
              " 68: 67,\n",
              " 69: 68,\n",
              " 70: 69,\n",
              " 71: 70,\n",
              " 72: 71,\n",
              " 73: 72,\n",
              " 74: 73,\n",
              " 75: 74,\n",
              " 76: 75,\n",
              " 77: 76,\n",
              " 78: 77,\n",
              " 79: 78,\n",
              " 80: 79,\n",
              " 81: 80,\n",
              " 82: 81,\n",
              " 83: 82,\n",
              " 84: 83,\n",
              " 85: 84,\n",
              " 86: 85,\n",
              " 87: 86,\n",
              " 88: 87,\n",
              " 89: 88,\n",
              " 90: 89,\n",
              " 92: 90,\n",
              " 93: 91,\n",
              " 94: 92,\n",
              " 95: 93,\n",
              " 96: 94,\n",
              " 97: 95,\n",
              " 98: 96,\n",
              " 99: 97,\n",
              " 100: 98,\n",
              " 101: 99,\n",
              " 102: 100,\n",
              " 103: 101,\n",
              " 104: 102,\n",
              " 105: 103,\n",
              " 106: 104,\n",
              " 107: 105,\n",
              " 108: 106,\n",
              " 109: 107,\n",
              " 110: 108,\n",
              " 111: 109,\n",
              " 112: 110,\n",
              " 113: 111,\n",
              " 114: 112,\n",
              " 115: 113,\n",
              " 116: 114,\n",
              " 117: 115,\n",
              " 118: 116,\n",
              " 119: 117,\n",
              " 120: 118,\n",
              " 121: 119,\n",
              " 122: 120,\n",
              " 123: 121,\n",
              " 124: 122,\n",
              " 125: 123,\n",
              " 126: 124,\n",
              " 127: 125,\n",
              " 128: 126,\n",
              " 129: 127,\n",
              " 130: 128,\n",
              " 131: 129,\n",
              " 132: 130,\n",
              " 133: 131,\n",
              " 134: 132,\n",
              " 135: 133,\n",
              " 136: 134,\n",
              " 137: 135,\n",
              " 138: 136,\n",
              " 139: 137,\n",
              " 140: 138,\n",
              " 141: 139,\n",
              " 142: 140,\n",
              " 143: 141,\n",
              " 144: 142,\n",
              " 145: 143,\n",
              " 146: 144,\n",
              " 147: 145,\n",
              " 148: 146,\n",
              " 149: 147,\n",
              " 150: 148,\n",
              " 151: 149,\n",
              " 152: 150,\n",
              " 153: 151,\n",
              " 154: 152,\n",
              " 155: 153,\n",
              " 156: 154,\n",
              " 157: 155,\n",
              " 158: 156,\n",
              " 159: 157,\n",
              " 160: 158,\n",
              " 161: 159,\n",
              " 162: 160,\n",
              " 163: 161,\n",
              " 164: 162,\n",
              " 165: 163,\n",
              " 166: 164,\n",
              " 167: 165,\n",
              " 168: 166,\n",
              " 169: 167,\n",
              " 170: 168,\n",
              " 171: 169,\n",
              " 172: 170,\n",
              " 173: 171,\n",
              " 174: 172,\n",
              " 175: 173,\n",
              " 176: 174,\n",
              " 177: 175,\n",
              " 178: 176,\n",
              " 179: 177,\n",
              " 180: 178,\n",
              " 181: 179,\n",
              " 182: 180,\n",
              " 183: 181,\n",
              " 184: 182,\n",
              " 185: 183,\n",
              " 186: 184,\n",
              " 187: 185,\n",
              " 188: 186,\n",
              " 189: 187,\n",
              " 190: 188,\n",
              " 191: 189,\n",
              " 192: 190,\n",
              " 193: 191,\n",
              " 194: 192,\n",
              " 195: 193,\n",
              " 196: 194,\n",
              " 197: 195,\n",
              " 198: 196,\n",
              " 199: 197,\n",
              " 200: 198,\n",
              " 201: 199,\n",
              " 202: 200,\n",
              " 203: 201,\n",
              " 204: 202,\n",
              " 205: 203,\n",
              " 206: 204,\n",
              " 207: 205,\n",
              " 208: 206,\n",
              " 209: 207,\n",
              " 210: 208,\n",
              " 211: 209,\n",
              " 212: 210,\n",
              " 213: 211,\n",
              " 214: 212,\n",
              " 215: 213,\n",
              " 216: 214,\n",
              " 217: 215,\n",
              " 218: 216,\n",
              " 219: 217,\n",
              " 220: 218,\n",
              " 222: 219,\n",
              " 223: 220,\n",
              " 224: 221,\n",
              " 225: 222,\n",
              " 226: 223,\n",
              " 227: 224,\n",
              " 228: 225,\n",
              " 229: 226,\n",
              " 230: 227,\n",
              " 231: 228,\n",
              " 232: 229,\n",
              " 233: 230,\n",
              " 234: 231,\n",
              " 235: 232,\n",
              " 236: 233,\n",
              " 237: 234,\n",
              " 238: 235,\n",
              " 239: 236,\n",
              " 240: 237,\n",
              " 241: 238,\n",
              " 242: 239,\n",
              " 243: 240,\n",
              " 244: 241,\n",
              " 245: 242,\n",
              " 246: 243,\n",
              " 247: 244,\n",
              " 248: 245,\n",
              " 249: 246,\n",
              " 250: 247,\n",
              " 251: 248,\n",
              " 252: 249,\n",
              " 253: 250,\n",
              " 254: 251,\n",
              " 255: 252,\n",
              " 256: 253,\n",
              " 257: 254,\n",
              " 258: 255,\n",
              " 259: 256,\n",
              " 260: 257,\n",
              " 261: 258,\n",
              " 262: 259,\n",
              " 263: 260,\n",
              " 264: 261,\n",
              " 265: 262,\n",
              " 266: 263,\n",
              " 267: 264,\n",
              " 268: 265,\n",
              " 269: 266,\n",
              " 270: 267,\n",
              " 271: 268,\n",
              " 272: 269,\n",
              " 273: 270,\n",
              " 274: 271,\n",
              " 275: 272,\n",
              " 276: 273,\n",
              " 277: 274,\n",
              " 278: 275,\n",
              " 279: 276,\n",
              " 280: 277,\n",
              " 281: 278,\n",
              " 282: 279,\n",
              " 283: 280,\n",
              " 284: 281,\n",
              " 285: 282,\n",
              " 286: 283,\n",
              " 287: 284,\n",
              " 288: 285,\n",
              " 289: 286,\n",
              " 290: 287,\n",
              " 291: 288,\n",
              " 292: 289,\n",
              " 293: 290,\n",
              " 294: 291,\n",
              " 295: 292,\n",
              " 296: 293,\n",
              " 297: 294,\n",
              " 298: 295,\n",
              " 299: 296,\n",
              " 300: 297,\n",
              " 301: 298,\n",
              " 302: 299,\n",
              " 303: 300,\n",
              " 304: 301,\n",
              " 305: 302,\n",
              " 306: 303,\n",
              " 307: 304,\n",
              " 308: 305,\n",
              " 309: 306,\n",
              " 310: 307,\n",
              " 311: 308,\n",
              " 312: 309,\n",
              " 313: 310,\n",
              " 314: 311,\n",
              " 315: 312,\n",
              " 316: 313,\n",
              " 317: 314,\n",
              " 318: 315,\n",
              " 319: 316,\n",
              " 320: 317,\n",
              " 321: 318,\n",
              " 322: 319,\n",
              " 324: 320,\n",
              " 325: 321,\n",
              " 326: 322,\n",
              " 327: 323,\n",
              " 328: 324,\n",
              " 329: 325,\n",
              " 330: 326,\n",
              " 331: 327,\n",
              " 332: 328,\n",
              " 333: 329,\n",
              " 334: 330,\n",
              " 335: 331,\n",
              " 336: 332,\n",
              " 337: 333,\n",
              " 338: 334,\n",
              " 339: 335,\n",
              " 340: 336,\n",
              " 341: 337,\n",
              " 342: 338,\n",
              " 343: 339,\n",
              " 344: 340,\n",
              " 345: 341,\n",
              " 346: 342,\n",
              " 347: 343,\n",
              " 348: 344,\n",
              " 349: 345,\n",
              " 350: 346,\n",
              " 351: 347,\n",
              " 352: 348,\n",
              " 353: 349,\n",
              " 354: 350,\n",
              " 355: 351,\n",
              " 356: 352,\n",
              " 357: 353,\n",
              " 358: 354,\n",
              " 359: 355,\n",
              " 360: 356,\n",
              " 361: 357,\n",
              " 362: 358,\n",
              " 363: 359,\n",
              " 364: 360,\n",
              " 365: 361,\n",
              " 366: 362,\n",
              " 367: 363,\n",
              " 368: 364,\n",
              " 369: 365,\n",
              " 370: 366,\n",
              " 371: 367,\n",
              " 372: 368,\n",
              " 373: 369,\n",
              " 374: 370,\n",
              " 375: 371,\n",
              " 376: 372,\n",
              " 377: 373,\n",
              " 378: 374,\n",
              " 379: 375,\n",
              " 380: 376,\n",
              " 381: 377,\n",
              " 382: 378,\n",
              " 383: 379,\n",
              " 384: 380,\n",
              " 385: 381,\n",
              " 386: 382,\n",
              " 387: 383,\n",
              " 388: 384,\n",
              " 389: 385,\n",
              " 390: 386,\n",
              " 391: 387,\n",
              " 392: 388,\n",
              " 393: 389,\n",
              " 394: 390,\n",
              " 395: 391,\n",
              " 396: 392,\n",
              " 397: 393,\n",
              " 398: 394,\n",
              " 399: 395,\n",
              " 400: 396,\n",
              " 401: 397,\n",
              " 402: 398,\n",
              " 403: 399,\n",
              " 404: 400,\n",
              " 405: 401,\n",
              " 406: 402,\n",
              " 407: 403,\n",
              " 408: 404,\n",
              " 409: 405,\n",
              " 410: 406,\n",
              " 411: 407,\n",
              " 412: 408,\n",
              " 413: 409,\n",
              " 414: 410,\n",
              " 415: 411,\n",
              " 416: 412,\n",
              " 417: 413,\n",
              " 418: 414,\n",
              " 419: 415,\n",
              " 420: 416,\n",
              " 421: 417,\n",
              " 422: 418,\n",
              " 423: 419,\n",
              " 424: 420,\n",
              " 425: 421,\n",
              " 426: 422,\n",
              " 427: 423,\n",
              " 428: 424,\n",
              " 429: 425,\n",
              " 430: 426,\n",
              " 431: 427,\n",
              " 432: 428,\n",
              " 433: 429,\n",
              " 434: 430,\n",
              " 435: 431,\n",
              " 436: 432,\n",
              " 437: 433,\n",
              " 438: 434,\n",
              " 439: 435,\n",
              " 440: 436,\n",
              " 441: 437,\n",
              " 442: 438,\n",
              " 443: 439,\n",
              " 444: 440,\n",
              " 445: 441,\n",
              " 446: 442,\n",
              " 447: 443,\n",
              " 448: 444,\n",
              " 449: 445,\n",
              " 450: 446,\n",
              " 451: 447,\n",
              " 452: 448,\n",
              " 453: 449,\n",
              " 454: 450,\n",
              " 455: 451,\n",
              " 456: 452,\n",
              " 457: 453,\n",
              " 458: 454,\n",
              " 459: 455,\n",
              " 460: 456,\n",
              " 461: 457,\n",
              " 462: 458,\n",
              " 463: 459,\n",
              " 464: 460,\n",
              " 465: 461,\n",
              " 466: 462,\n",
              " 467: 463,\n",
              " 468: 464,\n",
              " 469: 465,\n",
              " 470: 466,\n",
              " 471: 467,\n",
              " 472: 468,\n",
              " 473: 469,\n",
              " 474: 470,\n",
              " 475: 471,\n",
              " 476: 472,\n",
              " 477: 473,\n",
              " 478: 474,\n",
              " 479: 475,\n",
              " 480: 476,\n",
              " 481: 477,\n",
              " 482: 478,\n",
              " 483: 479,\n",
              " 484: 480,\n",
              " 485: 481,\n",
              " 486: 482,\n",
              " 487: 483,\n",
              " 488: 484,\n",
              " 489: 485,\n",
              " 490: 486,\n",
              " 491: 487,\n",
              " 492: 488,\n",
              " 493: 489,\n",
              " 494: 490,\n",
              " 495: 491,\n",
              " 496: 492,\n",
              " 497: 493,\n",
              " 498: 494,\n",
              " 499: 495,\n",
              " 500: 496,\n",
              " 501: 497,\n",
              " 502: 498,\n",
              " 503: 499,\n",
              " 504: 500,\n",
              " 505: 501,\n",
              " 506: 502,\n",
              " 507: 503,\n",
              " 508: 504,\n",
              " 509: 505,\n",
              " 510: 506,\n",
              " 511: 507,\n",
              " 512: 508,\n",
              " 513: 509,\n",
              " 514: 510,\n",
              " 515: 511,\n",
              " 516: 512,\n",
              " 517: 513,\n",
              " 518: 514,\n",
              " 519: 515,\n",
              " 520: 516,\n",
              " 521: 517,\n",
              " 522: 518,\n",
              " 523: 519,\n",
              " 524: 520,\n",
              " 525: 521,\n",
              " 526: 522,\n",
              " 527: 523,\n",
              " 528: 524,\n",
              " 529: 525,\n",
              " 530: 526,\n",
              " 531: 527,\n",
              " 532: 528,\n",
              " 533: 529,\n",
              " 534: 530,\n",
              " 535: 531,\n",
              " 536: 532,\n",
              " 537: 533,\n",
              " 538: 534,\n",
              " 539: 535,\n",
              " 540: 536,\n",
              " 541: 537,\n",
              " 542: 538,\n",
              " 543: 539,\n",
              " 544: 540,\n",
              " 545: 541,\n",
              " 546: 542,\n",
              " 547: 543,\n",
              " 548: 544,\n",
              " 549: 545,\n",
              " 550: 546,\n",
              " 551: 547,\n",
              " 552: 548,\n",
              " 553: 549,\n",
              " 554: 550,\n",
              " 555: 551,\n",
              " 556: 552,\n",
              " 557: 553,\n",
              " 558: 554,\n",
              " 559: 555,\n",
              " 560: 556,\n",
              " 561: 557,\n",
              " 562: 558,\n",
              " 563: 559,\n",
              " 564: 560,\n",
              " 565: 561,\n",
              " 566: 562,\n",
              " 567: 563,\n",
              " 568: 564,\n",
              " 569: 565,\n",
              " 570: 566,\n",
              " 571: 567,\n",
              " 572: 568,\n",
              " 573: 569,\n",
              " 574: 570,\n",
              " 575: 571,\n",
              " 576: 572,\n",
              " 577: 573,\n",
              " 578: 574,\n",
              " 579: 575,\n",
              " 580: 576,\n",
              " 581: 577,\n",
              " 582: 578,\n",
              " 583: 579,\n",
              " 584: 580,\n",
              " 585: 581,\n",
              " 586: 582,\n",
              " 587: 583,\n",
              " 588: 584,\n",
              " 589: 585,\n",
              " 590: 586,\n",
              " 591: 587,\n",
              " 592: 588,\n",
              " 593: 589,\n",
              " 594: 590,\n",
              " 595: 591,\n",
              " 596: 592,\n",
              " 597: 593,\n",
              " 598: 594,\n",
              " 599: 595,\n",
              " 600: 596,\n",
              " 601: 597,\n",
              " 602: 598,\n",
              " 603: 599,\n",
              " 604: 600,\n",
              " 605: 601,\n",
              " 606: 602,\n",
              " 607: 603,\n",
              " 608: 604,\n",
              " 609: 605,\n",
              " 610: 606,\n",
              " 611: 607,\n",
              " 612: 608,\n",
              " 613: 609,\n",
              " 614: 610,\n",
              " 615: 611,\n",
              " 616: 612,\n",
              " 617: 613,\n",
              " 618: 614,\n",
              " 619: 615,\n",
              " 620: 616,\n",
              " 621: 617,\n",
              " 623: 618,\n",
              " 624: 619,\n",
              " 625: 620,\n",
              " 626: 621,\n",
              " 627: 622,\n",
              " 628: 623,\n",
              " 629: 624,\n",
              " 630: 625,\n",
              " 631: 626,\n",
              " 632: 627,\n",
              " 633: 628,\n",
              " 634: 629,\n",
              " 635: 630,\n",
              " 636: 631,\n",
              " 637: 632,\n",
              " 638: 633,\n",
              " 639: 634,\n",
              " 640: 635,\n",
              " 641: 636,\n",
              " 642: 637,\n",
              " 643: 638,\n",
              " 644: 639,\n",
              " 645: 640,\n",
              " 647: 641,\n",
              " 648: 642,\n",
              " 649: 643,\n",
              " 650: 644,\n",
              " 651: 645,\n",
              " 652: 646,\n",
              " 653: 647,\n",
              " 654: 648,\n",
              " 655: 649,\n",
              " 656: 650,\n",
              " 657: 651,\n",
              " 658: 652,\n",
              " 659: 653,\n",
              " 660: 654,\n",
              " 661: 655,\n",
              " 662: 656,\n",
              " 663: 657,\n",
              " 664: 658,\n",
              " 665: 659,\n",
              " 666: 660,\n",
              " 667: 661,\n",
              " 668: 662,\n",
              " 669: 663,\n",
              " 670: 664,\n",
              " 671: 665,\n",
              " 672: 666,\n",
              " 673: 667,\n",
              " 674: 668,\n",
              " 675: 669,\n",
              " 676: 670,\n",
              " 678: 671,\n",
              " 679: 672,\n",
              " 680: 673,\n",
              " 681: 674,\n",
              " 682: 675,\n",
              " 683: 676,\n",
              " 684: 677,\n",
              " 685: 678,\n",
              " 687: 679,\n",
              " 688: 680,\n",
              " 690: 681,\n",
              " 691: 682,\n",
              " 692: 683,\n",
              " 693: 684,\n",
              " 694: 685,\n",
              " 695: 686,\n",
              " 696: 687,\n",
              " 697: 688,\n",
              " 698: 689,\n",
              " 699: 690,\n",
              " 700: 691,\n",
              " 701: 692,\n",
              " 702: 693,\n",
              " 703: 694,\n",
              " 704: 695,\n",
              " 705: 696,\n",
              " 706: 697,\n",
              " 707: 698,\n",
              " 708: 699,\n",
              " 709: 700,\n",
              " 710: 701,\n",
              " 711: 702,\n",
              " 712: 703,\n",
              " 713: 704,\n",
              " 714: 705,\n",
              " 715: 706,\n",
              " 716: 707,\n",
              " 717: 708,\n",
              " 718: 709,\n",
              " 719: 710,\n",
              " 720: 711,\n",
              " 721: 712,\n",
              " 722: 713,\n",
              " 723: 714,\n",
              " 724: 715,\n",
              " 725: 716,\n",
              " 726: 717,\n",
              " 727: 718,\n",
              " 728: 719,\n",
              " 729: 720,\n",
              " 730: 721,\n",
              " 731: 722,\n",
              " 732: 723,\n",
              " 733: 724,\n",
              " 734: 725,\n",
              " 735: 726,\n",
              " 736: 727,\n",
              " 737: 728,\n",
              " 738: 729,\n",
              " 739: 730,\n",
              " 741: 731,\n",
              " 742: 732,\n",
              " 743: 733,\n",
              " 744: 734,\n",
              " 745: 735,\n",
              " 746: 736,\n",
              " 747: 737,\n",
              " 748: 738,\n",
              " 749: 739,\n",
              " 750: 740,\n",
              " 751: 741,\n",
              " 752: 742,\n",
              " 753: 743,\n",
              " 754: 744,\n",
              " 755: 745,\n",
              " 756: 746,\n",
              " 757: 747,\n",
              " 758: 748,\n",
              " 759: 749,\n",
              " 760: 750,\n",
              " 761: 751,\n",
              " 762: 752,\n",
              " 763: 753,\n",
              " 764: 754,\n",
              " 765: 755,\n",
              " 766: 756,\n",
              " 767: 757,\n",
              " 768: 758,\n",
              " 769: 759,\n",
              " 770: 760,\n",
              " 771: 761,\n",
              " 772: 762,\n",
              " 773: 763,\n",
              " 774: 764,\n",
              " 775: 765,\n",
              " 776: 766,\n",
              " 777: 767,\n",
              " 778: 768,\n",
              " 779: 769,\n",
              " 780: 770,\n",
              " 781: 771,\n",
              " 782: 772,\n",
              " 783: 773,\n",
              " 784: 774,\n",
              " 785: 775,\n",
              " 786: 776,\n",
              " 787: 777,\n",
              " 788: 778,\n",
              " 789: 779,\n",
              " 790: 780,\n",
              " 791: 781,\n",
              " 792: 782,\n",
              " 793: 783,\n",
              " 794: 784,\n",
              " 795: 785,\n",
              " 796: 786,\n",
              " 797: 787,\n",
              " 798: 788,\n",
              " 799: 789,\n",
              " 800: 790,\n",
              " 801: 791,\n",
              " 802: 792,\n",
              " 803: 793,\n",
              " 804: 794,\n",
              " 805: 795,\n",
              " 806: 796,\n",
              " 807: 797,\n",
              " 808: 798,\n",
              " 809: 799,\n",
              " 810: 800,\n",
              " 811: 801,\n",
              " 812: 802,\n",
              " 813: 803,\n",
              " 814: 804,\n",
              " 815: 805,\n",
              " 816: 806,\n",
              " 818: 807,\n",
              " 819: 808,\n",
              " 820: 809,\n",
              " 821: 810,\n",
              " 822: 811,\n",
              " 823: 812,\n",
              " 824: 813,\n",
              " 825: 814,\n",
              " 826: 815,\n",
              " 827: 816,\n",
              " 828: 817,\n",
              " 829: 818,\n",
              " 830: 819,\n",
              " 831: 820,\n",
              " 832: 821,\n",
              " 833: 822,\n",
              " 834: 823,\n",
              " 835: 824,\n",
              " 836: 825,\n",
              " 837: 826,\n",
              " 838: 827,\n",
              " 839: 828,\n",
              " 840: 829,\n",
              " 841: 830,\n",
              " 842: 831,\n",
              " 843: 832,\n",
              " 844: 833,\n",
              " 845: 834,\n",
              " 846: 835,\n",
              " 847: 836,\n",
              " 848: 837,\n",
              " 849: 838,\n",
              " 850: 839,\n",
              " 851: 840,\n",
              " 852: 841,\n",
              " 853: 842,\n",
              " 854: 843,\n",
              " 855: 844,\n",
              " 856: 845,\n",
              " 857: 846,\n",
              " 858: 847,\n",
              " 859: 848,\n",
              " 860: 849,\n",
              " 861: 850,\n",
              " 862: 851,\n",
              " 863: 852,\n",
              " 864: 853,\n",
              " 865: 854,\n",
              " 866: 855,\n",
              " 867: 856,\n",
              " 868: 857,\n",
              " 869: 858,\n",
              " 870: 859,\n",
              " 871: 860,\n",
              " 872: 861,\n",
              " 873: 862,\n",
              " 874: 863,\n",
              " 875: 864,\n",
              " 876: 865,\n",
              " 877: 866,\n",
              " 878: 867,\n",
              " 879: 868,\n",
              " 880: 869,\n",
              " 881: 870,\n",
              " 882: 871,\n",
              " 884: 872,\n",
              " 885: 873,\n",
              " 886: 874,\n",
              " 887: 875,\n",
              " 888: 876,\n",
              " 889: 877,\n",
              " 890: 878,\n",
              " 891: 879,\n",
              " 892: 880,\n",
              " 893: 881,\n",
              " 894: 882,\n",
              " 895: 883,\n",
              " 896: 884,\n",
              " 897: 885,\n",
              " 898: 886,\n",
              " 899: 887,\n",
              " 900: 888,\n",
              " 901: 889,\n",
              " 902: 890,\n",
              " 903: 891,\n",
              " 904: 892,\n",
              " 905: 893,\n",
              " 906: 894,\n",
              " 907: 895,\n",
              " 908: 896,\n",
              " 909: 897,\n",
              " 910: 898,\n",
              " 911: 899,\n",
              " 912: 900,\n",
              " 913: 901,\n",
              " 914: 902,\n",
              " 915: 903,\n",
              " 916: 904,\n",
              " 917: 905,\n",
              " 918: 906,\n",
              " 919: 907,\n",
              " 920: 908,\n",
              " 921: 909,\n",
              " 922: 910,\n",
              " 923: 911,\n",
              " 924: 912,\n",
              " 925: 913,\n",
              " 926: 914,\n",
              " 927: 915,\n",
              " 928: 916,\n",
              " 929: 917,\n",
              " 930: 918,\n",
              " 931: 919,\n",
              " 932: 920,\n",
              " 933: 921,\n",
              " 934: 922,\n",
              " 935: 923,\n",
              " 936: 924,\n",
              " 937: 925,\n",
              " 938: 926,\n",
              " 939: 927,\n",
              " 940: 928,\n",
              " 941: 929,\n",
              " 942: 930,\n",
              " 943: 931,\n",
              " 944: 932,\n",
              " 945: 933,\n",
              " 946: 934,\n",
              " 947: 935,\n",
              " 948: 936,\n",
              " 949: 937,\n",
              " 950: 938,\n",
              " 951: 939,\n",
              " 952: 940,\n",
              " 953: 941,\n",
              " 954: 942,\n",
              " 955: 943,\n",
              " 956: 944,\n",
              " 957: 945,\n",
              " 958: 946,\n",
              " 959: 947,\n",
              " 960: 948,\n",
              " 961: 949,\n",
              " 962: 950,\n",
              " 963: 951,\n",
              " 964: 952,\n",
              " 965: 953,\n",
              " 966: 954,\n",
              " 967: 955,\n",
              " 968: 956,\n",
              " 969: 957,\n",
              " 970: 958,\n",
              " 971: 959,\n",
              " 972: 960,\n",
              " 973: 961,\n",
              " 974: 962,\n",
              " 975: 963,\n",
              " 976: 964,\n",
              " 977: 965,\n",
              " 978: 966,\n",
              " 979: 967,\n",
              " 980: 968,\n",
              " 981: 969,\n",
              " 982: 970,\n",
              " 983: 971,\n",
              " 984: 972,\n",
              " 985: 973,\n",
              " 986: 974,\n",
              " 987: 975,\n",
              " 988: 976,\n",
              " 989: 977,\n",
              " 990: 978,\n",
              " 991: 979,\n",
              " 992: 980,\n",
              " 993: 981,\n",
              " 994: 982,\n",
              " 996: 983,\n",
              " 997: 984,\n",
              " 998: 985,\n",
              " 999: 986,\n",
              " 1000: 987,\n",
              " 1001: 988,\n",
              " 1002: 989,\n",
              " 1003: 990,\n",
              " 1004: 991,\n",
              " 1005: 992,\n",
              " 1006: 993,\n",
              " 1007: 994,\n",
              " 1008: 995,\n",
              " 1009: 996,\n",
              " 1010: 997,\n",
              " 1011: 998,\n",
              " 1012: 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR2X9SxJISzC"
      },
      "source": [
        "#超参数设置\n",
        "num_epochs= 5\n",
        "batch_size =256\n",
        "dropout_keep = 0.5\n",
        "learning_rate = 0.0001\n",
        "show_every_n_batches = 20\n",
        "\n",
        "save_dir = './'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4OMV4hCIykJ"
      },
      "source": [
        "# #定义输入的占位符\n",
        "def get_inputs():\n",
        "  uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='uid')  \n",
        "  user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_gender')  \n",
        "  user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_age') \n",
        "  user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_job')\n",
        "\n",
        "  movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name='movie_id') \n",
        "  movie_categories = tf.keras.layers.Input(shape=(18,), dtype='int32', name='movie_categories') \n",
        "  movie_titles = tf.keras.layers.Input(shape=(15,), dtype='int32', name='movie_titles') \n",
        "  return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hjQwCIPPXT"
      },
      "source": [
        "tf.keras.Input函数用于向模型中输入数据，并指定数据的形状、数据类型等信息"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd86g2fOVMi"
      },
      "source": [
        "keras.layers.Embedding(input_dim,output_dim,input_length)\n",
        "\n",
        "\n",
        "*   input_dim:词汇表的维度（有多少个不同词汇）\n",
        "*   output_dim:嵌入词空间的维度（每个词嵌入为多少维度）\n",
        "\n",
        "\n",
        "*   input_length:输入语句的长度（每一个词汇多少维）\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE792AqWOHA0",
        "outputId": "ed6712e9-8b42-437c-c8a7-a62df41e23d9"
      },
      "source": [
        "#test  embedding\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[0,1,2],[3,4,5]])\n",
        "print(len(data))\n",
        "emb = keras.layers.Embedding(input_dim=6, output_dim=4, input_length=3) # 只有0,1所以input_dim是2\n",
        "emb(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
              "array([[[-0.03194529, -0.00087141,  0.04880381,  0.02591045],\n",
              "        [-0.04148025, -0.02257617,  0.03808982, -0.04969278],\n",
              "        [ 0.03256606, -0.03738389, -0.04365519,  0.0108851 ]],\n",
              "\n",
              "       [[ 0.01080049, -0.0311819 ,  0.04407945, -0.03666271],\n",
              "        [ 0.04563019,  0.04819504,  0.04425083,  0.04656512],\n",
              "        [-0.03247966, -0.03486309,  0.01843408,  0.04667502]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8KrsOODPn8T"
      },
      "source": [
        "#定义用户的嵌入矩阵\n",
        "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
        "  uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid) \n",
        "  gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
        "  age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
        "  job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
        "  return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xu30O2aylQ"
      },
      "source": [
        "\n",
        "```\n",
        "tf.keras.layers.Dense\n",
        "(\n",
        "    units,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "tf.keras.layers.Dense相当于在全连接层中添加一个层\n",
        "\n",
        "units:输出维度的大小,这一层多少神经元\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2WnRKHzjaEb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93oRMoyjR6X9"
      },
      "source": [
        "#将User的嵌入矩阵一起全连接生成User的特征\n",
        "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
        "    #第一层全连接\n",
        "  uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
        "  gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
        "  age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
        "  job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
        "\n",
        "    #第二层全连接\n",
        "  user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
        "  user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  #(?, 1, 200)\n",
        "\n",
        "  user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)##这一步不太明白\n",
        "  return user_combine_layer, user_combine_layer_flat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGbQBecHnNZz"
      },
      "source": [
        "#定义Movie Id的嵌入矩阵\n",
        "def get_movie_id_embed_layer(movie_id):\n",
        "  movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max,embed_dim,input_length=1,name='movie_id_embed_layer')(movie_id)\n",
        "  return movie_id_embed_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-68EPnfhpJjU"
      },
      "source": [
        "#合并电影类型的多个嵌入向量\n",
        "def get_movie_categories_embed_layers(movie_categories):\n",
        "  movie_categories_embed_layer = tf.keras.layers.Embedding(movie_categories_max,embed_dim,input_length=18,name='movie_categories_embed_layer')(movie_categories)\n",
        "  movie_categories_embed_layer = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer,axis=1,keepdims=True))(movie_categories_embed_layer)#每一行求一个和(\n",
        "  return movie_categories_embed_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXvbmvnSQ6H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYXUerzCPf0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzpfwH5e8o2b"
      },
      "source": [
        "#title 的文本卷积网络实现\n",
        "def get_movie_cnn_layer(movie_titles):\n",
        "\n",
        "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
        "  movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max, embed_dim, input_length=15, name='movie_title_embed_layer')(movie_titles)\n",
        "  sp=movie_title_embed_layer.shape\n",
        "  movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1], sp[2], 1])(movie_title_embed_layer)\n",
        "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
        "  pool_layer_lst = []\n",
        "  for window_size in window_sizes:\n",
        "      conv_layer = tf.keras.layers.Conv2D(filter_num, (window_size, embed_dim), 1, activation='relu')(movie_title_embed_layer_expand)\n",
        "      maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(sentences_size - window_size + 1 ,1), strides=1)(conv_layer)\n",
        "      pool_layer_lst.append(maxpool_layer)\n",
        "    #Dropout层\n",
        "  pool_layer = tf.keras.layers.concatenate(pool_layer_lst, 3, name =\"pool_layer\")  \n",
        "  max_num = len(window_sizes) * filter_num\n",
        "  pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
        "\n",
        "  dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
        "  return pool_layer_flat, dropout_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwbwEtFvVxEY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TjQIf-uEwk_"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "tf.keras.layers.Conv2D(\n",
        "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
        "    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
        "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
        "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
        "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "*   filters:卷积核的个数\n",
        "*   kernel_size:卷积核尺寸\n",
        "\n",
        "\n",
        "*   strides：滑动步长\n",
        "*   padding：补全策略，\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fqiu9BzFwJO"
      },
      "source": [
        "#将movie各个层一起做全连接\n",
        "def get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer):\n",
        "  #第一层全连接\n",
        "  movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
        "  movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim,name=\"movie_categories_fc_layer\",activation='relu')(movie_categories_embed_layer)\n",
        "\n",
        "  #第二层全连接\n",
        "  movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer,movie_categories_fc_layer,dropout_layer], 2)\n",
        "  movie_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(movie_combine_layer)\n",
        "  movie_combine_layer_flat = tf.keras.layers.Reshape([200],name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
        "\n",
        "  return movie_combine_layer,movie_combine_layer_flat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghyJC686FqEE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuPAQNdfLu2p"
      },
      "source": [
        "def get_batches(Xs,ys,batch_size):\n",
        "    for start in range(0,len(Xs),batch_size):\n",
        "      end = min(start+batch_size,len(Xs))\n",
        "      yield Xs[start:end],ys[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Etn4QKoThb"
      },
      "source": [
        "#构建计算图\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "import time\n",
        "\n",
        "MODEL_DIR= \"./models\"\n",
        "\n",
        "class mv_network(object):\n",
        "  def __init__(self,batch_szie = 256):\n",
        "    self.batch_size=batch_size\n",
        "    self.best_loss = 9999\n",
        "    self.losses={'train':[],'test':[]}\n",
        "\n",
        "    #获取占位符\n",
        "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
        "    #d得到用户的4个嵌入向量\n",
        "    uid_embed_layer,gender_embed_layer,age_embed_layer,job_embed_layer = get_user_embedding(uid,user_gender,user_age,user_job)\n",
        "    print(uid_embed_layer)\n",
        "    #得到用户特征\n",
        "    user_combine_layer,user_combine_layer_flat = get_user_feature_layer(uid_embed_layer,gender_embed_layer,age_embed_layer,job_embed_layer)\n",
        "    \n",
        "    #获取电影ID的嵌入特征\n",
        "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
        "    #获取电影类型的嵌入向量\n",
        "    movie_categories_embed_layer = get_movie_categories_embed_layers(movie_categories)\n",
        "\n",
        "    #获取电影名的特征向量\n",
        "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
        "    #得到电影特征\n",
        "\n",
        "    movie_combine_layer,movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer)\n",
        "    #计算出评分\n",
        "    #将用户特征与电影特征做矩阵乘法得到预测评分的方案\n",
        "    inference = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer[0] * layer[1],axis=1),name='inference')((user_combine_layer_flat,movie_combine_layer_flat))\n",
        "    inference = tf.keras.layers.Lambda(lambda layer:tf.expand_dims(layer,axis=1))(inference)\n",
        "    self.model = tf.keras.Model(\n",
        "        inputs=[uid,user_gender,user_age,user_job,movie_id,movie_categories,movie_titles],\n",
        "        outputs=inference\n",
        "        )\n",
        "    self.model.summary()\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
        "    self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
        "    \n",
        "    if tf.io.gfile.exists(MODEL_DIR):\n",
        "      pass\n",
        "    else:\n",
        "      tf.io.gfile.makedirs(MODEL_DIR)\n",
        "    \n",
        "    train_dir = os.path.join(MODEL_DIR,'summaries','train')\n",
        "    test_dir = os.path.join(MODEL_DIR,'summaries','eval')\n",
        "\n",
        "    checkpoint_dir = os.path.join(MODEL_DIR,'checkpoints')\n",
        "    self.checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
        "    self.checkpoint = tf.train.Checkpoint(model=self.model,optimizer=self.optimizer)\n",
        "    self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "  def compute_loss(self,labels,logits):\n",
        "    return tf.reduce_mean(tf.keras.losses.mse(labels,logits))\n",
        "  def compute_metrics(self,labels,logits):\n",
        "    return tf.keras.metrics.mae(labels,logits)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self,x,y):\n",
        "    #记录用于计算损失的操作，以便可以计算损失相对于变量的梯度\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = self.model([x[0],\n",
        "                  x[1],\n",
        "                  x[2],\n",
        "                  x[3],\n",
        "                  x[4],\n",
        "                  x[5],\n",
        "                  x[6]],training=True)\n",
        "      loss = self.ComputeLoss(y,logits)\n",
        "      self.ComputeMetrics(y,logits)\n",
        "    grads = tape.gradient(loss,self.model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads,self.model.trainable_variables))\n",
        "    return loss, logits\n",
        "\n",
        "  def training(self, features, targets_values, epochs=5, log_freq=50):\n",
        "    for epoch_i in range(epochs):\n",
        "      train_X,test_X,train_y,test_y = train_test_split(features,targets_values,test_size=0.2,random_state=0)\n",
        "      train_batches = get_batches(train_X,train_y,self.batch_size)\n",
        "      batch_num = (len(train_X)//self.batch_size)\n",
        "      train_start = time.time()\n",
        "      if True:\n",
        "        start = time.time()#当前时间的时间戳\n",
        "        avg_loss = tf.keras.metrics.Mean('loss',dtype=tf.float32)\n",
        "        for batch_i in range(batch_num):\n",
        "          x, y = next(train_batches)\n",
        "          categories = np.zeros([self.batch_size,18])\n",
        "          for i in range(self.batch_size):\n",
        "            categories[i] = x.take(6,1)[i]\n",
        "\n",
        "          titles = np.zeros([self.batch_size, sentences_size])\n",
        "          for i in range(self.batch_size):\n",
        "            titles[i] = x.take(5,1)[i]\n",
        "          loss,logits = self.train_step([np.reshape(x.take(0,1),[self.batch_size,1]).astype(np.float32),\n",
        "                        np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                         categories.astype(np.float32),\n",
        "                                                    titles.astype(np.float32)], np.reshape(y, [self.batch_size, 1]).astype(np.float32))\n",
        "          \n",
        "\n",
        "          avg_loss(loss)\n",
        "          self.losses['train'].append(loss)\n",
        "          if tf.equal(self.optimizer.iterations % log_freq,0):\n",
        "            rate = log_freq / (time.time()-start)\n",
        "            print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
        "                            self.optimizer.iterations.numpy(),\n",
        "                            epoch_i,\n",
        "                            batch_i,\n",
        "                            batch_num,\n",
        "                            loss, (self.ComputeMetrics.result()), rate))\n",
        "            avg_loss.reset_states()\n",
        "            self.ComputeMetrics.reset_states()\n",
        "                        # avg_mae.reset_states()\n",
        "            start = time.time()\n",
        "        train_end = time.time()\n",
        "        print(\n",
        "                '\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, self.optimizer.iterations.numpy(),\n",
        "                                                                         train_end - train_start))\n",
        "            #             with self.test_summary_writer.as_default():\n",
        "        self.testing((test_X, test_y), self.optimizer.iterations)\n",
        "            # self.checkpoint.save(self.checkpoint_prefix)\n",
        "      self.export_path = os.path.join(MODEL_DIR, 'export')\n",
        "      tf.saved_model.save(self.model, self.export_path)\n",
        "  def testing(self, test_dataset, step_num):\n",
        "      test_X, test_y = test_dataset\n",
        "      test_batches = get_batches(test_X, test_y, self.batch_size)\n",
        "\n",
        "      \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
        "      avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "        #         avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
        "\n",
        "      batch_num = (len(test_X) // self.batch_size)\n",
        "      for batch_i in range(batch_num):\n",
        "          x, y = next(test_batches)\n",
        "          categories = np.zeros([self.batch_size, 18])\n",
        "          for i in range(self.batch_size):\n",
        "              categories[i] = x.take(6, 1)[i]\n",
        "\n",
        "          titles = np.zeros([self.batch_size, sentences_size])\n",
        "          for i in range(self.batch_size):\n",
        "              titles[i] = x.take(5, 1)[i]\n",
        "\n",
        "          logits = self.model([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 categories.astype(np.float32),\n",
        "                                 titles.astype(np.float32)], training=False)\n",
        "          test_loss = self.ComputeLoss(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "          avg_loss(test_loss)\n",
        "            # 保存测试损失\n",
        "          self.losses['test'].append(test_loss)\n",
        "          self.ComputeMetrics(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "            # avg_loss(self.compute_loss(labels, logits))\n",
        "            # avg_mae(self.compute_metrics(labels, logits))\n",
        "\n",
        "      print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), self.ComputeMetrics.result()))\n",
        "        # print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), avg_mae.result()))\n",
        "        #         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "        #         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=step_num)\n",
        "        # summary_ops_v2.scalar('mae', avg_mae.result(), step=step_num)\n",
        "\n",
        "      if avg_loss.result() < self.best_loss:\n",
        "          self.best_loss = avg_loss.result()\n",
        "          print(\"best loss = {}\".format(self.best_loss))\n",
        "          self.checkpoint.save(self.checkpoint_prefix)\n",
        "\n",
        "  def forward(self, xs):\n",
        "      predictions = self.model(xs)\n",
        "        # logits = tf.nn.softmax(predictions)\n",
        "\n",
        "      return predictions\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElrCf6zpKAbY"
      },
      "source": [
        "使用keras.layers.Reshape实现keras不同层的对接,改变了数据的维度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng-l60Tn1N2f",
        "outputId": "96ec0229-4f13-4773-dbda-e1bd8db12ca9"
      },
      "source": [
        "mv_net=mv_network()\n",
        "mv_net.training(features,targets_values,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 32), dtype=tf.float32, name=None), name='uid_embed_layer/embedding_lookup/Identity_1:0', description=\"created by layer 'uid_embed_layer'\")\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "movie_titles (InputLayer)       [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "movie_title_embed_layer (Embedd (None, 15, 32)       166880      movie_titles[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 15, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 14, 1, 8)     520         reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 13, 1, 8)     776         reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 12, 1, 8)     1032        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 11, 1, 8)     1288        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "movie_categories (InputLayer)   [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "uid (InputLayer)                [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_age (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_job (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "movie_categories_embed_layer (E (None, 18, 32)       608         movie_categories[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d_4[0][0]            \n",
            "                                                                 max_pooling2d_5[0][0]            \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 1, 32)        0           movie_categories_embed_layer[0][0\n",
            "__________________________________________________________________________________________________\n",
            "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "movie_categories_fc_layer (Dens (None, 1, 32)        1056        lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
            "                                                                 gender_fc_layer[0][0]            \n",
            "                                                                 age_fc_layer[0][0]               \n",
            "                                                                 job_fc_layer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
            "                                                                 movie_categories_fc_layer[0][0]  \n",
            "                                                                 dropout_layer[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1, 200)       25800       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1, 200)       19400       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "user_combine_layer_flat (Reshap (None, 200)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "movie_combine_layer_flat (Resha (None, 200)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "inference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n",
            "                                                                 movie_combine_layer_flat[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 1)            0           inference[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 541,392\n",
            "Trainable params: 541,392\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Step #31300\tEpoch   0 Batch   49/3125   Loss: 0.828004 mae: 0.683221 (14.036320177852787 steps/sec)\n",
            "Step #31350\tEpoch   0 Batch   99/3125   Loss: 0.917272 mae: 0.672456 (29.095837127287766 steps/sec)\n",
            "Step #31400\tEpoch   0 Batch  149/3125   Loss: 0.711907 mae: 0.686273 (29.213062432579413 steps/sec)\n",
            "Step #31450\tEpoch   0 Batch  199/3125   Loss: 0.771259 mae: 0.685748 (29.193225657997125 steps/sec)\n",
            "Step #31500\tEpoch   0 Batch  249/3125   Loss: 0.836360 mae: 0.684573 (28.267816599513566 steps/sec)\n",
            "Step #31550\tEpoch   0 Batch  299/3125   Loss: 0.728233 mae: 0.684615 (29.431861687854575 steps/sec)\n",
            "Step #31600\tEpoch   0 Batch  349/3125   Loss: 0.820868 mae: 0.686354 (29.442894331397103 steps/sec)\n",
            "Step #31650\tEpoch   0 Batch  399/3125   Loss: 0.875808 mae: 0.688714 (28.74525042124434 steps/sec)\n",
            "Step #31700\tEpoch   0 Batch  449/3125   Loss: 0.752472 mae: 0.691510 (29.226709241381464 steps/sec)\n",
            "Step #31750\tEpoch   0 Batch  499/3125   Loss: 0.730596 mae: 0.679138 (28.87087805849777 steps/sec)\n",
            "Step #31800\tEpoch   0 Batch  549/3125   Loss: 0.737797 mae: 0.677914 (29.123972194107967 steps/sec)\n",
            "Step #31850\tEpoch   0 Batch  599/3125   Loss: 0.655363 mae: 0.691662 (28.10413616800472 steps/sec)\n",
            "Step #31900\tEpoch   0 Batch  649/3125   Loss: 0.705627 mae: 0.686301 (29.269303011211967 steps/sec)\n",
            "Step #31950\tEpoch   0 Batch  699/3125   Loss: 0.768924 mae: 0.688012 (28.96455558858009 steps/sec)\n",
            "Step #32000\tEpoch   0 Batch  749/3125   Loss: 0.739246 mae: 0.676452 (29.26612112170014 steps/sec)\n",
            "Step #32050\tEpoch   0 Batch  799/3125   Loss: 0.784415 mae: 0.685673 (29.260102340150198 steps/sec)\n",
            "Step #32100\tEpoch   0 Batch  849/3125   Loss: 0.832468 mae: 0.686238 (29.39530653914766 steps/sec)\n",
            "Step #32150\tEpoch   0 Batch  899/3125   Loss: 0.807392 mae: 0.682879 (29.256395937436306 steps/sec)\n",
            "Step #32200\tEpoch   0 Batch  949/3125   Loss: 0.669640 mae: 0.684259 (28.89590374792804 steps/sec)\n",
            "Step #32250\tEpoch   0 Batch  999/3125   Loss: 0.813653 mae: 0.678416 (28.56113260039457 steps/sec)\n",
            "Step #32300\tEpoch   0 Batch 1049/3125   Loss: 0.830705 mae: 0.691457 (28.401607818069536 steps/sec)\n",
            "Step #32350\tEpoch   0 Batch 1099/3125   Loss: 0.717283 mae: 0.683183 (29.063949497529467 steps/sec)\n",
            "Step #32400\tEpoch   0 Batch 1149/3125   Loss: 0.685138 mae: 0.691375 (29.160617470601984 steps/sec)\n",
            "Step #32450\tEpoch   0 Batch 1199/3125   Loss: 0.812745 mae: 0.680630 (29.19887545834874 steps/sec)\n",
            "Step #32500\tEpoch   0 Batch 1249/3125   Loss: 0.829234 mae: 0.686816 (29.567802785065407 steps/sec)\n",
            "Step #32550\tEpoch   0 Batch 1299/3125   Loss: 0.712145 mae: 0.691892 (28.570969258121426 steps/sec)\n",
            "Step #32600\tEpoch   0 Batch 1349/3125   Loss: 0.779564 mae: 0.668736 (29.174247781171996 steps/sec)\n",
            "Step #32650\tEpoch   0 Batch 1399/3125   Loss: 0.775226 mae: 0.685800 (28.77947503839387 steps/sec)\n",
            "Step #32700\tEpoch   0 Batch 1449/3125   Loss: 0.744058 mae: 0.682273 (28.876415695514552 steps/sec)\n",
            "Step #32750\tEpoch   0 Batch 1499/3125   Loss: 0.784871 mae: 0.683293 (29.68934062870869 steps/sec)\n",
            "Step #32800\tEpoch   0 Batch 1549/3125   Loss: 0.819601 mae: 0.681992 (29.244091538946275 steps/sec)\n",
            "Step #32850\tEpoch   0 Batch 1599/3125   Loss: 0.762629 mae: 0.677650 (28.90951082918378 steps/sec)\n",
            "Step #32900\tEpoch   0 Batch 1649/3125   Loss: 0.793284 mae: 0.685240 (29.223267838661908 steps/sec)\n",
            "Step #32950\tEpoch   0 Batch 1699/3125   Loss: 0.782729 mae: 0.677909 (29.148288152128302 steps/sec)\n",
            "Step #33000\tEpoch   0 Batch 1749/3125   Loss: 0.756455 mae: 0.680679 (29.17786438521673 steps/sec)\n",
            "Step #33050\tEpoch   0 Batch 1799/3125   Loss: 0.832221 mae: 0.681180 (29.11099089990029 steps/sec)\n",
            "Step #33100\tEpoch   0 Batch 1849/3125   Loss: 0.730170 mae: 0.681344 (29.107899897178974 steps/sec)\n",
            "Step #33150\tEpoch   0 Batch 1899/3125   Loss: 0.836249 mae: 0.681209 (28.927168132047296 steps/sec)\n",
            "Step #33200\tEpoch   0 Batch 1949/3125   Loss: 0.740578 mae: 0.675913 (29.322541779496206 steps/sec)\n",
            "Step #33250\tEpoch   0 Batch 1999/3125   Loss: 0.820250 mae: 0.682839 (29.25102168909966 steps/sec)\n",
            "Step #33300\tEpoch   0 Batch 2049/3125   Loss: 0.705277 mae: 0.693345 (29.013796118492774 steps/sec)\n",
            "Step #33350\tEpoch   0 Batch 2099/3125   Loss: 0.787147 mae: 0.674196 (29.253763655649152 steps/sec)\n",
            "Step #33400\tEpoch   0 Batch 2149/3125   Loss: 0.702219 mae: 0.674267 (28.67211256044189 steps/sec)\n",
            "Step #33450\tEpoch   0 Batch 2199/3125   Loss: 0.667253 mae: 0.679013 (28.67839774724744 steps/sec)\n",
            "Step #33500\tEpoch   0 Batch 2249/3125   Loss: 0.709677 mae: 0.677513 (29.054648011895885 steps/sec)\n",
            "Step #33550\tEpoch   0 Batch 2299/3125   Loss: 0.808955 mae: 0.689846 (29.358122840210015 steps/sec)\n",
            "Step #33600\tEpoch   0 Batch 2349/3125   Loss: 0.731039 mae: 0.682807 (29.241025210573802 steps/sec)\n",
            "Step #33650\tEpoch   0 Batch 2399/3125   Loss: 0.687960 mae: 0.682436 (29.379427345823473 steps/sec)\n",
            "Step #33700\tEpoch   0 Batch 2449/3125   Loss: 0.709890 mae: 0.675139 (29.50785672406047 steps/sec)\n",
            "Step #33750\tEpoch   0 Batch 2499/3125   Loss: 0.863589 mae: 0.686456 (29.117271899648355 steps/sec)\n",
            "Step #33800\tEpoch   0 Batch 2549/3125   Loss: 0.595317 mae: 0.687359 (28.76103112000531 steps/sec)\n",
            "Step #33850\tEpoch   0 Batch 2599/3125   Loss: 0.871271 mae: 0.683593 (29.260927019998263 steps/sec)\n",
            "Step #33900\tEpoch   0 Batch 2649/3125   Loss: 0.860984 mae: 0.682059 (29.122605196918144 steps/sec)\n",
            "Step #33950\tEpoch   0 Batch 2699/3125   Loss: 0.749195 mae: 0.679999 (29.217836551636797 steps/sec)\n",
            "Step #34000\tEpoch   0 Batch 2749/3125   Loss: 0.814539 mae: 0.676536 (29.480019702527816 steps/sec)\n",
            "Step #34050\tEpoch   0 Batch 2799/3125   Loss: 0.770236 mae: 0.686153 (28.630307673635965 steps/sec)\n",
            "Step #34100\tEpoch   0 Batch 2849/3125   Loss: 0.743572 mae: 0.678276 (28.31784606605587 steps/sec)\n",
            "Step #34150\tEpoch   0 Batch 2899/3125   Loss: 0.808064 mae: 0.676901 (29.038430969422823 steps/sec)\n",
            "Step #34200\tEpoch   0 Batch 2949/3125   Loss: 0.809232 mae: 0.683050 (28.873561142115086 steps/sec)\n",
            "Step #34250\tEpoch   0 Batch 2999/3125   Loss: 0.704819 mae: 0.677378 (28.976185429724232 steps/sec)\n",
            "Step #34300\tEpoch   0 Batch 3049/3125   Loss: 0.722559 mae: 0.672184 (29.693720919926072 steps/sec)\n",
            "Step #34350\tEpoch   0 Batch 3099/3125   Loss: 0.793392 mae: 0.675783 (29.388917383468847 steps/sec)\n",
            "\n",
            "Train time for epoch #1 (34375 total steps): 109.48234486579895\n",
            "Model test set loss: 0.779529 mae: 0.694742\n",
            "best loss = 0.7795291543006897\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #34400\tEpoch   1 Batch   24/3125   Loss: 0.656867 mae: 0.694203 (52.24718877182912 steps/sec)\n",
            "Step #34450\tEpoch   1 Batch   74/3125   Loss: 0.791676 mae: 0.671583 (28.986001619062534 steps/sec)\n",
            "Step #34500\tEpoch   1 Batch  124/3125   Loss: 0.666891 mae: 0.679613 (28.713827409955403 steps/sec)\n",
            "Step #34550\tEpoch   1 Batch  174/3125   Loss: 0.755131 mae: 0.680596 (28.914895855321838 steps/sec)\n",
            "Step #34600\tEpoch   1 Batch  224/3125   Loss: 0.619326 mae: 0.681886 (28.55629069095632 steps/sec)\n",
            "Step #34650\tEpoch   1 Batch  274/3125   Loss: 0.742361 mae: 0.680292 (29.08002167046445 steps/sec)\n",
            "Step #34700\tEpoch   1 Batch  324/3125   Loss: 0.727544 mae: 0.683053 (29.470727514459917 steps/sec)\n",
            "Step #34750\tEpoch   1 Batch  374/3125   Loss: 0.733369 mae: 0.682939 (28.51397355367676 steps/sec)\n",
            "Step #34800\tEpoch   1 Batch  424/3125   Loss: 0.813403 mae: 0.686440 (28.76105873070752 steps/sec)\n",
            "Step #34850\tEpoch   1 Batch  474/3125   Loss: 0.762775 mae: 0.681309 (28.520953697676955 steps/sec)\n",
            "Step #34900\tEpoch   1 Batch  524/3125   Loss: 0.803480 mae: 0.673719 (28.970025173223913 steps/sec)\n",
            "Step #34950\tEpoch   1 Batch  574/3125   Loss: 0.790787 mae: 0.682215 (29.59208496616559 steps/sec)\n",
            "Step #35000\tEpoch   1 Batch  624/3125   Loss: 0.600287 mae: 0.680248 (28.72148790902043 steps/sec)\n",
            "Step #35050\tEpoch   1 Batch  674/3125   Loss: 0.795232 mae: 0.684649 (29.126844115040058 steps/sec)\n",
            "Step #35100\tEpoch   1 Batch  724/3125   Loss: 0.619733 mae: 0.682193 (28.831270243666378 steps/sec)\n",
            "Step #35150\tEpoch   1 Batch  774/3125   Loss: 0.681961 mae: 0.675634 (29.120991658280698 steps/sec)\n",
            "Step #35200\tEpoch   1 Batch  824/3125   Loss: 0.760938 mae: 0.683761 (29.16779613563753 steps/sec)\n",
            "Step #35250\tEpoch   1 Batch  874/3125   Loss: 0.687443 mae: 0.680836 (28.8998220385066 steps/sec)\n",
            "Step #35300\tEpoch   1 Batch  924/3125   Loss: 0.855489 mae: 0.684563 (29.00818962041309 steps/sec)\n",
            "Step #35350\tEpoch   1 Batch  974/3125   Loss: 0.685655 mae: 0.679212 (27.94446264041508 steps/sec)\n",
            "Step #35400\tEpoch   1 Batch 1024/3125   Loss: 0.736902 mae: 0.677340 (28.801601016731322 steps/sec)\n",
            "Step #35450\tEpoch   1 Batch 1074/3125   Loss: 0.770916 mae: 0.682270 (28.588145163269147 steps/sec)\n",
            "Step #35500\tEpoch   1 Batch 1124/3125   Loss: 0.758002 mae: 0.685086 (29.055779173240804 steps/sec)\n",
            "Step #35550\tEpoch   1 Batch 1174/3125   Loss: 0.687778 mae: 0.684140 (29.07613499277586 steps/sec)\n",
            "Step #35600\tEpoch   1 Batch 1224/3125   Loss: 0.740616 mae: 0.675282 (28.83409265398517 steps/sec)\n",
            "Step #35650\tEpoch   1 Batch 1274/3125   Loss: 0.831377 mae: 0.690506 (28.666946890709205 steps/sec)\n",
            "Step #35700\tEpoch   1 Batch 1324/3125   Loss: 0.659279 mae: 0.676212 (28.850768830331255 steps/sec)\n",
            "Step #35750\tEpoch   1 Batch 1374/3125   Loss: 0.717741 mae: 0.673032 (28.79670491805755 steps/sec)\n",
            "Step #35800\tEpoch   1 Batch 1424/3125   Loss: 0.737239 mae: 0.677550 (28.608842069765863 steps/sec)\n",
            "Step #35850\tEpoch   1 Batch 1474/3125   Loss: 0.636832 mae: 0.679203 (28.84451895519713 steps/sec)\n",
            "Step #35900\tEpoch   1 Batch 1524/3125   Loss: 0.656142 mae: 0.675634 (28.84507439000253 steps/sec)\n",
            "Step #35950\tEpoch   1 Batch 1574/3125   Loss: 0.716178 mae: 0.679856 (27.83097247269093 steps/sec)\n",
            "Step #36000\tEpoch   1 Batch 1624/3125   Loss: 0.780242 mae: 0.673168 (28.8123521705106 steps/sec)\n",
            "Step #36050\tEpoch   1 Batch 1674/3125   Loss: 0.617728 mae: 0.678037 (29.12981371049425 steps/sec)\n",
            "Step #36100\tEpoch   1 Batch 1724/3125   Loss: 0.813798 mae: 0.677591 (29.01962965776616 steps/sec)\n",
            "Step #36150\tEpoch   1 Batch 1774/3125   Loss: 0.694775 mae: 0.679510 (28.830449787031693 steps/sec)\n",
            "Step #36200\tEpoch   1 Batch 1824/3125   Loss: 0.681622 mae: 0.674749 (28.926793069107543 steps/sec)\n",
            "Step #36250\tEpoch   1 Batch 1874/3125   Loss: 0.783783 mae: 0.679983 (28.02296353497225 steps/sec)\n",
            "Step #36300\tEpoch   1 Batch 1924/3125   Loss: 0.927751 mae: 0.675321 (29.07704609161143 steps/sec)\n",
            "Step #36350\tEpoch   1 Batch 1974/3125   Loss: 0.726748 mae: 0.672595 (28.35705557458359 steps/sec)\n",
            "Step #36400\tEpoch   1 Batch 2024/3125   Loss: 0.858664 mae: 0.683385 (29.18932085117518 steps/sec)\n",
            "Step #36450\tEpoch   1 Batch 2074/3125   Loss: 0.765740 mae: 0.676498 (29.35541880371551 steps/sec)\n",
            "Step #36500\tEpoch   1 Batch 2124/3125   Loss: 0.628470 mae: 0.677006 (29.23572181766373 steps/sec)\n",
            "Step #36550\tEpoch   1 Batch 2174/3125   Loss: 0.712046 mae: 0.673569 (28.590401764869238 steps/sec)\n",
            "Step #36600\tEpoch   1 Batch 2224/3125   Loss: 0.641456 mae: 0.674773 (28.332912040722942 steps/sec)\n",
            "Step #36650\tEpoch   1 Batch 2274/3125   Loss: 0.761477 mae: 0.676003 (28.855687302422698 steps/sec)\n",
            "Step #36700\tEpoch   1 Batch 2324/3125   Loss: 0.625256 mae: 0.684512 (29.09844106376301 steps/sec)\n",
            "Step #36750\tEpoch   1 Batch 2374/3125   Loss: 0.708039 mae: 0.675594 (28.48063240076095 steps/sec)\n",
            "Step #36800\tEpoch   1 Batch 2424/3125   Loss: 0.704758 mae: 0.677270 (28.220488715986818 steps/sec)\n",
            "Step #36850\tEpoch   1 Batch 2474/3125   Loss: 0.791997 mae: 0.674219 (28.77626844816867 steps/sec)\n",
            "Step #36900\tEpoch   1 Batch 2524/3125   Loss: 0.693096 mae: 0.680855 (28.517308085083933 steps/sec)\n",
            "Step #36950\tEpoch   1 Batch 2574/3125   Loss: 0.804709 mae: 0.685736 (29.033301288724875 steps/sec)\n",
            "Step #37000\tEpoch   1 Batch 2624/3125   Loss: 0.872467 mae: 0.680125 (29.48653559864999 steps/sec)\n",
            "Step #37050\tEpoch   1 Batch 2674/3125   Loss: 0.748870 mae: 0.679264 (28.736856615808264 steps/sec)\n",
            "Step #37100\tEpoch   1 Batch 2724/3125   Loss: 0.692639 mae: 0.670636 (28.252888267858385 steps/sec)\n",
            "Step #37150\tEpoch   1 Batch 2774/3125   Loss: 0.680942 mae: 0.675536 (28.235698349347885 steps/sec)\n",
            "Step #37200\tEpoch   1 Batch 2824/3125   Loss: 0.759505 mae: 0.684927 (28.044863715451786 steps/sec)\n",
            "Step #37250\tEpoch   1 Batch 2874/3125   Loss: 0.803724 mae: 0.669011 (28.412859185941844 steps/sec)\n",
            "Step #37300\tEpoch   1 Batch 2924/3125   Loss: 0.772023 mae: 0.676995 (28.590682403444593 steps/sec)\n",
            "Step #37350\tEpoch   1 Batch 2974/3125   Loss: 0.657636 mae: 0.677880 (28.90975791452375 steps/sec)\n",
            "Step #37400\tEpoch   1 Batch 3024/3125   Loss: 0.704458 mae: 0.669689 (28.926988579426414 steps/sec)\n",
            "Step #37450\tEpoch   1 Batch 3074/3125   Loss: 0.749138 mae: 0.670122 (28.2542167069342 steps/sec)\n",
            "Step #37500\tEpoch   1 Batch 3124/3125   Loss: 0.781000 mae: 0.671016 (28.609255767171096 steps/sec)\n",
            "\n",
            "Train time for epoch #2 (37500 total steps): 108.84279298782349\n",
            "Model test set loss: 0.775686 mae: 0.693323\n",
            "best loss = 0.7756858468055725\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #37550\tEpoch   2 Batch   49/3125   Loss: 0.818890 mae: 0.692263 (28.499698172427937 steps/sec)\n",
            "Step #37600\tEpoch   2 Batch   99/3125   Loss: 0.893166 mae: 0.664643 (28.662903446911937 steps/sec)\n",
            "Step #37650\tEpoch   2 Batch  149/3125   Loss: 0.686463 mae: 0.677543 (28.341385559509217 steps/sec)\n",
            "Step #37700\tEpoch   2 Batch  199/3125   Loss: 0.760674 mae: 0.676744 (29.01491607244819 steps/sec)\n",
            "Step #37750\tEpoch   2 Batch  249/3125   Loss: 0.829983 mae: 0.676314 (29.601382822166595 steps/sec)\n",
            "Step #37800\tEpoch   2 Batch  299/3125   Loss: 0.692605 mae: 0.677180 (29.361119231385253 steps/sec)\n",
            "Step #37850\tEpoch   2 Batch  349/3125   Loss: 0.791070 mae: 0.678037 (29.405326339009164 steps/sec)\n",
            "Step #37900\tEpoch   2 Batch  399/3125   Loss: 0.851649 mae: 0.681585 (28.689806731998623 steps/sec)\n",
            "Step #37950\tEpoch   2 Batch  449/3125   Loss: 0.721144 mae: 0.683555 (28.22610635423605 steps/sec)\n",
            "Step #38000\tEpoch   2 Batch  499/3125   Loss: 0.708574 mae: 0.672300 (28.717326822901153 steps/sec)\n",
            "Step #38050\tEpoch   2 Batch  549/3125   Loss: 0.718362 mae: 0.670210 (29.094561570354276 steps/sec)\n",
            "Step #38100\tEpoch   2 Batch  599/3125   Loss: 0.637871 mae: 0.684454 (29.1177893743739 steps/sec)\n",
            "Step #38150\tEpoch   2 Batch  649/3125   Loss: 0.681373 mae: 0.679157 (28.521178670261406 steps/sec)\n",
            "Step #38200\tEpoch   2 Batch  699/3125   Loss: 0.756526 mae: 0.681801 (29.52135652696349 steps/sec)\n",
            "Step #38250\tEpoch   2 Batch  749/3125   Loss: 0.721114 mae: 0.668248 (28.734927245752004 steps/sec)\n",
            "Step #38300\tEpoch   2 Batch  799/3125   Loss: 0.757268 mae: 0.679608 (28.847466971562888 steps/sec)\n",
            "Step #38350\tEpoch   2 Batch  849/3125   Loss: 0.826510 mae: 0.679275 (29.4044563941925 steps/sec)\n",
            "Step #38400\tEpoch   2 Batch  899/3125   Loss: 0.796476 mae: 0.676612 (29.024208199623363 steps/sec)\n",
            "Step #38450\tEpoch   2 Batch  949/3125   Loss: 0.669277 mae: 0.677410 (28.57761127563282 steps/sec)\n",
            "Step #38500\tEpoch   2 Batch  999/3125   Loss: 0.802085 mae: 0.671089 (29.017569785172086 steps/sec)\n",
            "Step #38550\tEpoch   2 Batch 1049/3125   Loss: 0.828152 mae: 0.685286 (28.947799595257102 steps/sec)\n",
            "Step #38600\tEpoch   2 Batch 1099/3125   Loss: 0.692682 mae: 0.675052 (28.65927237235932 steps/sec)\n",
            "Step #38650\tEpoch   2 Batch 1149/3125   Loss: 0.666188 mae: 0.684372 (29.25799594324109 steps/sec)\n",
            "Step #38700\tEpoch   2 Batch 1199/3125   Loss: 0.780025 mae: 0.672210 (29.00929309047892 steps/sec)\n",
            "Step #38750\tEpoch   2 Batch 1249/3125   Loss: 0.831779 mae: 0.679729 (29.146655562924696 steps/sec)\n",
            "Step #38800\tEpoch   2 Batch 1299/3125   Loss: 0.693816 mae: 0.685327 (29.225238909811765 steps/sec)\n",
            "Step #38850\tEpoch   2 Batch 1349/3125   Loss: 0.781241 mae: 0.661838 (29.263193087633354 steps/sec)\n",
            "Step #38900\tEpoch   2 Batch 1399/3125   Loss: 0.756402 mae: 0.676975 (29.461466015423934 steps/sec)\n",
            "Step #38950\tEpoch   2 Batch 1449/3125   Loss: 0.744262 mae: 0.673399 (28.756062057035166 steps/sec)\n",
            "Step #39000\tEpoch   2 Batch 1499/3125   Loss: 0.782501 mae: 0.674204 (29.642868198306452 steps/sec)\n",
            "Step #39050\tEpoch   2 Batch 1549/3125   Loss: 0.802537 mae: 0.674362 (29.37496647056294 steps/sec)\n",
            "Step #39100\tEpoch   2 Batch 1599/3125   Loss: 0.754370 mae: 0.669181 (28.858740853804537 steps/sec)\n",
            "Step #39150\tEpoch   2 Batch 1649/3125   Loss: 0.771880 mae: 0.676235 (29.41304677723974 steps/sec)\n",
            "Step #39200\tEpoch   2 Batch 1699/3125   Loss: 0.762041 mae: 0.670942 (28.968912683637786 steps/sec)\n",
            "Step #39250\tEpoch   2 Batch 1749/3125   Loss: 0.741043 mae: 0.673756 (28.883586373499543 steps/sec)\n",
            "Step #39300\tEpoch   2 Batch 1799/3125   Loss: 0.818301 mae: 0.673482 (29.204181757997738 steps/sec)\n",
            "Step #39350\tEpoch   2 Batch 1849/3125   Loss: 0.719335 mae: 0.673159 (29.427203189862844 steps/sec)\n",
            "Step #39400\tEpoch   2 Batch 1899/3125   Loss: 0.809684 mae: 0.673814 (29.491241932859623 steps/sec)\n",
            "Step #39450\tEpoch   2 Batch 1949/3125   Loss: 0.718471 mae: 0.669240 (29.981582034999995 steps/sec)\n",
            "Step #39500\tEpoch   2 Batch 1999/3125   Loss: 0.815803 mae: 0.675653 (29.248276157353526 steps/sec)\n",
            "Step #39550\tEpoch   2 Batch 2049/3125   Loss: 0.685496 mae: 0.684661 (29.170867412229455 steps/sec)\n",
            "Step #39600\tEpoch   2 Batch 2099/3125   Loss: 0.749644 mae: 0.665899 (28.77801381875776 steps/sec)\n",
            "Step #39650\tEpoch   2 Batch 2149/3125   Loss: 0.683761 mae: 0.666825 (29.12783526145997 steps/sec)\n",
            "Step #39700\tEpoch   2 Batch 2199/3125   Loss: 0.644422 mae: 0.670934 (28.554529343268918 steps/sec)\n",
            "Step #39750\tEpoch   2 Batch 2249/3125   Loss: 0.675456 mae: 0.669008 (28.54835273514099 steps/sec)\n",
            "Step #39800\tEpoch   2 Batch 2299/3125   Loss: 0.794368 mae: 0.681152 (29.583206082103317 steps/sec)\n",
            "Step #39850\tEpoch   2 Batch 2349/3125   Loss: 0.720175 mae: 0.674663 (29.221264460307594 steps/sec)\n",
            "Step #39900\tEpoch   2 Batch 2399/3125   Loss: 0.664057 mae: 0.675211 (28.56418248479207 steps/sec)\n",
            "Step #39950\tEpoch   2 Batch 2449/3125   Loss: 0.696451 mae: 0.668419 (29.11453524935757 steps/sec)\n",
            "Step #40000\tEpoch   2 Batch 2499/3125   Loss: 0.845362 mae: 0.678860 (29.08597062155418 steps/sec)\n",
            "Step #40050\tEpoch   2 Batch 2549/3125   Loss: 0.584674 mae: 0.680141 (29.446131315733865 steps/sec)\n",
            "Step #40100\tEpoch   2 Batch 2599/3125   Loss: 0.837818 mae: 0.675897 (29.51771659500307 steps/sec)\n",
            "Step #40150\tEpoch   2 Batch 2649/3125   Loss: 0.832998 mae: 0.674071 (29.589959726990678 steps/sec)\n",
            "Step #40200\tEpoch   2 Batch 2699/3125   Loss: 0.741826 mae: 0.672626 (29.14834487067315 steps/sec)\n",
            "Step #40250\tEpoch   2 Batch 2749/3125   Loss: 0.789538 mae: 0.667520 (28.36050307912967 steps/sec)\n",
            "Step #40300\tEpoch   2 Batch 2799/3125   Loss: 0.749359 mae: 0.678092 (28.919565032726247 steps/sec)\n",
            "Step #40350\tEpoch   2 Batch 2849/3125   Loss: 0.754427 mae: 0.671131 (28.992753857609305 steps/sec)\n",
            "Step #40400\tEpoch   2 Batch 2899/3125   Loss: 0.788678 mae: 0.669569 (29.58980525195791 steps/sec)\n",
            "Step #40450\tEpoch   2 Batch 2949/3125   Loss: 0.794243 mae: 0.675611 (29.43454677592263 steps/sec)\n",
            "Step #40500\tEpoch   2 Batch 2999/3125   Loss: 0.689069 mae: 0.669550 (29.369968114116954 steps/sec)\n",
            "Step #40550\tEpoch   2 Batch 3049/3125   Loss: 0.709052 mae: 0.663504 (29.174113850091814 steps/sec)\n",
            "Step #40600\tEpoch   2 Batch 3099/3125   Loss: 0.763072 mae: 0.667701 (29.00409347132952 steps/sec)\n",
            "\n",
            "Train time for epoch #3 (40625 total steps): 107.61566138267517\n",
            "Model test set loss: 0.772572 mae: 0.690479\n",
            "best loss = 0.7725722193717957\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #40650\tEpoch   3 Batch   24/3125   Loss: 0.650026 mae: 0.689870 (55.212431348173 steps/sec)\n",
            "Step #40700\tEpoch   3 Batch   74/3125   Loss: 0.772270 mae: 0.664406 (29.97737351201431 steps/sec)\n",
            "Step #40750\tEpoch   3 Batch  124/3125   Loss: 0.660244 mae: 0.672287 (28.8844655535361 steps/sec)\n",
            "Step #40800\tEpoch   3 Batch  174/3125   Loss: 0.744396 mae: 0.671724 (28.88331984532774 steps/sec)\n",
            "Step #40850\tEpoch   3 Batch  224/3125   Loss: 0.600281 mae: 0.673002 (29.546457415327428 steps/sec)\n",
            "Step #40900\tEpoch   3 Batch  274/3125   Loss: 0.724113 mae: 0.672879 (29.365374417793483 steps/sec)\n",
            "Step #40950\tEpoch   3 Batch  324/3125   Loss: 0.727465 mae: 0.673336 (29.189255847623365 steps/sec)\n",
            "Step #41000\tEpoch   3 Batch  374/3125   Loss: 0.714022 mae: 0.675007 (29.163954907477557 steps/sec)\n",
            "Step #41050\tEpoch   3 Batch  424/3125   Loss: 0.809461 mae: 0.679082 (28.97985320702331 steps/sec)\n",
            "Step #41100\tEpoch   3 Batch  474/3125   Loss: 0.753648 mae: 0.674258 (29.038390761116244 steps/sec)\n",
            "Step #41150\tEpoch   3 Batch  524/3125   Loss: 0.788462 mae: 0.665292 (28.932140630087247 steps/sec)\n",
            "Step #41200\tEpoch   3 Batch  574/3125   Loss: 0.772059 mae: 0.675003 (29.375604243339563 steps/sec)\n",
            "Step #41250\tEpoch   3 Batch  624/3125   Loss: 0.591657 mae: 0.672911 (29.181709278568277 steps/sec)\n",
            "Step #41300\tEpoch   3 Batch  674/3125   Loss: 0.801456 mae: 0.678359 (29.736498700948083 steps/sec)\n",
            "Step #41350\tEpoch   3 Batch  724/3125   Loss: 0.618749 mae: 0.674645 (29.580835941510077 steps/sec)\n",
            "Step #41400\tEpoch   3 Batch  774/3125   Loss: 0.662511 mae: 0.668123 (29.34099884239471 steps/sec)\n",
            "Step #41450\tEpoch   3 Batch  824/3125   Loss: 0.741583 mae: 0.676380 (29.676913020965777 steps/sec)\n",
            "Step #41500\tEpoch   3 Batch  874/3125   Loss: 0.668710 mae: 0.673668 (30.017350689175327 steps/sec)\n",
            "Step #41550\tEpoch   3 Batch  924/3125   Loss: 0.837313 mae: 0.677589 (28.93161775894577 steps/sec)\n",
            "Step #41600\tEpoch   3 Batch  974/3125   Loss: 0.692994 mae: 0.673599 (29.723416513654843 steps/sec)\n",
            "Step #41650\tEpoch   3 Batch 1024/3125   Loss: 0.715608 mae: 0.670647 (29.488558930940748 steps/sec)\n",
            "Step #41700\tEpoch   3 Batch 1074/3125   Loss: 0.744142 mae: 0.674014 (28.983457822796623 steps/sec)\n",
            "Step #41750\tEpoch   3 Batch 1124/3125   Loss: 0.730199 mae: 0.677824 (29.179114777893044 steps/sec)\n",
            "Step #41800\tEpoch   3 Batch 1174/3125   Loss: 0.654803 mae: 0.675822 (29.678328352396186 steps/sec)\n",
            "Step #41850\tEpoch   3 Batch 1224/3125   Loss: 0.742069 mae: 0.667981 (29.52397899768218 steps/sec)\n",
            "Step #41900\tEpoch   3 Batch 1274/3125   Loss: 0.807953 mae: 0.683246 (29.33503950081732 steps/sec)\n",
            "Step #41950\tEpoch   3 Batch 1324/3125   Loss: 0.647996 mae: 0.668622 (29.397972598084966 steps/sec)\n",
            "Step #42000\tEpoch   3 Batch 1374/3125   Loss: 0.719816 mae: 0.666991 (29.462691163462953 steps/sec)\n",
            "Step #42050\tEpoch   3 Batch 1424/3125   Loss: 0.708467 mae: 0.669825 (30.03851297076979 steps/sec)\n",
            "Step #42100\tEpoch   3 Batch 1474/3125   Loss: 0.612224 mae: 0.670123 (30.03011672090884 steps/sec)\n",
            "Step #42150\tEpoch   3 Batch 1524/3125   Loss: 0.633619 mae: 0.667723 (29.392710995847338 steps/sec)\n",
            "Step #42200\tEpoch   3 Batch 1574/3125   Loss: 0.704542 mae: 0.672469 (29.200241493696435 steps/sec)\n",
            "Step #42250\tEpoch   3 Batch 1624/3125   Loss: 0.776105 mae: 0.665823 (30.01107481517766 steps/sec)\n",
            "Step #42300\tEpoch   3 Batch 1674/3125   Loss: 0.609909 mae: 0.670880 (28.876976335676783 steps/sec)\n",
            "Step #42350\tEpoch   3 Batch 1724/3125   Loss: 0.802006 mae: 0.670914 (29.473187737625285 steps/sec)\n",
            "Step #42400\tEpoch   3 Batch 1774/3125   Loss: 0.673236 mae: 0.673222 (30.000165940248923 steps/sec)\n",
            "Step #42450\tEpoch   3 Batch 1824/3125   Loss: 0.673147 mae: 0.667283 (29.24152262972249 steps/sec)\n",
            "Step #42500\tEpoch   3 Batch 1874/3125   Loss: 0.769041 mae: 0.672469 (29.491996744724556 steps/sec)\n",
            "Step #42550\tEpoch   3 Batch 1924/3125   Loss: 0.888911 mae: 0.668867 (28.572339456753443 steps/sec)\n",
            "Step #42600\tEpoch   3 Batch 1974/3125   Loss: 0.692224 mae: 0.665101 (29.16491207770961 steps/sec)\n",
            "Step #42650\tEpoch   3 Batch 2024/3125   Loss: 0.847280 mae: 0.676117 (29.373254907618215 steps/sec)\n",
            "Step #42700\tEpoch   3 Batch 2074/3125   Loss: 0.738112 mae: 0.669166 (30.02946311154285 steps/sec)\n",
            "Step #42750\tEpoch   3 Batch 2124/3125   Loss: 0.629891 mae: 0.669705 (29.97037763654469 steps/sec)\n",
            "Step #42800\tEpoch   3 Batch 2174/3125   Loss: 0.699444 mae: 0.666133 (28.953019020492476 steps/sec)\n",
            "Step #42850\tEpoch   3 Batch 2224/3125   Loss: 0.638445 mae: 0.667504 (28.737758389837463 steps/sec)\n",
            "Step #42900\tEpoch   3 Batch 2274/3125   Loss: 0.735753 mae: 0.667976 (28.975949218251838 steps/sec)\n",
            "Step #42950\tEpoch   3 Batch 2324/3125   Loss: 0.606699 mae: 0.677163 (29.271325238916678 steps/sec)\n",
            "Step #43000\tEpoch   3 Batch 2374/3125   Loss: 0.697473 mae: 0.669035 (30.10325082932249 steps/sec)\n",
            "Step #43050\tEpoch   3 Batch 2424/3125   Loss: 0.691392 mae: 0.669794 (29.516420397647583 steps/sec)\n",
            "Step #43100\tEpoch   3 Batch 2474/3125   Loss: 0.772312 mae: 0.667264 (29.32116427763802 steps/sec)\n",
            "Step #43150\tEpoch   3 Batch 2524/3125   Loss: 0.683168 mae: 0.674688 (29.218276190920697 steps/sec)\n",
            "Step #43200\tEpoch   3 Batch 2574/3125   Loss: 0.808346 mae: 0.677848 (29.122912557884018 steps/sec)\n",
            "Step #43250\tEpoch   3 Batch 2624/3125   Loss: 0.842894 mae: 0.672263 (29.048921098487682 steps/sec)\n",
            "Step #43300\tEpoch   3 Batch 2674/3125   Loss: 0.739222 mae: 0.670842 (29.394597869137222 steps/sec)\n",
            "Step #43350\tEpoch   3 Batch 2724/3125   Loss: 0.684857 mae: 0.663861 (29.290564666351294 steps/sec)\n",
            "Step #43400\tEpoch   3 Batch 2774/3125   Loss: 0.666313 mae: 0.667594 (29.261804824373527 steps/sec)\n",
            "Step #43450\tEpoch   3 Batch 2824/3125   Loss: 0.745545 mae: 0.677200 (28.66140703505226 steps/sec)\n",
            "Step #43500\tEpoch   3 Batch 2874/3125   Loss: 0.775750 mae: 0.662101 (28.98574120995434 steps/sec)\n",
            "Step #43550\tEpoch   3 Batch 2924/3125   Loss: 0.759121 mae: 0.670614 (29.211398166236606 steps/sec)\n",
            "Step #43600\tEpoch   3 Batch 2974/3125   Loss: 0.656138 mae: 0.671077 (29.209571353796843 steps/sec)\n",
            "Step #43650\tEpoch   3 Batch 3024/3125   Loss: 0.684310 mae: 0.661713 (29.246595640383596 steps/sec)\n",
            "Step #43700\tEpoch   3 Batch 3074/3125   Loss: 0.746491 mae: 0.662516 (28.72315582833043 steps/sec)\n",
            "Step #43750\tEpoch   3 Batch 3124/3125   Loss: 0.760295 mae: 0.664025 (29.584963068735686 steps/sec)\n",
            "\n",
            "Train time for epoch #4 (43750 total steps): 106.70000147819519\n",
            "Model test set loss: 0.769468 mae: 0.689286\n",
            "best loss = 0.7694679498672485\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #43800\tEpoch   4 Batch   49/3125   Loss: 0.806481 mae: 0.688069 (27.953249346371905 steps/sec)\n",
            "Step #43850\tEpoch   4 Batch   99/3125   Loss: 0.870856 mae: 0.658374 (28.95175195793258 steps/sec)\n",
            "Step #43900\tEpoch   4 Batch  149/3125   Loss: 0.670646 mae: 0.668902 (28.05566900830465 steps/sec)\n",
            "Step #43950\tEpoch   4 Batch  199/3125   Loss: 0.733976 mae: 0.668527 (29.295945271261907 steps/sec)\n",
            "Step #44000\tEpoch   4 Batch  249/3125   Loss: 0.817494 mae: 0.668741 (29.379353261187056 steps/sec)\n",
            "Step #44050\tEpoch   4 Batch  299/3125   Loss: 0.688788 mae: 0.668236 (29.482742598884798 steps/sec)\n",
            "Step #44100\tEpoch   4 Batch  349/3125   Loss: 0.764797 mae: 0.670697 (29.18063731625497 steps/sec)\n",
            "Step #44150\tEpoch   4 Batch  399/3125   Loss: 0.837799 mae: 0.674177 (29.22118302797297 steps/sec)\n",
            "Step #44200\tEpoch   4 Batch  449/3125   Loss: 0.702992 mae: 0.676687 (29.112954936773104 steps/sec)\n",
            "Step #44250\tEpoch   4 Batch  499/3125   Loss: 0.696394 mae: 0.665319 (28.79415470223757 steps/sec)\n",
            "Step #44300\tEpoch   4 Batch  549/3125   Loss: 0.705966 mae: 0.662851 (29.048361809145458 steps/sec)\n",
            "Step #44350\tEpoch   4 Batch  599/3125   Loss: 0.619457 mae: 0.676588 (29.235913374844298 steps/sec)\n",
            "Step #44400\tEpoch   4 Batch  649/3125   Loss: 0.677127 mae: 0.672583 (28.79699752984434 steps/sec)\n",
            "Step #44450\tEpoch   4 Batch  699/3125   Loss: 0.736797 mae: 0.675036 (29.120069716475502 steps/sec)\n",
            "Step #44500\tEpoch   4 Batch  749/3125   Loss: 0.719294 mae: 0.660685 (29.205552357150356 steps/sec)\n",
            "Step #44550\tEpoch   4 Batch  799/3125   Loss: 0.732524 mae: 0.672690 (29.345860042847168 steps/sec)\n",
            "Step #44600\tEpoch   4 Batch  849/3125   Loss: 0.810730 mae: 0.672607 (29.558459350550322 steps/sec)\n",
            "Step #44650\tEpoch   4 Batch  899/3125   Loss: 0.782637 mae: 0.670492 (28.35850120024708 steps/sec)\n",
            "Step #44700\tEpoch   4 Batch  949/3125   Loss: 0.664281 mae: 0.671084 (28.871025118311685 steps/sec)\n",
            "Step #44750\tEpoch   4 Batch  999/3125   Loss: 0.787286 mae: 0.663454 (28.84565365194306 steps/sec)\n",
            "Step #44800\tEpoch   4 Batch 1049/3125   Loss: 0.817391 mae: 0.678149 (29.016646350490703 steps/sec)\n",
            "Step #44850\tEpoch   4 Batch 1099/3125   Loss: 0.679612 mae: 0.668104 (28.597914644480184 steps/sec)\n",
            "Step #44900\tEpoch   4 Batch 1149/3125   Loss: 0.659032 mae: 0.677912 (28.751642611589624 steps/sec)\n",
            "Step #44950\tEpoch   4 Batch 1199/3125   Loss: 0.762630 mae: 0.664407 (29.841703867671242 steps/sec)\n",
            "Step #45000\tEpoch   4 Batch 1249/3125   Loss: 0.823715 mae: 0.672355 (28.955993255965545 steps/sec)\n",
            "Step #45050\tEpoch   4 Batch 1299/3125   Loss: 0.667723 mae: 0.676605 (29.143269434942262 steps/sec)\n",
            "Step #45100\tEpoch   4 Batch 1349/3125   Loss: 0.773100 mae: 0.656384 (29.457178799560182 steps/sec)\n",
            "Step #45150\tEpoch   4 Batch 1399/3125   Loss: 0.760028 mae: 0.669897 (29.23407126936791 steps/sec)\n",
            "Step #45200\tEpoch   4 Batch 1449/3125   Loss: 0.710901 mae: 0.666300 (29.094339570289577 steps/sec)\n",
            "Step #45250\tEpoch   4 Batch 1499/3125   Loss: 0.780593 mae: 0.666178 (29.528281523661533 steps/sec)\n",
            "Step #45300\tEpoch   4 Batch 1549/3125   Loss: 0.789327 mae: 0.666360 (29.50011133832037 steps/sec)\n",
            "Step #45350\tEpoch   4 Batch 1599/3125   Loss: 0.749513 mae: 0.663347 (28.164892756211103 steps/sec)\n",
            "Step #45400\tEpoch   4 Batch 1649/3125   Loss: 0.749257 mae: 0.668181 (28.61591559994711 steps/sec)\n",
            "Step #45450\tEpoch   4 Batch 1699/3125   Loss: 0.740949 mae: 0.663978 (28.88586996570723 steps/sec)\n",
            "Step #45500\tEpoch   4 Batch 1749/3125   Loss: 0.731029 mae: 0.666943 (29.176528856862422 steps/sec)\n",
            "Step #45550\tEpoch   4 Batch 1799/3125   Loss: 0.818586 mae: 0.666334 (29.749938468762345 steps/sec)\n",
            "Step #45600\tEpoch   4 Batch 1849/3125   Loss: 0.698916 mae: 0.666778 (29.117340625701154 steps/sec)\n",
            "Step #45650\tEpoch   4 Batch 1899/3125   Loss: 0.810042 mae: 0.666438 (29.237641578628985 steps/sec)\n",
            "Step #45700\tEpoch   4 Batch 1949/3125   Loss: 0.694239 mae: 0.662384 (29.015108761415828 steps/sec)\n",
            "Step #45750\tEpoch   4 Batch 1999/3125   Loss: 0.799158 mae: 0.667878 (28.810266207072964 steps/sec)\n",
            "Step #45800\tEpoch   4 Batch 2049/3125   Loss: 0.682145 mae: 0.677614 (29.604826098016634 steps/sec)\n",
            "Step #45850\tEpoch   4 Batch 2099/3125   Loss: 0.722999 mae: 0.659185 (29.01672664682548 steps/sec)\n",
            "Step #45900\tEpoch   4 Batch 2149/3125   Loss: 0.676486 mae: 0.659635 (29.24105782770443 steps/sec)\n",
            "Step #45950\tEpoch   4 Batch 2199/3125   Loss: 0.634527 mae: 0.664197 (28.89592365525542 steps/sec)\n",
            "Step #46000\tEpoch   4 Batch 2249/3125   Loss: 0.664729 mae: 0.661795 (28.392940648297753 steps/sec)\n",
            "Step #46050\tEpoch   4 Batch 2299/3125   Loss: 0.772222 mae: 0.673935 (28.79315055515816 steps/sec)\n",
            "Step #46100\tEpoch   4 Batch 2349/3125   Loss: 0.717633 mae: 0.667296 (29.17651667935482 steps/sec)\n",
            "Step #46150\tEpoch   4 Batch 2399/3125   Loss: 0.637692 mae: 0.668815 (29.029571750031007 steps/sec)\n",
            "Step #46200\tEpoch   4 Batch 2449/3125   Loss: 0.682922 mae: 0.661660 (28.75821510682827 steps/sec)\n",
            "Step #46250\tEpoch   4 Batch 2499/3125   Loss: 0.841615 mae: 0.673258 (29.44670602719291 steps/sec)\n",
            "Step #46300\tEpoch   4 Batch 2549/3125   Loss: 0.564276 mae: 0.672982 (29.396163581087514 steps/sec)\n",
            "Step #46350\tEpoch   4 Batch 2599/3125   Loss: 0.803179 mae: 0.669387 (29.648752078761284 steps/sec)\n",
            "Step #46400\tEpoch   4 Batch 2649/3125   Loss: 0.819673 mae: 0.666014 (28.97134586530668 steps/sec)\n",
            "Step #46450\tEpoch   4 Batch 2699/3125   Loss: 0.725118 mae: 0.665842 (28.912818934772886 steps/sec)\n",
            "Step #46500\tEpoch   4 Batch 2749/3125   Loss: 0.765953 mae: 0.660400 (29.055513483302672 steps/sec)\n",
            "Step #46550\tEpoch   4 Batch 2799/3125   Loss: 0.738602 mae: 0.670209 (29.041326260152648 steps/sec)\n",
            "Step #46600\tEpoch   4 Batch 2849/3125   Loss: 0.724364 mae: 0.663326 (28.776300036705305 steps/sec)\n",
            "Step #46650\tEpoch   4 Batch 2899/3125   Loss: 0.769989 mae: 0.662695 (28.151690234111925 steps/sec)\n",
            "Step #46700\tEpoch   4 Batch 2949/3125   Loss: 0.782344 mae: 0.669298 (29.44409313150124 steps/sec)\n",
            "Step #46750\tEpoch   4 Batch 2999/3125   Loss: 0.675435 mae: 0.662427 (29.163285736793792 steps/sec)\n",
            "Step #46800\tEpoch   4 Batch 3049/3125   Loss: 0.691973 mae: 0.655261 (29.13136348097479 steps/sec)\n",
            "Step #46850\tEpoch   4 Batch 3099/3125   Loss: 0.721221 mae: 0.661415 (29.527487436591606 steps/sec)\n",
            "\n",
            "Train time for epoch #5 (46875 total steps): 107.71804308891296\n",
            "Model test set loss: 0.766407 mae: 0.686921\n",
            "best loss = 0.7664069533348083\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeE5X1jF0mQQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0OkH0KsXOr"
      },
      "source": [
        "train_step:优化的是loss值，解决的其实是一个极小化loss问题。这里的含义是使用Adam下降算法（在tensorflow中已经写好了各种优化算法，这里只需要声明和调用即可），使loss值最小，也就是使网络的输出与样本的输出接近。这里的Loss损失函数，可以是均方误差，自定义函数或者交叉熵。train_step在后面调用sess.run()会话计算时，会喂入输入数据。每喂入一组，就计算一次会话，更新一轮参数，所以train_step的含义我理解应该是每次喂入训练数据后执行的结果，可以翻译成“训练步骤”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koOz8Vbwfzp3"
      },
      "source": [
        "Tensorflow的Checkpoint机制将可追踪变量以二进制的方式储存成一个.ckpt文件，储存了变量的名称及对应张量的值。\n",
        "\n",
        "  Checkpoint 只保存模型的参数，不保存模型的计算过程，因此一般用于在具有模型源代码的时候恢复之前训练好的模型参数。如果需要导出模型（无需源代码也能运行模型）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAY1Qa204rJy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqu5ec9pK_30"
      },
      "source": [
        "指定用户与电影进行评分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knIa8av81ZmU"
      },
      "source": [
        "def rating_movie(mv_net, user_id_val,movie_id_val):\n",
        "  categories = np.zeros([1,18])\n",
        "  categories[0] = movies.values[movieid2idx[movie_id_val]][2]#拿到 类别\n",
        "\n",
        "  titles = np.zeros([1,sentences_size])\n",
        "  titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
        "  inference_val = mv_net.model([np.reshape(users.values[user_id_val-1][0],[1,1]),\n",
        "           np.reshape(users.values[user_id_val-1][1],[1,1]),\n",
        "           np.reshape(users.values[user_id_val-1][2],[1,1]),\n",
        "           np.reshape(users.values[user_id_val-1][3],[1,1]),\n",
        "           np.reshape(movies.values[movieid2idx[movie_id_val]][0],[1, 1]),\n",
        "           categories,\n",
        "           titles])\n",
        "  return (inference_val.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0g52MwJQqOu",
        "outputId": "1bb881d3-c1ad-4deb-ad0e-7d89189413b2"
      },
      "source": [
        "rating_movie(mv_net, 234, 1401)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.9449265]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSyTaXndWuVm"
      },
      "source": [
        "### 生成Movie特征矩阵\n",
        "将训练好的电影特征组合成电影特征矩阵保存到本地\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Jn4O9NSlXFMm",
        "outputId": "9c87857c-cfa9-41c8-cae8-f021a7e7bf20"
      },
      "source": [
        "movie_layer_model = keras.models.Model(inputs=[mv_net.model.input[4],mv_net.model.input[5],mv_net.model.input[6]],\n",
        "                     outputs = mv_net.model.get_layer(\"movie_combine_layer_flat\").output)\n",
        "movie_matrics = []\n",
        "for item in movies.values:\n",
        "  categories = np.zeros([1,18])\n",
        "  categories[0] = item.take(2)\n",
        "\n",
        "  titles = np.zeros([1,sentences_size])\n",
        "  titles[0] = item.take(1)\n",
        "\n",
        "  movie_combine_layer_flat_val = movie_layer_model([np.reshape(item.take(0),[1,1]),categories, titles])\n",
        "  movie_matrics.append(movie_combine_layer_flat_val)\n",
        "  movie_matrics = pickle.load(open('movie_matrics.p',mode='rb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-fa5c8e99f7d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmovie_combine_layer_flat_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmovie_matrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_combine_layer_flat_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmovie_matrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movie_matrics.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movie_matrics.p'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "jPVG_grybhtn",
        "outputId": "92a80aa2-b5e8-47fa-91fe-95378f59480f"
      },
      "source": [
        "movie_matrics = pickle.load(open('movie_matrics.p',mode='rb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-3278ea6afbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovie_matrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movie_matrics.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movie_matrics.p'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "BSKf3LfsdoFY",
        "outputId": "ccbb7bbf-283b-4584-97bd-e5e424a6fcec"
      },
      "source": [
        "#生成用户特征矩阵\n",
        "user_layer_model = keras.models.Model(inputs=[mv_net.model.input[0], mv_net.model.input[1], mv_net.model.input[2], mv_net.model.input[3]], \n",
        "                                 outputs=mv_net.model.get_layer(\"user_combine_layer_flat\").output)\n",
        "users_matrics = []\n",
        "\n",
        "for item in users.values:\n",
        "\n",
        "    user_combine_layer_flat_val = user_layer_model([np.reshape(item.take(0), [1, 1]), \n",
        "                                                    np.reshape(item.take(1), [1, 1]), \n",
        "                                                    np.reshape(item.take(2), [1, 1]), \n",
        "                                                    np.reshape(item.take(3), [1, 1])])  \n",
        "    users_matrics.append(user_combine_layer_flat_val)\n",
        "\n",
        "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
        "users_matrics = pickle.dump(open('users_matrics.p', mode='rb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-c139b0a88f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_matrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'users_matrics.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0musers_matrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'users_matrics.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: dump() missing required argument 'file' (pos 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvdM4Qgd4CO"
      },
      "source": [
        "#推荐 使用保存的用户与电影特征矩阵做电影推荐 计算用户观看的电影的特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，加了一点随机选择，保证每次的推荐有点不同\n",
        "\n",
        "def recommend_same_type_movie(movie_id_val,top_k=20):\n",
        "  norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.sqare(movie_matrics),1,keepdims=True))\n",
        "  normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
        "\n",
        "  #推荐同类型的电影\n",
        "  probs_embedding = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1,200])\n",
        "  probs_similarity =tf.matmul(probs.embeddings, tf.transpose(normalized_movie_matrics))\n",
        "\n",
        "  print(\"您输入的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
        "  print(\"推荐给您的电影是：\")\n",
        "  p = np.squeeze(sim)\n",
        "  p[np.argsort(p)[:-top_k]] = 0\n",
        "  p = p /np.sum(p)\n",
        "  results = set()\n",
        "  while len(results)!=5:\n",
        "    c = np.random.choice(3883,1,p=p)[0]\n",
        "    results.add(c)\n",
        "  for val in (results):\n",
        "    print(val)\n",
        "    print(movie_orig[val])\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aN9StgFZpKA"
      },
      "source": [
        "recommend_same_type_movie(1401,20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCPfa3n_aC_B"
      },
      "source": [
        "### 推荐您喜欢的电影 \n",
        "思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择部分。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tncLHuSPaBEH"
      },
      "source": [
        "def recommend_your_favorite_movie(user_id_val,top_k=10):\n",
        "  probs_embedding = (user_matrics[user_id_val-1]).reshape([1,200])\n",
        "  probs_similarity = tf.matmul(pro_embedding,tf.transpose(movie_matrics))\n",
        "  sim = (probs_similarity_numpy())\n",
        "\n",
        "  print(\"以下是给您的推荐：\")\n",
        "  p = np.squeeze(sim)\n",
        "  p[np.argsort(p)[:-top_k]] = 0\n",
        "  p = p/np.sum(p)\n",
        "  results = set()\n",
        "  while len(results)!=5:\n",
        "    c = np.random.choice(3883,1,p=p)[0]\n",
        "    results.add(c)\n",
        "  for val in (results):\n",
        "    print(val)\n",
        "    print(movies_orig[val])\n",
        "\n",
        "  return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE-ZeNUnP-rW"
      },
      "source": [
        "看过这个电影的人还喜欢那些电影\n",
        "\n",
        "\n",
        "1.   选出喜欢某个电影的top_k个人，得到这几个人的特征向量\n",
        "2.   然后计算这几个人对所有电影的评分\n",
        "\n",
        "\n",
        "3.   每个人的top1作为推荐\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucwtr1sFPxH_"
      },
      "source": [
        "import random\n",
        "\n",
        "def recommend_other_favorite_movie(movie_id_val,top_k=20):\n",
        "  probs_movie_embedding=(movie_matrics[movieid2idx[movie_id_val]]).reshape([1,200])\n",
        "  prods_user_favorite_similarity = tf.matmul(probs_movie_embedding,tf.transpose(users_matrics))\n",
        "  favorite_user_id = np.argsort(probs_user_favorite_similarity.numpy())[0][-top_k:]\n",
        "\n",
        "  print(\"您看的电影是：{}\".format(movie_orig[movieid2idx[movie_id_val]]))\n",
        "  print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
        "  probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
        "    probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
        "    sim = (probs_similarity.numpy())\n",
        "  p = np.argmax(sim, 1)\n",
        "  print(\"喜欢看这个电影的人还喜欢看：\")\n",
        "\n",
        "  if len(set(p)) < 5:\n",
        "\n",
        "    results = set(p)\n",
        "  else:\n",
        "    results = set()\n",
        "    while len(results) != 5:\n",
        "\n",
        "      c = p[random.randrange(top_k)]\n",
        "      results.add(c)\n",
        "  for val in (results):\n",
        "    print(val)\n",
        "    print(movies_orig[val])\n",
        "        \n",
        "  return results\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34eLYyg1UZ5g"
      },
      "source": [
        "recommend_other_favorite_movie(1401,20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnH6rY66UkUN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiWtyTDN_EX"
      },
      "source": [
        "squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉\n",
        "\n",
        "argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y\n",
        "\n",
        "def choice(a, size=None, replace=True, p=None)\n",
        "表示从a中随机选取size个数\n",
        "replacement:代表的意思是抽样之后还放不放回去，如果是False的话，那么通一次挑选出来的数都不一样，如果是True的话， 有可能会出现重复的，因为前面的抽的放回去了。\n",
        "p表示每个元素被抽取的概率，如果没有指定，a中所有元素被选取的概率是相等的。"
      ]
    }
  ]
}