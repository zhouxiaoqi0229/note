{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U9nfyusqdnVjNC3BtvK3a2MWxWd4MciR",
      "authorship_tag": "ABX9TyPC8YQlV9Gyf6+UV4yMceoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouxiaoqi0229/note/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYp8dcOWp9X5",
        "outputId": "45c70b2b-fdf4-4d77-aa43-dc22a5823f80"
      },
      "source": [
        "!pwd\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/movie_recommender\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RUGxoDZqECn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "de78bb57-3653-4526-a18e-773fbe2010fc"
      },
      "source": [
        "import os\n",
        "os.chdir('./drive/MyDrive/Colab Notebooks/movie_recommender')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a4d95d1c7092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/Colab Notebooks/movie_recommender'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDrive/Colab Notebooks/movie_recommender'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjJHmPTq6_w"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split#数据集划分\n",
        "import numpy as np\n",
        "from collections import Counter#用于统计字符出现的次数\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle #提供保存数据在本地的方法\n",
        "import re #正则表达式\n",
        "from tensorflow.python.ops import math_ops \n",
        "from urllib.request import urlretrieve#将url表示的网络对象复制到本地\n",
        "from os.path import isfile,isdir#判断是否存在文件file 文件夹dir\n",
        "from tqdm import tqdm  #Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)\n",
        "import zipfile #用来做zip格式的压缩与解压缩\n",
        "import hashlib #hash 或者MD5加密\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5dSKsHxhAJ6",
        "outputId": "9a6fca5c-d6aa-4b70-e7a9-a0ebe9761646"
      },
      "source": [
        "#数据查看\n",
        "\n",
        "#用户数据\n",
        "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
        "users = pd.read_csv('./users.dat',sep='::', header=None, names=users_title, engine = 'python')\n",
        "users.info\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of       UserID Gender  Age  OccupationID Zip-code\n",
              "0          1      F    1            10    48067\n",
              "1          2      M   56            16    70072\n",
              "2          3      M   25            15    55117\n",
              "3          4      M   45             7    02460\n",
              "4          5      M   25            20    55455\n",
              "...      ...    ...  ...           ...      ...\n",
              "6035    6036      F   25            15    32603\n",
              "6036    6037      F   45             1    76006\n",
              "6037    6038      F   56             1    14706\n",
              "6038    6039      F   45             0    01060\n",
              "6039    6040      M   25             6    11106\n",
              "\n",
              "[6040 rows x 5 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ehhfojiqobYl",
        "outputId": "8bcaf545-a4ac-4306-f17d-0ee2015ca13e"
      },
      "source": [
        "#电影数据\n",
        "movies_title = ['MovieID', 'Title', 'Genres']\n",
        "movies = pd.read_csv('./movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Animation|Children's|Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children's|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MovieID                               Title                        Genres\n",
              "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
              "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
              "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
              "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
              "4        5  Father of the Bride Part II (1995)                        Comedy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "X7wrcMzQooJU",
        "outputId": "61cbbcc5-294f-4567-8978-68c5db3fc820"
      },
      "source": [
        "#评分数据\n",
        "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
        "ratings = pd.read_csv('./ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>timestamps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  MovieID  Rating  timestamps\n",
              "0       1     1193       5   978300760\n",
              "1       1      661       3   978302109\n",
              "2       1      914       3   978301968\n",
              "3       1     3408       4   978300275\n",
              "4       1     2355       5   978824291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QvfhpFNo2Ss"
      },
      "source": [
        "**处理数据**\n",
        "\n",
        "\n",
        "*   userID movieId occuption不变\n",
        "*   性别 0,1 \n",
        "\n",
        "*   年龄 0-6\n",
        "*   类别;是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
        "\n",
        "\n",
        "*   Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉\n",
        "*   Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlo4z-bAoyHc"
      },
      "source": [
        "#数据预处理\n",
        "\n",
        "def load_data():\n",
        "  #修改user数据集\n",
        "  users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
        "  users = pd.read_csv('./users.dat',sep='::', header=None, names=users_title, engine = 'python')\n",
        "  users = users.filter(regex='UserID|Gender|Age|JobID')\n",
        "  users_orig = users.values\n",
        "  users['Gender'] = users['Gender'].map({'F':0, 'M':1})#性别映射\n",
        "\n",
        "  age_map = {val:ii for ii, val in enumerate(set(users['Age']))}\n",
        "  users['Age'] = users['Age'].map(age_map)#年龄映射为0-6\n",
        "  #修改movie数据集\n",
        "  movies_title = ['MovieID', 'Title', 'Genres']\n",
        "  movies = pd.read_csv('./movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
        "  movies_orig = movies.values\n",
        "\n",
        "  #去掉标题中的年份\n",
        "  pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
        "  title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
        "  movies['Title'] = movies['Title'].map(title_map)\n",
        "\n",
        "  #电影类型转换为数字字典\n",
        "  genres_set = set()\n",
        "  for val in movies['Genres'].str.split('|'):\n",
        "    genres_set.update(val)\n",
        "  genres_set.add('<PAD>')\n",
        "\n",
        "  genres2int = {val:ii for ii, val in enumerate(genres_set)} #每种类型到一个数字的映射\n",
        "  #eg: a|b|c:1|2|3这种类型的一个映射\n",
        "  genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))} \n",
        "  #将类型补齐\n",
        "  for key in genres_map:\n",
        "    for cnt in range(max(genres2int.values())-len(genres_map[key])):\n",
        "      genres_map[key].insert(len(genres_map[key])+cnt,genres2int['<PAD>'])\n",
        "  movies['Genres'] = movies['Genres'].map(genres_map)\n",
        "  movies.head(10)\n",
        "\n",
        "  #电影Title转数字字典\n",
        "  title_set = set()#所有电影名称涉及到的单词\n",
        "  for val in movies['Title'].str.split():\n",
        "    title_set.update(val)\n",
        "  title_set.add('<PAD>')\n",
        "\n",
        "  title_count = 15\n",
        "  title2int = {val:ii for ii, val in enumerate(title_set)}#每个单词映射成一个数字\n",
        "  title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
        "  for key in title_map:\n",
        "    for cnt in range(title_count - len(title_map[key])):\n",
        "      title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
        "    \n",
        "  movies['Title'] = movies['Title'].map(title_map)\n",
        "  movies.head(5)\n",
        "\n",
        "  #读取评分数据集\n",
        "  ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
        "  ratings = pd.read_csv('./ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "  ratings = ratings.filter(regex='UserID|MovieID|ratings')#不要时间戳\n",
        "\n",
        "  #合并三个表\n",
        "  data = pd.merge(pd.merge(ratings, users), movies)#1000209 rows × 8 columns\n",
        "  #分成两张表\n",
        "  target_fields = ['ratings']\n",
        "\n",
        "  features_pd,targets_pd = data.drop(target_fields,axis=1),data[target_fields]\n",
        "  features = features_pd.values #将dataframe转变为array\n",
        "  targets_values = targets_pd.values\n",
        "\n",
        "  return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig\n",
        "\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZbf4uicMTDK"
      },
      "source": [
        "\n",
        "\n",
        "*   title_count:title字段的长度（15）\n",
        "\n",
        "*   title_set：title文本的集合\n",
        "\n",
        "*   genres2int：电影类型转数字的字典\n",
        "\n",
        "*   features：是输入X\n",
        "*   ratings：评分数据集的Pandas对象\n",
        "\n",
        "\n",
        "*   movies_orig：没有做数据处理的原始电影数据\n",
        "\n",
        "\n",
        "*   users_orig：没有做数据处理的原始用户数据\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsFFMB9yI6HI"
      },
      "source": [
        "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
        "\n",
        "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "aajMPrEAPHFC",
        "outputId": "b0a44f49-b89e-4717-d9ad-5e3d8fd12eec"
      },
      "source": [
        "#辅助函数\n",
        "def save_params(params):\n",
        "  '''\n",
        "  save params to filr\n",
        "  '''\n",
        "  pickle.dump(params,open('params.p','wb'))\n",
        "def load_params():\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "  return pickle.load(open('params.p', mode='rb'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-0cfb74049660>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    return pickle.load(open('params.p', mode='rb'))\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxHFKIWeSNTk",
        "outputId": "f434f6d3-c2eb-4af4-d435-9e4d918d08fb"
      },
      "source": [
        "import numpy as np\n",
        "student = np.array([('name','S20'), ('age', 'i1'), ('marks', 'f4')]) \n",
        "student.take(1,0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['age', 'i1'], dtype='<U5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2wXPJNZQDJz"
      },
      "source": [
        "#嵌入矩阵的维度\n",
        "embed_dim = 32\n",
        "#用户ID个数\n",
        "uid_max = max(features.take(0,1)) + 1 # 6040+1\n",
        "#性别个数\n",
        "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
        "#年龄类别个数\n",
        "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
        "#职业个数\n",
        "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
        "\n",
        "#电影ID个数\n",
        "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
        "#电影类型个数\n",
        "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
        "#电影名单词个数\n",
        "movie_title_max = len(title_set) # 5216\n",
        "#电影名长度\n",
        "sentences_size = title_count # = 15\n",
        "\n",
        "#对电影类型嵌入向量做加和操作的标志\n",
        "combiner = \"sum\"\n",
        "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
        "window_sizes = {2, 3, 4, 5}\n",
        "#文本卷积核数量\n",
        "filter_num = 8\n",
        "\n",
        "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
        "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}\n",
        "movieid2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR2X9SxJISzC"
      },
      "source": [
        "#超参数设置\n",
        "num_epochs= 5\n",
        "batch_size =256\n",
        "dropout_keep = 0.5\n",
        "learning_rate = 0.0001\n",
        "show_every_n_batches = 20\n",
        "\n",
        "save_dir = './'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4OMV4hCIykJ"
      },
      "source": [
        "# #定义输入的占位符\n",
        "def get_inputs():\n",
        "  \n",
        "  uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='uid')  \n",
        "  user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_gender')  \n",
        "  user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_age') \n",
        "  user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_job')\n",
        "\n",
        "  movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name='movie_id') \n",
        "  movie_categories = tf.keras.layers.Input(shape=(18,), dtype='int32', name='movie_categories') \n",
        "  movie_titles = tf.keras.layers.Input(shape=(15,), dtype='int32', name='movie_titles') \n",
        "  uid,user_gender,user_age,user_job,movie_id,movie_categories,movie_titles"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd86g2fOVMi"
      },
      "source": [
        "keras.layers.Embedding(input_dim,output_dim,input_length)\n",
        "\n",
        "\n",
        "*   input_dim:词汇表的维度（有多少个不同词汇）\n",
        "*   output_dim:嵌入词空间的维度（每个词嵌入为多少维度）\n",
        "\n",
        "\n",
        "*   input_length:输入语句的长度（每一个词汇多少维）\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE792AqWOHA0",
        "outputId": "fc79ba0e-c9a5-4208-9462-0293823f4591"
      },
      "source": [
        "#test  embedding\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[0,0,0],[1,1,1]])\n",
        "emb = keras.layers.Embedding(input_dim=2, output_dim=3, input_length=2) # 只有0,1所以input_dime是2\n",
        "emb(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=\n",
              "array([[[-0.01238128, -0.02503235, -0.01358749],\n",
              "        [-0.01238128, -0.02503235, -0.01358749],\n",
              "        [-0.01238128, -0.02503235, -0.01358749]],\n",
              "\n",
              "       [[ 0.01913581, -0.02758959, -0.02679027],\n",
              "        [ 0.01913581, -0.02758959, -0.02679027],\n",
              "        [ 0.01913581, -0.02758959, -0.02679027]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8KrsOODPn8T"
      },
      "source": [
        "#定义用户的嵌入矩阵\n",
        "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
        "  uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid)\n",
        "  gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
        "  age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
        "  job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
        "  return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xu30O2aylQ"
      },
      "source": [
        "\n",
        "```\n",
        "tf.keras.layers.Dense\n",
        "(\n",
        "    units,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "tf.keras.layers.Dense相当于在全连接层中添加一个层\n",
        "\n",
        "units:输出维度的大小,这一层多少神经元\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2WnRKHzjaEb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93oRMoyjR6X9"
      },
      "source": [
        "#将User的嵌入矩阵一起全连接生成User的特征\n",
        "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
        "    #第一层全连接\n",
        "  uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
        "  gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
        "  age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
        "  job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
        "\n",
        "    #第二层全连接\n",
        "  user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
        "  user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  #(?, 1, 200)\n",
        "\n",
        "  user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)##这一步不太明白\n",
        "  return user_combine_layer, user_combine_layer_flat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGbQBecHnNZz"
      },
      "source": [
        "#定义Movie Id的嵌入矩阵\n",
        "def get_movie_id_embed_layer(movie_id):\n",
        "  get_movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max,embed_dim,input_length=1,name='movie_id_embed_layer')(movie_id)\n",
        "  return movie_id_embed_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-68EPnfhpJjU"
      },
      "source": [
        "#合并电影类型的多个嵌入向量\n",
        "def get_movie_categories_embed_layers(movie_categories):\n",
        "  movie_categories_embed_layer = tf.keras.layers.Embedding(movie_categories_max,embed_dim,input_length=18,name='movie_categories_embed_layer')(movie_categories)\n",
        "  movie_categories_embed_layer = tf.kears.layers.Lambda(lambda layer:tf.reduce_sum(layer,axis=1,keepdims=True))(movie_categories_embed_layer)#每一行求一个和(\n",
        "  return movie_categories_embed_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXvbmvnSQ6H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzpfwH5e8o2b"
      },
      "source": [
        "#title 的文本卷积网络实现\n",
        "def get_movie_cnn_layer(movie_title)\n",
        "  movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max,embed_dim,input_length=15,name='movie_title_embed_layer')(movie_titles)\n",
        "  sp = movie_title_embed_layer.shape\n",
        "  print(sp)\n",
        "  movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1],sp[2],1])(movie_title_embed_layer)\n",
        "  pool_layer_1st = []\n",
        "  for window_size in window_sizes:\n",
        "    conv_layer = tf.keras.layers.Conv2D(filter_num,(window_size,embed_dim),1,activation='relu')(movie_title_embed_layer_expand)\n",
        "    maxpool_layer = tf.keras.layers.MaxPool2D(pool_size=(sentences_size-window_size+1,1),strides=1)(conv_layer)\n",
        "    pool_layer_1st.append(maxpool_later)\n",
        "    #dropout层\n",
        "    print(pool_layer_1st.shape)\n",
        "    pool_layer = tf.keras.layers.concatenate(pool_layer_1st,3,name='pool_layer')\n",
        "    max_num = len(window_sizes)*filter_num\n",
        "\n",
        "    pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
        "\n",
        "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
        "    return pool_layer_flat, dropout_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwbwEtFvVxEY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TjQIf-uEwk_"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "tf.keras.layers.Conv2D(\n",
        "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
        "    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
        "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
        "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
        "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "*   filters:卷积核的个数\n",
        "*   kernel_size:卷积核尺寸\n",
        "\n",
        "\n",
        "*   strides：滑动步长\n",
        "*   padding：补全策略，\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fqiu9BzFwJO"
      },
      "source": [
        "#将movie各个层一起做全连接\n",
        "def get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer):\n",
        "  #第一层全连接\n",
        "  movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
        "  movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim,name=\"movie_categories_fc_layer\",activation='relu')(movie_categories_embed_layer)\n",
        "\n",
        "  #第二层全连接\n",
        "  movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer,movie_categories_fc_layer,dropout_layer],2)\n",
        "  movie_combine_layer_flat = tf.keras.layers.Reshape([200],name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
        "\n",
        "  return movie_combine_layer,movie_combine_layer_flat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Etn4QKoThb"
      },
      "source": [
        "#构建计算图\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "import time\n",
        "\n",
        "MODEL_DIR= \"./models\"\n",
        "\n",
        "class mv_network(object):\n",
        "  def __init__(self,batch_szie = 256):\n",
        "    self.batch_size=batch_size\n",
        "    self.best_loss = 9999\n",
        "    self.losses={'train':[],'test':[]}\n",
        "\n",
        "    #获取占位符\n",
        "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
        "    #d得到用户的4个嵌入向量\n",
        "    uid_embed_layer,gender_embed_layer,age_embed_layer,job_embed_layer = get_user_embedding(uid,user_gender,use_age,user_job)\n",
        "    print(uid_embed_layer)\n",
        "    #得到用户特征\n",
        "    user_combine_layer,gender_combine_layer_flat = get_user_feature_layer(uid_embed_layer,gender_embed_layer,age_embed_layer,job_embed_layer)\n",
        "    \n",
        "    #获取电影ID的嵌入特征\n",
        "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
        "    #获取电影类型的嵌入向量\n",
        "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
        "\n",
        "    #获取电影名的特征向量\n",
        "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
        "    #得到电影特征\n",
        "\n",
        "    movie_combine_layer,movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer)\n",
        "    #计算出评分\n",
        "    #将用户特征与电影特征做矩阵乘法得到预测评分的方案\n",
        "    inference = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer[0] * layer[1],axis=1),name='inference')((user_combine_layer_flat,movie_combine_layer_flat))\n",
        "    inference = tf.keras.layers.Lambda(lambda layer:tf.expand_dims(layer,axis1))(inference)\n",
        "    self.model = tf.keras.Model(\n",
        "        input=[uid,user_gender,user_age,user_job,movie_id,movie_categories,movie_titles],\n",
        "        output=inference\n",
        "        )\n",
        "    self.model.summary()\n",
        "    self.optimizer = tf.kears.optimizer.Adam(learning_rate)\n",
        "    self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
        "    self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
        "    \n",
        "    if tf.io.gfile.exists(MODEL_DIR):\n",
        "      pass\n",
        "    else:\n",
        "      tf.io.gfile.makedirs(MODEL_DIR)\n",
        "    \n",
        "    train_dir = os.path.join(MODEL_DIR,'summaries','train')\n",
        "    test_dir = os.path.join(MODEL_DIR,'summaries','eval')\n",
        "\n",
        "    checkpoint_dir = os.path.join(MODEL_DIR,'checkpoints')\n",
        "    self.checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
        "    self.checkpoint = tf.train.Checkpoint(model=self.model,optimizer=self.optimizer)\n",
        "    self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "  def compute_loss(self,labels,logits):\n",
        "    return tf.reduce_mean(tf.keras.losses.mse(labels,logits))\n",
        "  def compute_metrics(self,labels,logits):\n",
        "    return tf.keras.metrics.mae(labels,logits)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self,x,y):\n",
        "    #记录用于计算损失的操作，以便可以计算损失相对于变量的梯度\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = self.model([x[0],\n",
        "                  x[1],\n",
        "                  x[2],\n",
        "                  x[3],\n",
        "                  x[4],\n",
        "                  x[5],\n",
        "                  x[6]],training=True)\n",
        "      loss = self.ComputeLoss(y,logits)\n",
        "      self.ComputeMetrics(y,logits)\n",
        "    grads = tape.gradient(loss,self.model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads,self.model.trainable_variables))\n",
        "    return loss, logits\n",
        "\n",
        "  def training(self, features, targets_values, epochs=5, log_freq=50):\n",
        "    for epoch_i in range(epochs):\n",
        "      train_X,test_X,train_y,test_y = train_test_split(features,targets_values,test_size=0.2,random_state=0)\n",
        "      train_batches = get_batches(train_X,train_y,self.batch_size)\n",
        "      batch_num = (len(train_X)//self.batch_size)\n",
        "      if True:\n",
        "        start = time.time()#当前时间的时间戳\n",
        "        avg_loss = tf.keras.metrics.Mean('loss',dtype=tf.float32)\n",
        "        for batch_i in range(batch_num):\n",
        "          x, y = next(train_batches)\n",
        "          categories = np.zeros([self.batch_size,18])\n",
        "          for i in range(self.batch_size):\n",
        "            categories[i] = x.take(6,1)[i]\n",
        "\n",
        "          titles = np.zeros([self.batch_size, sentences_size])\n",
        "          for i in range(self.batch_size):\n",
        "            titles[i] = x.take(5,1)[i]\n",
        "          loss,logits = self.train_step([np.reshape(x.take(0,1),[self.batch_size,1]).astype(np.float32),\n",
        "                        np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                         categories.astype(np.float32),\n",
        "                                                    titles.astype(np.float32)], np.reshape(y, [self.batch_size, 1]).astype(np.float32))\n",
        "          \n",
        "\n",
        "          avg_loss(loss)\n",
        "          self.losses['train'].append(loss)\n",
        "          if tf.equal(self.optimizer.iterations % log_freq,0):\n",
        "            rate = log_freq / (time.time()-start)\n",
        "            print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
        "                            self.optimizer.iterations.numpy(),\n",
        "                            epoch_i,\n",
        "                            batch_i,\n",
        "                            batch_num,\n",
        "                            loss, (self.ComputeMetrics.result()), rate))\n",
        "            avg_loss.reset_states()\n",
        "            self.ComputeMetrics.reset_states()\n",
        "                        # avg_mae.reset_states()\n",
        "            start = time.time()\n",
        "        train_end = time.time()\n",
        "        print(\n",
        "                '\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, self.optimizer.iterations.numpy(),\n",
        "                                                                         train_end - train_start))\n",
        "            #             with self.test_summary_writer.as_default():\n",
        "        self.testing((test_X, test_y), self.optimizer.iterations)\n",
        "            # self.checkpoint.save(self.checkpoint_prefix)\n",
        "      self.export_path = os.path.join(MODEL_DIR, 'export')\n",
        "      tf.saved_model.save(self.model, self.export_path)\n",
        "  def testing(self, test_dataset, step_num):\n",
        "      test_X, test_y = test_dataset\n",
        "      test_batches = get_batches(test_X, test_y, self.batch_size)\n",
        "\n",
        "      \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
        "      avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "        #         avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
        "\n",
        "      batch_num = (len(test_X) // self.batch_size)\n",
        "      for batch_i in range(batch_num):\n",
        "          x, y = next(test_batches)\n",
        "          categories = np.zeros([self.batch_size, 18])\n",
        "          for i in range(self.batch_size):\n",
        "              categories[i] = x.take(6, 1)[i]\n",
        "\n",
        "          titles = np.zeros([self.batch_size, sentences_size])\n",
        "          for i in range(self.batch_size):\n",
        "              titles[i] = x.take(5, 1)[i]\n",
        "\n",
        "          logits = self.model([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 categories.astype(np.float32),\n",
        "                                 titles.astype(np.float32)], training=False)\n",
        "          test_loss = self.ComputeLoss(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "          avg_loss(test_loss)\n",
        "            # 保存测试损失\n",
        "          self.losses['test'].append(test_loss)\n",
        "          self.ComputeMetrics(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "            # avg_loss(self.compute_loss(labels, logits))\n",
        "            # avg_mae(self.compute_metrics(labels, logits))\n",
        "\n",
        "      print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), self.ComputeMetrics.result()))\n",
        "        # print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), avg_mae.result()))\n",
        "        #         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "        #         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=step_num)\n",
        "        # summary_ops_v2.scalar('mae', avg_mae.result(), step=step_num)\n",
        "\n",
        "      if avg_loss.result() < self.best_loss:\n",
        "          self.best_loss = avg_loss.result()\n",
        "          print(\"best loss = {}\".format(self.best_loss))\n",
        "          self.checkpoint.save(self.checkpoint_prefix)\n",
        "\n",
        "  def forward(self, xs):\n",
        "      predictions = self.model(xs)\n",
        "        # logits = tf.nn.softmax(predictions)\n",
        "\n",
        "      return predictions\n",
        "  def get_batches(Xs,ys,batch_size):\n",
        "    for start in range(0,len(Xs),batch_size):\n",
        "      end = min(strat+batch_size,len(Xs))\n",
        "      yield Xs[start:end],ys[start:end]\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ng-l60Tn1N2f",
        "outputId": "2b87ed5b-11a5-4dd0-d6c4-17473b65a377"
      },
      "source": [
        "mv_net=mv_network()\n",
        "mv_net.training(features,targets_values,epochs=5)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-86f33c074e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmv_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmv_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmv_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-efe8229a334e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_szie)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#获取占位符\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_gender\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_age\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_job\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmovie_categories\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmovie_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m#d得到用户的4个嵌入向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0muid_embed_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgender_embed_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage_embed_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjob_embed_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_gender\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_age\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeE5X1jF0mQQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0OkH0KsXOr"
      },
      "source": [
        "train_step:优化的是loss值，解决的其实是一个极小化loss问题。这里的含义是使用Adam下降算法（在tensorflow中已经写好了各种优化算法，这里只需要声明和调用即可），使loss值最小，也就是使网络的输出与样本的输出接近。这里的Loss损失函数，可以是均方误差，自定义函数或者交叉熵。train_step在后面调用sess.run()会话计算时，会喂入输入数据。每喂入一组，就计算一次会话，更新一轮参数，所以train_step的含义我理解应该是每次喂入训练数据后执行的结果，可以翻译成“训练步骤”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koOz8Vbwfzp3"
      },
      "source": [
        "Tensorflow的Checkpoint机制将可追踪变量以二进制的方式储存成一个.ckpt文件，储存了变量的名称及对应张量的值。\n",
        "\n",
        "  Checkpoint 只保存模型的参数，不保存模型的计算过程，因此一般用于在具有模型源代码的时候恢复之前训练好的模型参数。如果需要导出模型（无需源代码也能运行模型）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAY1Qa204rJy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "knIa8av81ZmU",
        "outputId": "c59e1851-6b12-4a25-b78c-afa07e804995"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c79aea2e9ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_combine_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgender_combine_layer_flat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'user_combine_layer' is not defined"
          ]
        }
      ]
    }
  ]
}