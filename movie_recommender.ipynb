{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U9nfyusqdnVjNC3BtvK3a2MWxWd4MciR",
      "authorship_tag": "ABX9TyPb7I/vDtm/xeBWfqPEHFpA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouxiaoqi0229/note/blob/main/movie_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYp8dcOWp9X5",
        "outputId": "fc626730-bcf6-4f57-de7c-b526848bc728"
      },
      "source": [
        " !pwd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RUGxoDZqECn"
      },
      "source": [
        "import os\n",
        "os.chdir('./drive/MyDrive/Colab Notebooks/movie_recommender')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjJHmPTq6_w"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split#数据集划分\n",
        "import numpy as np\n",
        "from collections import Counter#用于统计字符出现的次数\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle #提供保存数据在本地的方法\n",
        "import re #正则表达式\n",
        "from tensorflow.python.ops import math_ops \n",
        "from urllib.request import urlretrieve#将url表示的网络对象复制到本地\n",
        "from os.path import isfile,isdir#判断是否存在文件file 文件夹dir\n",
        "from tqdm import tqdm  #Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)\n",
        "import zipfile #用来做zip格式的压缩与解压缩\n",
        "import hashlib #hash 或者MD5加密\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5dSKsHxhAJ6"
      },
      "source": [
        "#数据查看\n",
        "\n",
        "#用户数据\n",
        "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
        "users = pd.read_csv('./users.dat',sep='::', header=None, names=users_title, engine = 'python')\n",
        "users.info\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ehhfojiqobYl",
        "outputId": "8bcaf545-a4ac-4306-f17d-0ee2015ca13e"
      },
      "source": [
        "#电影数据\n",
        "movies_title = ['MovieID', 'Title', 'Genres']\n",
        "movies = pd.read_csv('./movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Animation|Children's|Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children's|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MovieID                               Title                        Genres\n",
              "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
              "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
              "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
              "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
              "4        5  Father of the Bride Part II (1995)                        Comedy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "X7wrcMzQooJU",
        "outputId": "61cbbcc5-294f-4567-8978-68c5db3fc820"
      },
      "source": [
        "#评分数据\n",
        "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
        "ratings = pd.read_csv('./ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>timestamps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  MovieID  Rating  timestamps\n",
              "0       1     1193       5   978300760\n",
              "1       1      661       3   978302109\n",
              "2       1      914       3   978301968\n",
              "3       1     3408       4   978300275\n",
              "4       1     2355       5   978824291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QvfhpFNo2Ss"
      },
      "source": [
        "**处理数据**\n",
        "\n",
        "\n",
        "*   userID movieId occuption不变\n",
        "*   性别 0,1 \n",
        "\n",
        "*   年龄 0-6\n",
        "*   类别;是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
        "\n",
        "\n",
        "*   Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉\n",
        "*   Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlo4z-bAoyHc"
      },
      "source": [
        "#数据预处理\n",
        "\n",
        "def load_data():\n",
        "  #修改user数据集\n",
        "  users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
        "  users = pd.read_csv('./users.dat',sep='::', header=None, names=users_title, engine = 'python')\n",
        "  users = users.filter(regex='UserID|Gender|Age|JobID')\n",
        "  users_orig = users.values\n",
        "  users['Gender'] = users['Gender'].map({'F':0, 'M':1})#性别映射\n",
        "\n",
        "  age_map = {val:ii for ii, val in enumerate(set(users['Age']))}\n",
        "  users['Age'] = users['Age'].map(age_map)#年龄映射为0-6\n",
        "  #修改movie数据集\n",
        "  movies_title = ['MovieID', 'Title', 'Genres']\n",
        "  movies = pd.read_csv('./movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
        "  movies_orig = movies.values\n",
        "\n",
        "  #去掉标题中的年份\n",
        "  pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
        "  title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
        "  movies['Title'] = movies['Title'].map(title_map)\n",
        "\n",
        "  #电影类型转换为数字字典\n",
        "  genres_set = set()\n",
        "  for val in movies['Genres'].str.split('|'):\n",
        "    genres_set.update(val)\n",
        "  genres_set.add('<PAD>')\n",
        "\n",
        "  genres2int = {val:ii for ii, val in enumerate(genres_set)} #每种类型到一个数字的映射\n",
        "  #eg: a|b|c:1|2|3这种类型的一个映射\n",
        "  genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))} \n",
        "  #将类型补齐\n",
        "  for key in genres_map:\n",
        "    for cnt in range(max(genres2int.values())-len(genres_map[key])):\n",
        "      genres_map[key].insert(len(genres_map[key])+cnt,genres2int['<PAD>'])\n",
        "  movies['Genres'] = movies['Genres'].map(genres_map)\n",
        "  movies.head(10)\n",
        "\n",
        "  #电影Title转数字字典\n",
        "  title_set = set()#所有电影名称涉及到的单词\n",
        "  for val in movies['Title'].str.split():\n",
        "    title_set.update(val)\n",
        "  title_set.add('<PAD>')\n",
        "\n",
        "  title_count = 15\n",
        "  title2int = {val:ii for ii, val in enumerate(title_set)}#每个单词映射成一个数字\n",
        "  title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
        "  for key in title_map:\n",
        "    for cnt in range(title_count - len(title_map[key])):\n",
        "      title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
        "    \n",
        "  movies['Title'] = movies['Title'].map(title_map)\n",
        "  movies.head(5)\n",
        "\n",
        "  #读取评分数据集\n",
        "  ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
        "  ratings = pd.read_csv('./ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "  ratings = ratings.filter(regex='UserID|MovieID|ratings')#不要时间戳\n",
        "\n",
        "  #合并三个表\n",
        "  data = pd.merge(pd.merge(ratings, users), movies)#1000209 rows × 8 columns\n",
        "  #分成两张表\n",
        "  target_fields = ['ratings']\n",
        "\n",
        "  features_pd,targets_pd = data.drop(target_fields,axis=1),data[target_fields]# 一个表没有评分 另一个表只有评分\n",
        "  features = features_pd.values #将dataframe转变为array\n",
        "  targets_values = targets_pd.values\n",
        "\n",
        "  return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZbf4uicMTDK"
      },
      "source": [
        "\n",
        "\n",
        "*   title_count:title字段的长度（15）\n",
        "\n",
        "*   title_set：title文本的集合\n",
        "\n",
        "*   genres2int：电影类型转数字的字典\n",
        "\n",
        "*   features：是输入X\n",
        "*   ratings：评分数据集的Pandas对象\n",
        "\n",
        "\n",
        "*   movies_orig：没有做数据处理的原始电影数据\n",
        "\n",
        "\n",
        "*   users_orig：没有做数据处理的原始用户数据\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsFFMB9yI6HI"
      },
      "source": [
        "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
        "\n",
        "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aajMPrEAPHFC"
      },
      "source": [
        "#辅助函数\n",
        "def save_params(params):\n",
        "  '''\n",
        "  save params to filr\n",
        "  '''\n",
        "  pickle.dump(params,open('params.p','wb'))\n",
        "def load_params():\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "  return pickle.load(open('params.p', mode='rb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxHFKIWeSNTk",
        "outputId": "f434f6d3-c2eb-4af4-d435-9e4d918d08fb"
      },
      "source": [
        "import numpy as np\n",
        "student = np.array([('name','S20'), ('age', 'i1'), ('marks', 'f4')]) \n",
        "student.take(1,0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['age', 'i1'], dtype='<U5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2wXPJNZQDJz"
      },
      "source": [
        "#嵌入矩阵的维度\n",
        "embed_dim = 32\n",
        "#用户ID个数\n",
        "uid_max = max(features.take(0,1)) + 1 # 6040+1\n",
        "#性别个数\n",
        "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
        "#年龄类别个数\n",
        "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
        "#职业个数\n",
        "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
        "\n",
        "#电影ID个数\n",
        "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
        "#电影类型个数\n",
        "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
        "#电影名单词个数\n",
        "movie_title_max = len(title_set) # 5216\n",
        "#电影名长度\n",
        "sentences_size = title_count # = 15\n",
        "\n",
        "#对电影类型嵌入向量做加和操作的标志\n",
        "combiner = \"sum\"\n",
        "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
        "window_sizes = {2, 3, 4, 5}\n",
        "#文本卷积核数量\n",
        "filter_num = 8\n",
        "\n",
        "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
        "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}\n",
        "movieid2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR2X9SxJISzC"
      },
      "source": [
        "#超参数设置\n",
        "num_epochs= 5\n",
        "batch_size =256\n",
        "dropout_keep = 0.5\n",
        "learning_rate = 0.0001\n",
        "show_every_n_batches = 20\n",
        "\n",
        "save_dir = './'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4OMV4hCIykJ"
      },
      "source": [
        "# #定义输入的占位符\n",
        "def get_inputs():\n",
        "  uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='uid')  \n",
        "  user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_gender')  \n",
        "  user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_age') \n",
        "  user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_job')\n",
        "\n",
        "  movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name='movie_id') \n",
        "  movie_categories = tf.keras.layers.Input(shape=(18,), dtype='int32', name='movie_categories') \n",
        "  movie_titles = tf.keras.layers.Input(shape=(15,), dtype='int32', name='movie_titles') \n",
        "  return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hjQwCIPPXT"
      },
      "source": [
        "tf.keras.Input函数用于向模型中输入数据，并指定数据的形状、数据类型等信息"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd86g2fOVMi"
      },
      "source": [
        "keras.layers.Embedding(input_dim,output_dim,input_length)\n",
        "\n",
        "\n",
        "*   input_dim:词汇表的维度（有多少个不同词汇）\n",
        "*   output_dim:嵌入词空间的维度（每个词嵌入为多少维度）\n",
        "\n",
        "\n",
        "*   input_length:输入语句的长度（每一个词汇多少维）\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE792AqWOHA0",
        "outputId": "40302825-ebf2-4260-abb6-3b00a96866d5"
      },
      "source": [
        "#test  embedding\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[0,1,2],[3,4,5]])\n",
        "print(len(data))\n",
        "emb = keras.layers.Embedding(input_dim=6, output_dim=4, input_length=3) # 只有0,1所以input_dim是2\n",
        "emb(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
              "array([[[ 0.0439744 , -0.02778723,  0.01964632, -0.02765288],\n",
              "        [-0.00518662,  0.03885773, -0.01417477,  0.03181381],\n",
              "        [-0.04332082, -0.01849406, -0.04606277, -0.0005514 ]],\n",
              "\n",
              "       [[-0.0499388 ,  0.00722983, -0.04274812, -0.0004092 ],\n",
              "        [-0.0242069 ,  0.03826846, -0.00732299, -0.02551873],\n",
              "        [-0.03418063, -0.00546832,  0.02819309, -0.04676614]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8KrsOODPn8T"
      },
      "source": [
        "#定义用户的嵌入矩阵\n",
        "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
        "  uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid) \n",
        "  gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
        "  age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
        "  job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
        "  return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xu30O2aylQ"
      },
      "source": [
        "\n",
        "```\n",
        "tf.keras.layers.Dense\n",
        "(\n",
        "    units,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "tf.keras.layers.Dense相当于在全连接层中添加一个层\n",
        "\n",
        "units:输出维度的大小,这一层多少神经元\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2WnRKHzjaEb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93oRMoyjR6X9"
      },
      "source": [
        "#将User的嵌入矩阵一起全连接生成User的特征\n",
        "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
        "    #第一层全连接\n",
        "  uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
        "  gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
        "  age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
        "  job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
        "\n",
        "    #第二层全连接\n",
        "  user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
        "  user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  #(?, 1, 200)\n",
        "\n",
        "  user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)##这一步不太明白\n",
        "  return user_combine_layer, user_combine_layer_flat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGbQBecHnNZz"
      },
      "source": [
        "#定义Movie Id的嵌入矩阵\n",
        "def get_movie_id_embed_layer(movie_id):\n",
        "  movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max,embed_dim,input_length=1,name='movie_id_embed_layer')(movie_id)\n",
        "  return movie_id_embed_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-68EPnfhpJjU"
      },
      "source": [
        "#合并电影类型的多个嵌入向量\n",
        "def get_movie_categories_embed_layers(movie_categories):\n",
        "  movie_categories_embed_layer = tf.keras.layers.Embedding(movie_categories_max,embed_dim,input_length=18,name='movie_categories_embed_layer')(movie_categories)\n",
        "  movie_categories_embed_layer = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer,axis=1,keepdims=True))(movie_categories_embed_layer)#每一行求一个和(\n",
        "  return movie_categories_embed_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXvbmvnSQ6H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYXUerzCPf0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzpfwH5e8o2b"
      },
      "source": [
        "#title 的文本卷积网络实现\n",
        "def get_movie_cnn_layer(movie_titles):\n",
        "\n",
        "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
        "  movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max, embed_dim, input_length=15, name='movie_title_embed_layer')(movie_titles)\n",
        "  sp=movie_title_embed_layer.shape\n",
        "  movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1], sp[2], 1])(movie_title_embed_layer)\n",
        "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
        "  pool_layer_lst = []\n",
        "  for window_size in window_sizes:\n",
        "      conv_layer = tf.keras.layers.Conv2D(filter_num, (window_size, embed_dim), 1, activation='relu')(movie_title_embed_layer_expand)\n",
        "      maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(sentences_size - window_size + 1 ,1), strides=1)(conv_layer)\n",
        "      pool_layer_lst.append(maxpool_layer)\n",
        "    #Dropout层\n",
        "  pool_layer = tf.keras.layers.concatenate(pool_layer_lst, 3, name =\"pool_layer\")  \n",
        "  max_num = len(window_sizes) * filter_num\n",
        "  pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
        "\n",
        "  dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
        "  return pool_layer_flat, dropout_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwbwEtFvVxEY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TjQIf-uEwk_"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "tf.keras.layers.Conv2D(\n",
        "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
        "    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
        "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
        "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
        "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "*   filters:卷积核的个数\n",
        "*   kernel_size:卷积核尺寸\n",
        "\n",
        "\n",
        "*   strides：滑动步长\n",
        "*   padding：补全策略，\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fqiu9BzFwJO"
      },
      "source": [
        "#将movie各个层一起做全连接\n",
        "def get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer):\n",
        "  #第一层全连接\n",
        "  movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
        "  movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim,name=\"movie_categories_fc_layer\",activation='relu')(movie_categories_embed_layer)\n",
        "\n",
        "  #第二层全连接\n",
        "  movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer,movie_categories_fc_layer,dropout_layer], 2)\n",
        "  movie_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(movie_combine_layer)\n",
        "  movie_combine_layer_flat = tf.keras.layers.Reshape([200],name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
        "\n",
        "  return movie_combine_layer,movie_combine_layer_flat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghyJC686FqEE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuPAQNdfLu2p"
      },
      "source": [
        "def get_batches(Xs,ys,batch_size):\n",
        "    for start in range(0,len(Xs),batch_size):\n",
        "      end = min(start+batch_size,len(Xs))\n",
        "      yield Xs[start:end],ys[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Etn4QKoThb"
      },
      "source": [
        "#构建计算图\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "import time\n",
        "\n",
        "MODEL_DIR= \"./models\"\n",
        "\n",
        "class mv_network(object):\n",
        "  def __init__(self,batch_szie = 256):\n",
        "    self.batch_size=batch_size\n",
        "    self.best_loss = 9999\n",
        "    self.losses={'train':[],'test':[]}\n",
        "\n",
        "    #获取占位符\n",
        "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
        "    #d得到用户的4个嵌入向量\n",
        "    uid_embed_layer,gender_embed_layer,age_embed_layer,job_embed_layer = get_user_embedding(uid,user_gender,user_age,user_job)\n",
        "    print(uid_embed_layer)\n",
        "    #得到用户特征\n",
        "    user_combine_layer,user_combine_layer_flat = get_user_feature_layer(uid_embed_layer,gender_embed_layer,age_embed_layer,job_embed_layer)\n",
        "    \n",
        "    #获取电影ID的嵌入特征\n",
        "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
        "    #获取电影类型的嵌入向量\n",
        "    movie_categories_embed_layer = get_movie_categories_embed_layers(movie_categories)\n",
        "\n",
        "    #获取电影名的特征向量\n",
        "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
        "    #得到电影特征\n",
        "\n",
        "    movie_combine_layer,movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer)\n",
        "    #计算出评分\n",
        "    #将用户特征与电影特征做矩阵乘法得到预测评分的方案\n",
        "    inference = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer[0] * layer[1],axis=1),name='inference')((user_combine_layer_flat,movie_combine_layer_flat))\n",
        "    inference = tf.keras.layers.Lambda(lambda layer:tf.expand_dims(layer,axis=1))(inference)\n",
        "    self.model = tf.keras.Model(\n",
        "        inputs=[uid,user_gender,user_age,user_job,movie_id,movie_categories,movie_titles],\n",
        "        outputs=inference\n",
        "        )\n",
        "    self.model.summary()\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
        "    self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
        "    \n",
        "    if tf.io.gfile.exists(MODEL_DIR):\n",
        "      pass\n",
        "    else:\n",
        "      tf.io.gfile.makedirs(MODEL_DIR)\n",
        "    \n",
        "    train_dir = os.path.join(MODEL_DIR,'summaries','train')\n",
        "    test_dir = os.path.join(MODEL_DIR,'summaries','eval')\n",
        "\n",
        "    checkpoint_dir = os.path.join(MODEL_DIR,'checkpoints')\n",
        "    self.checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
        "    self.checkpoint = tf.train.Checkpoint(model=self.model,optimizer=self.optimizer)\n",
        "    self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "  def compute_loss(self,labels,logits):\n",
        "    return tf.reduce_mean(tf.keras.losses.mse(labels,logits))\n",
        "  def compute_metrics(self,labels,logits):\n",
        "    return tf.keras.metrics.mae(labels,logits)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self,x,y):\n",
        "    #记录用于计算损失的操作，以便可以计算损失相对于变量的梯度\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = self.model([x[0],\n",
        "                  x[1],\n",
        "                  x[2],\n",
        "                  x[3],\n",
        "                  x[4],\n",
        "                  x[5],\n",
        "                  x[6]],training=True)\n",
        "      loss = self.ComputeLoss(y,logits)\n",
        "      self.ComputeMetrics(y,logits)\n",
        "    grads = tape.gradient(loss,self.model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads,self.model.trainable_variables))\n",
        "    return loss, logits\n",
        "\n",
        "  def training(self, features, targets_values, epochs=5, log_freq=50):\n",
        "    for epoch_i in range(epochs):\n",
        "      train_X,test_X,train_y,test_y = train_test_split(features,targets_values,test_size=0.2,random_state=0)\n",
        "      train_batches = get_batches(train_X,train_y,self.batch_size)\n",
        "      batch_num = (len(train_X)//self.batch_size)\n",
        "      train_start = time.time()\n",
        "      if True:\n",
        "        start = time.time()#当前时间的时间戳\n",
        "        avg_loss = tf.keras.metrics.Mean('loss',dtype=tf.float32)\n",
        "        for batch_i in range(batch_num):\n",
        "          x, y = next(train_batches)\n",
        "          categories = np.zeros([self.batch_size,18])\n",
        "          for i in range(self.batch_size):\n",
        "            categories[i] = x.take(6,1)[i]\n",
        "\n",
        "          titles = np.zeros([self.batch_size, sentences_size])\n",
        "          for i in range(self.batch_size):\n",
        "            titles[i] = x.take(5,1)[i]\n",
        "          loss,logits = self.train_step([np.reshape(x.take(0,1),[self.batch_size,1]).astype(np.float32),\n",
        "                        np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                         categories.astype(np.float32),\n",
        "                                                    titles.astype(np.float32)], np.reshape(y, [self.batch_size, 1]).astype(np.float32))\n",
        "          \n",
        "\n",
        "          avg_loss(loss)\n",
        "          self.losses['train'].append(loss)\n",
        "          if tf.equal(self.optimizer.iterations % log_freq,0):\n",
        "            rate = log_freq / (time.time()-start)\n",
        "            print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
        "                            self.optimizer.iterations.numpy(),\n",
        "                            epoch_i,\n",
        "                            batch_i,\n",
        "                            batch_num,\n",
        "                            loss, (self.ComputeMetrics.result()), rate))\n",
        "            avg_loss.reset_states()\n",
        "            self.ComputeMetrics.reset_states()\n",
        "                        # avg_mae.reset_states()\n",
        "            start = time.time()\n",
        "        train_end = time.time()\n",
        "        print(\n",
        "                '\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, self.optimizer.iterations.numpy(),\n",
        "                                                                         train_end - train_start))\n",
        "            #             with self.test_summary_writer.as_default():\n",
        "        self.testing((test_X, test_y), self.optimizer.iterations)\n",
        "            # self.checkpoint.save(self.checkpoint_prefix)\n",
        "      self.export_path = os.path.join(MODEL_DIR, 'export')\n",
        "      tf.saved_model.save(self.model, self.export_path)\n",
        "  def testing(self, test_dataset, step_num):\n",
        "      test_X, test_y = test_dataset\n",
        "      test_batches = get_batches(test_X, test_y, self.batch_size)\n",
        "\n",
        "      \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
        "      avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "        #         avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
        "\n",
        "      batch_num = (len(test_X) // self.batch_size)\n",
        "      for batch_i in range(batch_num):\n",
        "          x, y = next(test_batches)\n",
        "          categories = np.zeros([self.batch_size, 18])\n",
        "          for i in range(self.batch_size):\n",
        "              categories[i] = x.take(6, 1)[i]\n",
        "\n",
        "          titles = np.zeros([self.batch_size, sentences_size])\n",
        "          for i in range(self.batch_size):\n",
        "              titles[i] = x.take(5, 1)[i]\n",
        "\n",
        "          logits = self.model([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 categories.astype(np.float32),\n",
        "                                 titles.astype(np.float32)], training=False)\n",
        "          test_loss = self.ComputeLoss(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "          avg_loss(test_loss)\n",
        "            # 保存测试损失\n",
        "          self.losses['test'].append(test_loss)\n",
        "          self.ComputeMetrics(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "            # avg_loss(self.compute_loss(labels, logits))\n",
        "            # avg_mae(self.compute_metrics(labels, logits))\n",
        "\n",
        "      print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), self.ComputeMetrics.result()))\n",
        "        # print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), avg_mae.result()))\n",
        "        #         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "        #         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=step_num)\n",
        "        # summary_ops_v2.scalar('mae', avg_mae.result(), step=step_num)\n",
        "\n",
        "      if avg_loss.result() < self.best_loss:\n",
        "          self.best_loss = avg_loss.result()\n",
        "          print(\"best loss = {}\".format(self.best_loss))\n",
        "          self.checkpoint.save(self.checkpoint_prefix)\n",
        "\n",
        "  def forward(self, xs):\n",
        "      predictions = self.model(xs)\n",
        "        # logits = tf.nn.softmax(predictions)\n",
        "\n",
        "      return predictions\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElrCf6zpKAbY"
      },
      "source": [
        "使用keras.layers.Reshape实现keras不同层的对接,改变了数据的维度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng-l60Tn1N2f",
        "outputId": "0cfce6e1-5acb-4333-9cac-6434816164e8"
      },
      "source": [
        "mv_net=mv_network()\n",
        "mv_net.training(features,targets_values,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 32), dtype=tf.float32, name=None), name='uid_embed_layer/embedding_lookup/Identity_1:0', description=\"created by layer 'uid_embed_layer'\")\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "movie_titles (InputLayer)       [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "movie_title_embed_layer (Embedd (None, 15, 32)       166880      movie_titles[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_15 (Reshape)            (None, 15, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 14, 1, 8)     520         reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 13, 1, 8)     776         reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 12, 1, 8)     1032        reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 11, 1, 8)     1288        reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "movie_categories (InputLayer)   [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "uid (InputLayer)                [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_age (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_job (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "movie_categories_embed_layer (E (None, 18, 32)       608         movie_categories[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d_48[0][0]           \n",
            "                                                                 max_pooling2d_49[0][0]           \n",
            "                                                                 max_pooling2d_50[0][0]           \n",
            "                                                                 max_pooling2d_51[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 1, 32)        0           movie_categories_embed_layer[0][0\n",
            "__________________________________________________________________________________________________\n",
            "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "movie_categories_fc_layer (Dens (None, 1, 32)        1056        lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
            "                                                                 gender_fc_layer[0][0]            \n",
            "                                                                 age_fc_layer[0][0]               \n",
            "                                                                 job_fc_layer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
            "                                                                 movie_categories_fc_layer[0][0]  \n",
            "                                                                 dropout_layer[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 1, 200)       25800       concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 1, 200)       19400       concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "user_combine_layer_flat (Reshap (None, 200)          0           dense_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "movie_combine_layer_flat (Resha (None, 200)          0           dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "inference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n",
            "                                                                 movie_combine_layer_flat[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 1)            0           inference[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 541,392\n",
            "Trainable params: 541,392\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Step #50\tEpoch   0 Batch   49/3125   Loss: 10.732363 mae: 3.375007 (15.44586634644902 steps/sec)\n",
            "Step #100\tEpoch   0 Batch   99/3125   Loss: 1.876218 mae: 2.102433 (29.905492942843722 steps/sec)\n",
            "Step #150\tEpoch   0 Batch  149/3125   Loss: 1.323200 mae: 0.954181 (29.93347741849107 steps/sec)\n",
            "Step #200\tEpoch   0 Batch  199/3125   Loss: 1.304790 mae: 0.927811 (29.158744298849765 steps/sec)\n",
            "Step #250\tEpoch   0 Batch  249/3125   Loss: 1.205232 mae: 0.914910 (29.06884019852981 steps/sec)\n",
            "Step #300\tEpoch   0 Batch  299/3125   Loss: 1.320238 mae: 0.911525 (29.051118227680536 steps/sec)\n",
            "Step #350\tEpoch   0 Batch  349/3125   Loss: 1.196252 mae: 0.904295 (29.135471372757795 steps/sec)\n",
            "Step #400\tEpoch   0 Batch  399/3125   Loss: 1.273553 mae: 0.896916 (27.549992649926715 steps/sec)\n",
            "Step #450\tEpoch   0 Batch  449/3125   Loss: 1.097474 mae: 0.891020 (29.05455945493884 steps/sec)\n",
            "Step #500\tEpoch   0 Batch  499/3125   Loss: 1.032184 mae: 0.874077 (28.0026529155165 steps/sec)\n",
            "Step #550\tEpoch   0 Batch  549/3125   Loss: 1.044900 mae: 0.856724 (28.13372491423659 steps/sec)\n",
            "Step #600\tEpoch   0 Batch  599/3125   Loss: 0.914006 mae: 0.855713 (28.459815400516067 steps/sec)\n",
            "Step #650\tEpoch   0 Batch  649/3125   Loss: 1.030447 mae: 0.836441 (28.441508790664376 steps/sec)\n",
            "Step #700\tEpoch   0 Batch  699/3125   Loss: 0.906392 mae: 0.818075 (27.964114653894157 steps/sec)\n",
            "Step #750\tEpoch   0 Batch  749/3125   Loss: 0.945585 mae: 0.800212 (27.71581415547696 steps/sec)\n",
            "Step #800\tEpoch   0 Batch  799/3125   Loss: 1.035302 mae: 0.795483 (27.32735432787269 steps/sec)\n",
            "Step #850\tEpoch   0 Batch  849/3125   Loss: 1.047623 mae: 0.790712 (27.557519030655353 steps/sec)\n",
            "Step #900\tEpoch   0 Batch  899/3125   Loss: 0.982896 mae: 0.777566 (28.137771435349 steps/sec)\n",
            "Step #950\tEpoch   0 Batch  949/3125   Loss: 0.795788 mae: 0.770605 (28.378878334750144 steps/sec)\n",
            "Step #1000\tEpoch   0 Batch  999/3125   Loss: 0.953227 mae: 0.769272 (27.210409841851117 steps/sec)\n",
            "Step #1050\tEpoch   0 Batch 1049/3125   Loss: 0.969817 mae: 0.768517 (27.340179746371398 steps/sec)\n",
            "Step #1100\tEpoch   0 Batch 1099/3125   Loss: 0.886979 mae: 0.760927 (27.38385848337325 steps/sec)\n",
            "Step #1150\tEpoch   0 Batch 1149/3125   Loss: 0.835967 mae: 0.765109 (27.56444084714621 steps/sec)\n",
            "Step #1200\tEpoch   0 Batch 1199/3125   Loss: 0.830912 mae: 0.750066 (27.277630907204895 steps/sec)\n",
            "Step #1250\tEpoch   0 Batch 1249/3125   Loss: 0.984715 mae: 0.756905 (27.330986974607352 steps/sec)\n",
            "Step #1300\tEpoch   0 Batch 1299/3125   Loss: 0.871327 mae: 0.760769 (26.860750232853146 steps/sec)\n",
            "Step #1350\tEpoch   0 Batch 1349/3125   Loss: 0.911125 mae: 0.742533 (27.5261121443046 steps/sec)\n",
            "Step #1400\tEpoch   0 Batch 1399/3125   Loss: 0.874284 mae: 0.747581 (27.814347316733382 steps/sec)\n",
            "Step #1450\tEpoch   0 Batch 1449/3125   Loss: 0.871747 mae: 0.746962 (27.805470744188586 steps/sec)\n",
            "Step #1500\tEpoch   0 Batch 1499/3125   Loss: 0.878116 mae: 0.747051 (27.722269693149805 steps/sec)\n",
            "Step #1550\tEpoch   0 Batch 1549/3125   Loss: 0.934915 mae: 0.743993 (28.0369938499556 steps/sec)\n",
            "Step #1600\tEpoch   0 Batch 1599/3125   Loss: 0.886070 mae: 0.742021 (27.656919221405353 steps/sec)\n",
            "Step #1650\tEpoch   0 Batch 1649/3125   Loss: 0.877589 mae: 0.741529 (27.96987687877731 steps/sec)\n",
            "Step #1700\tEpoch   0 Batch 1699/3125   Loss: 0.921259 mae: 0.732394 (27.751011674032217 steps/sec)\n",
            "Step #1750\tEpoch   0 Batch 1749/3125   Loss: 0.873354 mae: 0.735495 (28.259912520551634 steps/sec)\n",
            "Step #1800\tEpoch   0 Batch 1799/3125   Loss: 0.992584 mae: 0.740694 (27.78969351395546 steps/sec)\n",
            "Step #1850\tEpoch   0 Batch 1849/3125   Loss: 0.788619 mae: 0.738983 (27.595909890423098 steps/sec)\n",
            "Step #1900\tEpoch   0 Batch 1899/3125   Loss: 0.891424 mae: 0.736113 (28.095284473945554 steps/sec)\n",
            "Step #1950\tEpoch   0 Batch 1949/3125   Loss: 0.836087 mae: 0.729934 (27.687371631729462 steps/sec)\n",
            "Step #2000\tEpoch   0 Batch 1999/3125   Loss: 0.948203 mae: 0.738057 (27.800708422593168 steps/sec)\n",
            "Step #2050\tEpoch   0 Batch 2049/3125   Loss: 0.825769 mae: 0.741974 (26.717153373245214 steps/sec)\n",
            "Step #2100\tEpoch   0 Batch 2099/3125   Loss: 0.884306 mae: 0.727127 (26.989539576641132 steps/sec)\n",
            "Step #2150\tEpoch   0 Batch 2149/3125   Loss: 0.786471 mae: 0.726279 (27.214449371037517 steps/sec)\n",
            "Step #2200\tEpoch   0 Batch 2199/3125   Loss: 0.767315 mae: 0.727479 (27.695979091513586 steps/sec)\n",
            "Step #2250\tEpoch   0 Batch 2249/3125   Loss: 0.824589 mae: 0.729274 (27.78527158763763 steps/sec)\n",
            "Step #2300\tEpoch   0 Batch 2299/3125   Loss: 0.882396 mae: 0.738325 (27.37853532188662 steps/sec)\n",
            "Step #2350\tEpoch   0 Batch 2349/3125   Loss: 0.833198 mae: 0.732010 (27.75682967714013 steps/sec)\n",
            "Step #2400\tEpoch   0 Batch 2399/3125   Loss: 0.783197 mae: 0.734980 (27.839477287646446 steps/sec)\n",
            "Step #2450\tEpoch   0 Batch 2449/3125   Loss: 0.843739 mae: 0.724308 (27.90595058915611 steps/sec)\n",
            "Step #2500\tEpoch   0 Batch 2499/3125   Loss: 0.926208 mae: 0.732859 (27.39482234889722 steps/sec)\n",
            "Step #2550\tEpoch   0 Batch 2549/3125   Loss: 0.699700 mae: 0.737158 (27.50025701763036 steps/sec)\n",
            "Step #2600\tEpoch   0 Batch 2599/3125   Loss: 1.013382 mae: 0.733900 (28.590978639028997 steps/sec)\n",
            "Step #2650\tEpoch   0 Batch 2649/3125   Loss: 0.919145 mae: 0.729380 (28.535168849419385 steps/sec)\n",
            "Step #2700\tEpoch   0 Batch 2699/3125   Loss: 0.804273 mae: 0.729046 (28.283359571429592 steps/sec)\n",
            "Step #2750\tEpoch   0 Batch 2749/3125   Loss: 1.007716 mae: 0.727109 (27.417457470722187 steps/sec)\n",
            "Step #2800\tEpoch   0 Batch 2799/3125   Loss: 0.850810 mae: 0.730245 (27.356107131117174 steps/sec)\n",
            "Step #2850\tEpoch   0 Batch 2849/3125   Loss: 0.855654 mae: 0.726638 (27.914293225251487 steps/sec)\n",
            "Step #2900\tEpoch   0 Batch 2899/3125   Loss: 0.877684 mae: 0.725208 (28.20066444932545 steps/sec)\n",
            "Step #2950\tEpoch   0 Batch 2949/3125   Loss: 0.887763 mae: 0.729349 (28.31575080110754 steps/sec)\n",
            "Step #3000\tEpoch   0 Batch 2999/3125   Loss: 0.762028 mae: 0.725877 (27.680329489517742 steps/sec)\n",
            "Step #3050\tEpoch   0 Batch 3049/3125   Loss: 0.807910 mae: 0.717544 (27.376187210829794 steps/sec)\n",
            "Step #3100\tEpoch   0 Batch 3099/3125   Loss: 0.901056 mae: 0.724026 (27.95831007531399 steps/sec)\n",
            "Model test set loss: 0.845677 mae: 0.729943\n",
            "best loss = 0.8456774353981018\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #3150\tEpoch   1 Batch   24/3125   Loss: 0.745030 mae: 0.729810 (50.40022859905931 steps/sec)\n",
            "Step #3200\tEpoch   1 Batch   74/3125   Loss: 0.899267 mae: 0.721507 (26.42457778865877 steps/sec)\n",
            "Step #3250\tEpoch   1 Batch  124/3125   Loss: 0.733254 mae: 0.729355 (26.7418154553175 steps/sec)\n",
            "Step #3300\tEpoch   1 Batch  174/3125   Loss: 0.760179 mae: 0.726484 (27.534760608057898 steps/sec)\n",
            "Step #3350\tEpoch   1 Batch  224/3125   Loss: 0.615787 mae: 0.722644 (27.237172864287018 steps/sec)\n",
            "Step #3400\tEpoch   1 Batch  274/3125   Loss: 0.852740 mae: 0.721864 (27.368259504854414 steps/sec)\n",
            "Step #3450\tEpoch   1 Batch  324/3125   Loss: 0.825671 mae: 0.724753 (27.67764075310457 steps/sec)\n",
            "Step #3500\tEpoch   1 Batch  374/3125   Loss: 0.822463 mae: 0.726558 (26.282300322884172 steps/sec)\n",
            "Step #3550\tEpoch   1 Batch  424/3125   Loss: 0.884801 mae: 0.727767 (26.882621318104114 steps/sec)\n",
            "Step #3600\tEpoch   1 Batch  474/3125   Loss: 0.804894 mae: 0.723367 (27.65637942382631 steps/sec)\n",
            "Step #3650\tEpoch   1 Batch  524/3125   Loss: 0.841993 mae: 0.718049 (27.729087542463287 steps/sec)\n",
            "Step #3700\tEpoch   1 Batch  574/3125   Loss: 0.912964 mae: 0.730162 (27.719191763251644 steps/sec)\n",
            "Step #3750\tEpoch   1 Batch  624/3125   Loss: 0.665331 mae: 0.723382 (27.38302894935003 steps/sec)\n",
            "Step #3800\tEpoch   1 Batch  674/3125   Loss: 0.853861 mae: 0.724968 (27.391315818964493 steps/sec)\n",
            "Step #3850\tEpoch   1 Batch  724/3125   Loss: 0.677279 mae: 0.724539 (27.71553944050386 steps/sec)\n",
            "Step #3900\tEpoch   1 Batch  774/3125   Loss: 0.780273 mae: 0.716815 (27.69374078345289 steps/sec)\n",
            "Step #3950\tEpoch   1 Batch  824/3125   Loss: 0.859692 mae: 0.721191 (27.5447096253311 steps/sec)\n",
            "Step #4000\tEpoch   1 Batch  874/3125   Loss: 0.786947 mae: 0.722269 (27.50075106883882 steps/sec)\n",
            "Step #4050\tEpoch   1 Batch  924/3125   Loss: 0.936044 mae: 0.726860 (27.091885864227944 steps/sec)\n",
            "Step #4100\tEpoch   1 Batch  974/3125   Loss: 0.760482 mae: 0.723797 (26.988466323293245 steps/sec)\n",
            "Step #4150\tEpoch   1 Batch 1024/3125   Loss: 0.826785 mae: 0.721742 (27.396665426046056 steps/sec)\n",
            "Step #4200\tEpoch   1 Batch 1074/3125   Loss: 0.866811 mae: 0.721422 (27.35675303442931 steps/sec)\n",
            "Step #4250\tEpoch   1 Batch 1124/3125   Loss: 0.853188 mae: 0.725106 (27.72181528906589 steps/sec)\n",
            "Step #4300\tEpoch   1 Batch 1174/3125   Loss: 0.768554 mae: 0.726413 (27.37006686060549 steps/sec)\n",
            "Step #4350\tEpoch   1 Batch 1224/3125   Loss: 0.766650 mae: 0.716942 (27.169524247787795 steps/sec)\n",
            "Step #4400\tEpoch   1 Batch 1274/3125   Loss: 0.952498 mae: 0.734438 (27.967814139672637 steps/sec)\n",
            "Step #4450\tEpoch   1 Batch 1324/3125   Loss: 0.797243 mae: 0.720313 (27.751962809507237 steps/sec)\n",
            "Step #4500\tEpoch   1 Batch 1374/3125   Loss: 0.807215 mae: 0.717982 (27.557247444700607 steps/sec)\n",
            "Step #4550\tEpoch   1 Batch 1424/3125   Loss: 0.833770 mae: 0.721152 (27.3723460407342 steps/sec)\n",
            "Step #4600\tEpoch   1 Batch 1474/3125   Loss: 0.728266 mae: 0.722154 (26.950696394330148 steps/sec)\n",
            "Step #4650\tEpoch   1 Batch 1524/3125   Loss: 0.743646 mae: 0.718190 (26.933141171957505 steps/sec)\n",
            "Step #4700\tEpoch   1 Batch 1574/3125   Loss: 0.795841 mae: 0.726766 (27.687945539632878 steps/sec)\n",
            "Step #4750\tEpoch   1 Batch 1624/3125   Loss: 0.908455 mae: 0.714823 (27.80965570905968 steps/sec)\n",
            "Step #4800\tEpoch   1 Batch 1674/3125   Loss: 0.793833 mae: 0.721142 (26.73283317537418 steps/sec)\n",
            "Step #4850\tEpoch   1 Batch 1724/3125   Loss: 0.927513 mae: 0.718689 (27.025266083571477 steps/sec)\n",
            "Step #4900\tEpoch   1 Batch 1774/3125   Loss: 0.764403 mae: 0.719703 (27.48821775486953 steps/sec)\n",
            "Step #4950\tEpoch   1 Batch 1824/3125   Loss: 0.735780 mae: 0.722993 (27.595790058761484 steps/sec)\n",
            "Step #5000\tEpoch   1 Batch 1874/3125   Loss: 0.911900 mae: 0.721419 (27.72197652751957 steps/sec)\n",
            "Step #5050\tEpoch   1 Batch 1924/3125   Loss: 1.018376 mae: 0.717877 (27.07693928555889 steps/sec)\n",
            "Step #5100\tEpoch   1 Batch 1974/3125   Loss: 0.830373 mae: 0.715766 (27.55295709696703 steps/sec)\n",
            "Step #5150\tEpoch   1 Batch 2024/3125   Loss: 0.868403 mae: 0.726085 (27.82436654068551 steps/sec)\n",
            "Step #5200\tEpoch   1 Batch 2074/3125   Loss: 0.900907 mae: 0.715918 (27.914449279315 steps/sec)\n",
            "Step #5250\tEpoch   1 Batch 2124/3125   Loss: 0.671295 mae: 0.719687 (27.168524624144162 steps/sec)\n",
            "Step #5300\tEpoch   1 Batch 2174/3125   Loss: 0.743141 mae: 0.711318 (26.902840320958283 steps/sec)\n",
            "Step #5350\tEpoch   1 Batch 2224/3125   Loss: 0.727200 mae: 0.717642 (27.364663362702437 steps/sec)\n",
            "Step #5400\tEpoch   1 Batch 2274/3125   Loss: 0.847238 mae: 0.718085 (27.331820482704657 steps/sec)\n",
            "Step #5450\tEpoch   1 Batch 2324/3125   Loss: 0.694357 mae: 0.727544 (27.543949904903428 steps/sec)\n",
            "Step #5500\tEpoch   1 Batch 2374/3125   Loss: 0.774975 mae: 0.716722 (27.36471692293078 steps/sec)\n",
            "Step #5550\tEpoch   1 Batch 2424/3125   Loss: 0.783944 mae: 0.720272 (27.421089015583647 steps/sec)\n",
            "Step #5600\tEpoch   1 Batch 2474/3125   Loss: 0.884337 mae: 0.713271 (27.60548889502592 steps/sec)\n",
            "Step #5650\tEpoch   1 Batch 2524/3125   Loss: 0.792663 mae: 0.723577 (27.802385369872855 steps/sec)\n",
            "Step #5700\tEpoch   1 Batch 2574/3125   Loss: 0.891674 mae: 0.725060 (27.563879293416928 steps/sec)\n",
            "Step #5750\tEpoch   1 Batch 2624/3125   Loss: 0.967582 mae: 0.724084 (26.991279887265197 steps/sec)\n",
            "Step #5800\tEpoch   1 Batch 2674/3125   Loss: 0.836558 mae: 0.720168 (26.975645985808004 steps/sec)\n",
            "Step #5850\tEpoch   1 Batch 2724/3125   Loss: 0.809321 mae: 0.712858 (26.527920225181305 steps/sec)\n",
            "Step #5900\tEpoch   1 Batch 2774/3125   Loss: 0.781399 mae: 0.718820 (27.963559069703685 steps/sec)\n",
            "Step #5950\tEpoch   1 Batch 2824/3125   Loss: 0.864671 mae: 0.726411 (27.788621960483653 steps/sec)\n",
            "Step #6000\tEpoch   1 Batch 2874/3125   Loss: 0.865648 mae: 0.710878 (27.873018826547227 steps/sec)\n",
            "Step #6050\tEpoch   1 Batch 2924/3125   Loss: 0.836761 mae: 0.717261 (27.77756511592021 steps/sec)\n",
            "Step #6100\tEpoch   1 Batch 2974/3125   Loss: 0.753366 mae: 0.720601 (27.23972362711742 steps/sec)\n",
            "Step #6150\tEpoch   1 Batch 3024/3125   Loss: 0.717766 mae: 0.711610 (27.21098180019582 steps/sec)\n",
            "Step #6200\tEpoch   1 Batch 3074/3125   Loss: 0.831273 mae: 0.711193 (28.000454490409304 steps/sec)\n",
            "Step #6250\tEpoch   1 Batch 3124/3125   Loss: 0.866824 mae: 0.712924 (27.30379052519077 steps/sec)\n",
            "\n",
            "Train time for epoch #2 (6250 total steps): 114.31458520889282\n",
            "Model test set loss: 0.832610 mae: 0.722382\n",
            "best loss = 0.8326102495193481\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #6300\tEpoch   2 Batch   49/3125   Loss: 0.918764 mae: 0.722110 (28.611422019675373 steps/sec)\n",
            "Step #6350\tEpoch   2 Batch   99/3125   Loss: 1.009746 mae: 0.710719 (30.483021301101875 steps/sec)\n",
            "Step #6400\tEpoch   2 Batch  149/3125   Loss: 0.784740 mae: 0.722915 (30.45596846518127 steps/sec)\n",
            "Step #6450\tEpoch   2 Batch  199/3125   Loss: 0.870824 mae: 0.717847 (29.9816806194564 steps/sec)\n",
            "Step #6500\tEpoch   2 Batch  249/3125   Loss: 0.819365 mae: 0.710587 (30.04248475887747 steps/sec)\n",
            "Step #6550\tEpoch   2 Batch  299/3125   Loss: 0.771316 mae: 0.717095 (29.79881361859118 steps/sec)\n",
            "Step #6600\tEpoch   2 Batch  349/3125   Loss: 0.894090 mae: 0.718769 (29.796836393551164 steps/sec)\n",
            "Step #6650\tEpoch   2 Batch  399/3125   Loss: 0.921634 mae: 0.717659 (29.910295584151232 steps/sec)\n",
            "Step #6700\tEpoch   2 Batch  449/3125   Loss: 0.804501 mae: 0.723711 (30.043384258694186 steps/sec)\n",
            "Step #6750\tEpoch   2 Batch  499/3125   Loss: 0.796351 mae: 0.710047 (30.641785645195885 steps/sec)\n",
            "Step #6800\tEpoch   2 Batch  549/3125   Loss: 0.795436 mae: 0.711437 (30.010422034438935 steps/sec)\n",
            "Step #6850\tEpoch   2 Batch  599/3125   Loss: 0.724536 mae: 0.725157 (29.6400109166834 steps/sec)\n",
            "Step #6900\tEpoch   2 Batch  649/3125   Loss: 0.778256 mae: 0.719233 (30.17599951652833 steps/sec)\n",
            "Step #6950\tEpoch   2 Batch  699/3125   Loss: 0.776572 mae: 0.719284 (30.169509568810124 steps/sec)\n",
            "Step #7000\tEpoch   2 Batch  749/3125   Loss: 0.778859 mae: 0.706838 (30.40595165325635 steps/sec)\n",
            "Step #7050\tEpoch   2 Batch  799/3125   Loss: 0.901348 mae: 0.716820 (30.717060281340263 steps/sec)\n",
            "Step #7100\tEpoch   2 Batch  849/3125   Loss: 0.950031 mae: 0.715315 (30.308787596178625 steps/sec)\n",
            "Step #7150\tEpoch   2 Batch  899/3125   Loss: 0.886854 mae: 0.714470 (30.268096433487397 steps/sec)\n",
            "Step #7200\tEpoch   2 Batch  949/3125   Loss: 0.710792 mae: 0.714612 (29.16623031752449 steps/sec)\n",
            "Step #7250\tEpoch   2 Batch  999/3125   Loss: 0.866311 mae: 0.714063 (29.826802324928153 steps/sec)\n",
            "Step #7300\tEpoch   2 Batch 1049/3125   Loss: 0.883134 mae: 0.720488 (30.666721064346167 steps/sec)\n",
            "Step #7350\tEpoch   2 Batch 1099/3125   Loss: 0.788034 mae: 0.713610 (30.310548592728452 steps/sec)\n",
            "Step #7400\tEpoch   2 Batch 1149/3125   Loss: 0.744971 mae: 0.719280 (30.072304525905118 steps/sec)\n",
            "Step #7450\tEpoch   2 Batch 1199/3125   Loss: 0.830666 mae: 0.709681 (30.088389176942055 steps/sec)\n",
            "Step #7500\tEpoch   2 Batch 1249/3125   Loss: 0.918040 mae: 0.720904 (29.63077251305701 steps/sec)\n",
            "Step #7550\tEpoch   2 Batch 1299/3125   Loss: 0.802281 mae: 0.722904 (30.0463887202255 steps/sec)\n",
            "Step #7600\tEpoch   2 Batch 1349/3125   Loss: 0.842637 mae: 0.703083 (30.690174485215856 steps/sec)\n",
            "Step #7650\tEpoch   2 Batch 1399/3125   Loss: 0.827659 mae: 0.716704 (30.1293600898672 steps/sec)\n",
            "Step #7700\tEpoch   2 Batch 1449/3125   Loss: 0.825852 mae: 0.715403 (30.084180827107254 steps/sec)\n",
            "Step #7750\tEpoch   2 Batch 1499/3125   Loss: 0.854958 mae: 0.713323 (30.370447713685216 steps/sec)\n",
            "Step #7800\tEpoch   2 Batch 1549/3125   Loss: 0.909224 mae: 0.714456 (30.377160811870365 steps/sec)\n",
            "Step #7850\tEpoch   2 Batch 1599/3125   Loss: 0.843094 mae: 0.711885 (30.253364018244184 steps/sec)\n",
            "Step #7900\tEpoch   2 Batch 1649/3125   Loss: 0.871100 mae: 0.714918 (30.543204360152412 steps/sec)\n",
            "Step #7950\tEpoch   2 Batch 1699/3125   Loss: 0.865660 mae: 0.709368 (30.06279041030656 steps/sec)\n",
            "Step #8000\tEpoch   2 Batch 1749/3125   Loss: 0.820275 mae: 0.710628 (30.295748573624383 steps/sec)\n",
            "Step #8050\tEpoch   2 Batch 1799/3125   Loss: 0.927799 mae: 0.714772 (30.165122296537188 steps/sec)\n",
            "Step #8100\tEpoch   2 Batch 1849/3125   Loss: 0.775373 mae: 0.714601 (30.130992069672402 steps/sec)\n",
            "Step #8150\tEpoch   2 Batch 1899/3125   Loss: 0.874537 mae: 0.711205 (30.220936936775956 steps/sec)\n",
            "Step #8200\tEpoch   2 Batch 1949/3125   Loss: 0.813032 mae: 0.706602 (30.635031168191077 steps/sec)\n",
            "Step #8250\tEpoch   2 Batch 1999/3125   Loss: 0.904692 mae: 0.715226 (30.356696452287494 steps/sec)\n",
            "Step #8300\tEpoch   2 Batch 2049/3125   Loss: 0.775116 mae: 0.719873 (29.993361035255163 steps/sec)\n",
            "Step #8350\tEpoch   2 Batch 2099/3125   Loss: 0.856619 mae: 0.705517 (30.33937564441183 steps/sec)\n",
            "Step #8400\tEpoch   2 Batch 2149/3125   Loss: 0.740387 mae: 0.703259 (30.747752549114765 steps/sec)\n",
            "Step #8450\tEpoch   2 Batch 2199/3125   Loss: 0.756076 mae: 0.707565 (30.47551730511905 steps/sec)\n",
            "Step #8500\tEpoch   2 Batch 2249/3125   Loss: 0.759456 mae: 0.708095 (30.328726844025038 steps/sec)\n",
            "Step #8550\tEpoch   2 Batch 2299/3125   Loss: 0.883302 mae: 0.719209 (30.17968201238558 steps/sec)\n",
            "Step #8600\tEpoch   2 Batch 2349/3125   Loss: 0.810310 mae: 0.712863 (30.25121692098975 steps/sec)\n",
            "Step #8650\tEpoch   2 Batch 2399/3125   Loss: 0.734924 mae: 0.712993 (30.25408414959582 steps/sec)\n",
            "Step #8700\tEpoch   2 Batch 2449/3125   Loss: 0.790329 mae: 0.704068 (30.363632072733076 steps/sec)\n",
            "Step #8750\tEpoch   2 Batch 2499/3125   Loss: 0.904383 mae: 0.714065 (30.5553264841134 steps/sec)\n",
            "Step #8800\tEpoch   2 Batch 2549/3125   Loss: 0.652089 mae: 0.717240 (29.582329753477293 steps/sec)\n",
            "Step #8850\tEpoch   2 Batch 2599/3125   Loss: 0.951573 mae: 0.714303 (30.056706602737613 steps/sec)\n",
            "Step #8900\tEpoch   2 Batch 2649/3125   Loss: 0.883311 mae: 0.711593 (30.242810390313082 steps/sec)\n",
            "Step #8950\tEpoch   2 Batch 2699/3125   Loss: 0.753789 mae: 0.707853 (30.40306879119955 steps/sec)\n",
            "Step #9000\tEpoch   2 Batch 2749/3125   Loss: 0.963351 mae: 0.708432 (29.87512192425717 steps/sec)\n",
            "Step #9050\tEpoch   2 Batch 2799/3125   Loss: 0.824991 mae: 0.714133 (30.22894786381729 steps/sec)\n",
            "Step #9100\tEpoch   2 Batch 2849/3125   Loss: 0.820524 mae: 0.708501 (29.849141105892436 steps/sec)\n",
            "Step #9150\tEpoch   2 Batch 2899/3125   Loss: 0.841910 mae: 0.707396 (30.180372580420475 steps/sec)\n",
            "Step #9200\tEpoch   2 Batch 2949/3125   Loss: 0.852814 mae: 0.710644 (30.387852463396666 steps/sec)\n",
            "Step #9250\tEpoch   2 Batch 2999/3125   Loss: 0.750468 mae: 0.708482 (29.894566924408434 steps/sec)\n",
            "Step #9300\tEpoch   2 Batch 3049/3125   Loss: 0.768689 mae: 0.700227 (30.693246818898235 steps/sec)\n",
            "Step #9350\tEpoch   2 Batch 3099/3125   Loss: 0.866088 mae: 0.705167 (30.072304525905118 steps/sec)\n",
            "\n",
            "Train time for epoch #3 (9375 total steps): 103.71197557449341\n",
            "Model test set loss: 0.820054 mae: 0.714898\n",
            "best loss = 0.8200536966323853\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #9400\tEpoch   3 Batch   24/3125   Loss: 0.712096 mae: 0.714686 (55.91407826177144 steps/sec)\n",
            "Step #9450\tEpoch   3 Batch   74/3125   Loss: 0.867451 mae: 0.703509 (30.797783953858072 steps/sec)\n",
            "Step #9500\tEpoch   3 Batch  124/3125   Loss: 0.713659 mae: 0.713248 (30.79288652182774 steps/sec)\n",
            "Step #9550\tEpoch   3 Batch  174/3125   Loss: 0.745622 mae: 0.711804 (30.35981664984234 steps/sec)\n",
            "Step #9600\tEpoch   3 Batch  224/3125   Loss: 0.615304 mae: 0.705637 (30.760791092015477 steps/sec)\n",
            "Step #9650\tEpoch   3 Batch  274/3125   Loss: 0.801542 mae: 0.706528 (30.57533744882246 steps/sec)\n",
            "Step #9700\tEpoch   3 Batch  324/3125   Loss: 0.778202 mae: 0.710308 (30.864844737682553 steps/sec)\n",
            "Step #9750\tEpoch   3 Batch  374/3125   Loss: 0.806719 mae: 0.711444 (30.73117156114613 steps/sec)\n",
            "Step #9800\tEpoch   3 Batch  424/3125   Loss: 0.853285 mae: 0.714263 (30.732252384800873 steps/sec)\n",
            "Step #9850\tEpoch   3 Batch  474/3125   Loss: 0.787804 mae: 0.708031 (31.26332079462411 steps/sec)\n",
            "Step #9900\tEpoch   3 Batch  524/3125   Loss: 0.834111 mae: 0.702899 (30.67102221140616 steps/sec)\n",
            "Step #9950\tEpoch   3 Batch  574/3125   Loss: 0.856811 mae: 0.712780 (31.0100270316269 steps/sec)\n",
            "Step #10000\tEpoch   3 Batch  624/3125   Loss: 0.634637 mae: 0.709059 (30.042248057351465 steps/sec)\n",
            "Step #10050\tEpoch   3 Batch  674/3125   Loss: 0.836631 mae: 0.711331 (30.332569553745795 steps/sec)\n",
            "Step #10100\tEpoch   3 Batch  724/3125   Loss: 0.655321 mae: 0.708090 (30.678690125118145 steps/sec)\n",
            "Step #10150\tEpoch   3 Batch  774/3125   Loss: 0.757653 mae: 0.701788 (30.38933201641282 steps/sec)\n",
            "Step #10200\tEpoch   3 Batch  824/3125   Loss: 0.812988 mae: 0.706866 (30.819640137342468 steps/sec)\n",
            "Step #10250\tEpoch   3 Batch  874/3125   Loss: 0.736284 mae: 0.706082 (30.090198050032484 steps/sec)\n",
            "Step #10300\tEpoch   3 Batch  924/3125   Loss: 0.892129 mae: 0.711968 (30.411754298956186 steps/sec)\n",
            "Step #10350\tEpoch   3 Batch  974/3125   Loss: 0.759577 mae: 0.709129 (30.65003919033055 steps/sec)\n",
            "Step #10400\tEpoch   3 Batch 1024/3125   Loss: 0.803139 mae: 0.704850 (30.62891039816304 steps/sec)\n",
            "Step #10450\tEpoch   3 Batch 1074/3125   Loss: 0.841012 mae: 0.706226 (30.46915022367123 steps/sec)\n",
            "Step #10500\tEpoch   3 Batch 1124/3125   Loss: 0.804974 mae: 0.710335 (30.736355703342532 steps/sec)\n",
            "Step #10550\tEpoch   3 Batch 1174/3125   Loss: 0.734501 mae: 0.710490 (29.795316606637552 steps/sec)\n",
            "Step #10600\tEpoch   3 Batch 1224/3125   Loss: 0.760181 mae: 0.702124 (30.739977983738456 steps/sec)\n",
            "Step #10650\tEpoch   3 Batch 1274/3125   Loss: 0.898164 mae: 0.719206 (30.679462064310435 steps/sec)\n",
            "Step #10700\tEpoch   3 Batch 1324/3125   Loss: 0.724253 mae: 0.704091 (30.54330222424179 steps/sec)\n",
            "Step #10750\tEpoch   3 Batch 1374/3125   Loss: 0.755440 mae: 0.701853 (30.587975844105685 steps/sec)\n",
            "Step #10800\tEpoch   3 Batch 1424/3125   Loss: 0.797987 mae: 0.705679 (30.273881539106704 steps/sec)\n",
            "Step #10850\tEpoch   3 Batch 1474/3125   Loss: 0.692747 mae: 0.707663 (30.04828295894387 steps/sec)\n",
            "Step #10900\tEpoch   3 Batch 1524/3125   Loss: 0.710431 mae: 0.702623 (30.0869042503818 steps/sec)\n",
            "Step #10950\tEpoch   3 Batch 1574/3125   Loss: 0.775933 mae: 0.711990 (30.20225692581743 steps/sec)\n",
            "Step #11000\tEpoch   3 Batch 1624/3125   Loss: 0.857005 mae: 0.700073 (30.3054326318021 steps/sec)\n",
            "Step #11050\tEpoch   3 Batch 1674/3125   Loss: 0.734303 mae: 0.707448 (30.542990840679675 steps/sec)\n",
            "Step #11150\tEpoch   3 Batch 1774/3125   Loss: 0.760748 mae: 0.705120 (30.382503496197536 steps/sec)\n",
            "Step #11200\tEpoch   3 Batch 1824/3125   Loss: 0.709908 mae: 0.706008 (30.71411812640177 steps/sec)\n",
            "Step #11250\tEpoch   3 Batch 1874/3125   Loss: 0.889543 mae: 0.708179 (30.607484614906355 steps/sec)\n",
            "Step #11300\tEpoch   3 Batch 1924/3125   Loss: 0.973984 mae: 0.701435 (30.393837614176967 steps/sec)\n",
            "Step #11350\tEpoch   3 Batch 1974/3125   Loss: 0.783174 mae: 0.700865 (30.37405904451324 steps/sec)\n",
            "Step #11400\tEpoch   3 Batch 2024/3125   Loss: 0.855471 mae: 0.709907 (30.605050240743736 steps/sec)\n",
            "Step #11450\tEpoch   3 Batch 2074/3125   Loss: 0.854355 mae: 0.699948 (30.375022504420862 steps/sec)\n",
            "Step #11500\tEpoch   3 Batch 2124/3125   Loss: 0.637580 mae: 0.703948 (30.81444144828251 steps/sec)\n",
            "Step #11550\tEpoch   3 Batch 2174/3125   Loss: 0.736480 mae: 0.697379 (30.564366484644303 steps/sec)\n",
            "Step #11600\tEpoch   3 Batch 2224/3125   Loss: 0.703675 mae: 0.703339 (30.68964452494425 steps/sec)\n",
            "Step #11650\tEpoch   3 Batch 2274/3125   Loss: 0.827955 mae: 0.702852 (30.376971607918936 steps/sec)\n",
            "Step #11700\tEpoch   3 Batch 2324/3125   Loss: 0.670033 mae: 0.712158 (30.48000419741879 steps/sec)\n",
            "Step #11750\tEpoch   3 Batch 2374/3125   Loss: 0.752990 mae: 0.702936 (30.405744456863292 steps/sec)\n",
            "Step #11800\tEpoch   3 Batch 2424/3125   Loss: 0.745730 mae: 0.704020 (30.780173134364283 steps/sec)\n",
            "Step #11850\tEpoch   3 Batch 2474/3125   Loss: 0.857289 mae: 0.698497 (30.278051332087166 steps/sec)\n",
            "Step #11900\tEpoch   3 Batch 2524/3125   Loss: 0.766666 mae: 0.707505 (30.203274765502325 steps/sec)\n",
            "Step #11950\tEpoch   3 Batch 2574/3125   Loss: 0.867887 mae: 0.711233 (30.391758619105538 steps/sec)\n",
            "Step #12000\tEpoch   3 Batch 2624/3125   Loss: 0.947502 mae: 0.710332 (30.149531990115925 steps/sec)\n",
            "Step #12050\tEpoch   3 Batch 2674/3125   Loss: 0.801937 mae: 0.704346 (30.425533427432384 steps/sec)\n",
            "Step #12100\tEpoch   3 Batch 2724/3125   Loss: 0.768811 mae: 0.695953 (30.42158769449656 steps/sec)\n",
            "Step #12150\tEpoch   3 Batch 2774/3125   Loss: 0.740273 mae: 0.704561 (30.96206499200168 steps/sec)\n",
            "Step #12200\tEpoch   3 Batch 2824/3125   Loss: 0.794342 mae: 0.713438 (30.887842545574678 steps/sec)\n",
            "Step #12250\tEpoch   3 Batch 2874/3125   Loss: 0.828080 mae: 0.695256 (30.842811013306303 steps/sec)\n",
            "Step #12300\tEpoch   3 Batch 2924/3125   Loss: 0.806192 mae: 0.702757 (30.837409521848933 steps/sec)\n",
            "Step #12350\tEpoch   3 Batch 2974/3125   Loss: 0.729413 mae: 0.705207 (31.130732623608033 steps/sec)\n",
            "Step #12400\tEpoch   3 Batch 3024/3125   Loss: 0.710853 mae: 0.695867 (31.263539843589278 steps/sec)\n",
            "Step #12450\tEpoch   3 Batch 3074/3125   Loss: 0.782384 mae: 0.696797 (30.511595966708835 steps/sec)\n",
            "Step #12500\tEpoch   3 Batch 3124/3125   Loss: 0.865312 mae: 0.698175 (30.199495185135266 steps/sec)\n",
            "\n",
            "Train time for epoch #4 (12500 total steps): 102.52857899665833\n",
            "Model test set loss: 0.811045 mae: 0.710391\n",
            "best loss = 0.8110451698303223\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n",
            "Step #12550\tEpoch   4 Batch   49/3125   Loss: 0.899743 mae: 0.709960 (27.85829352489736 steps/sec)\n",
            "Step #12600\tEpoch   4 Batch   99/3125   Loss: 0.982372 mae: 0.695977 (29.4144246747957 steps/sec)\n",
            "Step #12650\tEpoch   4 Batch  149/3125   Loss: 0.766809 mae: 0.707141 (29.91658485494575 steps/sec)\n",
            "Step #12700\tEpoch   4 Batch  199/3125   Loss: 0.822756 mae: 0.703936 (29.526211166237417 steps/sec)\n",
            "Step #12750\tEpoch   4 Batch  249/3125   Loss: 0.834725 mae: 0.698990 (29.42482495111652 steps/sec)\n",
            "Step #12800\tEpoch   4 Batch  299/3125   Loss: 0.762333 mae: 0.703772 (29.525334054166684 steps/sec)\n",
            "Step #12850\tEpoch   4 Batch  349/3125   Loss: 0.864889 mae: 0.706066 (30.401178036371206 steps/sec)\n",
            "Step #12900\tEpoch   4 Batch  399/3125   Loss: 0.898732 mae: 0.706814 (29.936464210246378 steps/sec)\n",
            "Step #12950\tEpoch   4 Batch  449/3125   Loss: 0.803137 mae: 0.709696 (30.14334803787064 steps/sec)\n",
            "Step #13000\tEpoch   4 Batch  499/3125   Loss: 0.762351 mae: 0.696963 (29.82431240530365 steps/sec)\n",
            "Step #13050\tEpoch   4 Batch  549/3125   Loss: 0.771174 mae: 0.699617 (29.92181797324838 steps/sec)\n",
            "Step #13100\tEpoch   4 Batch  599/3125   Loss: 0.691662 mae: 0.710374 (29.627570150232117 steps/sec)\n",
            "Step #13150\tEpoch   4 Batch  649/3125   Loss: 0.764660 mae: 0.706269 (29.749748556950966 steps/sec)\n",
            "Step #13200\tEpoch   4 Batch  699/3125   Loss: 0.765256 mae: 0.705572 (30.247525673557227 steps/sec)\n",
            "Step #13250\tEpoch   4 Batch  749/3125   Loss: 0.752471 mae: 0.693328 (29.6232511965683 steps/sec)\n",
            "Step #13300\tEpoch   4 Batch  799/3125   Loss: 0.871666 mae: 0.703462 (29.571071464024328 steps/sec)\n",
            "Step #13350\tEpoch   4 Batch  849/3125   Loss: 0.911766 mae: 0.701899 (29.785397350584223 steps/sec)\n",
            "Step #13400\tEpoch   4 Batch  899/3125   Loss: 0.856701 mae: 0.700804 (30.081470841448432 steps/sec)\n",
            "Step #13450\tEpoch   4 Batch  949/3125   Loss: 0.679082 mae: 0.701536 (29.81935077300033 steps/sec)\n",
            "Step #13500\tEpoch   4 Batch  999/3125   Loss: 0.848126 mae: 0.701312 (29.96845038187356 steps/sec)\n",
            "Step #13550\tEpoch   4 Batch 1049/3125   Loss: 0.849540 mae: 0.707939 (29.81017822266319 steps/sec)\n",
            "Step #13600\tEpoch   4 Batch 1099/3125   Loss: 0.759863 mae: 0.700857 (29.514023563346832 steps/sec)\n",
            "Step #13650\tEpoch   4 Batch 1149/3125   Loss: 0.730783 mae: 0.706937 (29.480276636247627 steps/sec)\n",
            "Step #13700\tEpoch   4 Batch 1199/3125   Loss: 0.834156 mae: 0.698284 (29.89561100945471 steps/sec)\n",
            "Step #13750\tEpoch   4 Batch 1249/3125   Loss: 0.902825 mae: 0.706573 (29.850292488889863 steps/sec)\n",
            "Step #13800\tEpoch   4 Batch 1299/3125   Loss: 0.784375 mae: 0.709123 (30.181593095550703 steps/sec)\n",
            "Step #13850\tEpoch   4 Batch 1349/3125   Loss: 0.819666 mae: 0.688980 (29.960782364050125 steps/sec)\n",
            "Step #13900\tEpoch   4 Batch 1399/3125   Loss: 0.797686 mae: 0.702765 (30.012436297961678 steps/sec)\n",
            "Step #13950\tEpoch   4 Batch 1449/3125   Loss: 0.788184 mae: 0.702666 (30.053519188289922 steps/sec)\n",
            "Step #14000\tEpoch   4 Batch 1499/3125   Loss: 0.846333 mae: 0.700626 (30.246033722820744 steps/sec)\n",
            "Step #14050\tEpoch   4 Batch 1549/3125   Loss: 0.892001 mae: 0.702435 (29.795921962790903 steps/sec)\n",
            "Step #14100\tEpoch   4 Batch 1599/3125   Loss: 0.820663 mae: 0.699094 (29.664932211984922 steps/sec)\n",
            "Step #14150\tEpoch   4 Batch 1649/3125   Loss: 0.852924 mae: 0.702815 (29.175919993934297 steps/sec)\n",
            "Step #14200\tEpoch   4 Batch 1699/3125   Loss: 0.830140 mae: 0.697398 (29.825907263369132 steps/sec)\n",
            "Step #14250\tEpoch   4 Batch 1749/3125   Loss: 0.789909 mae: 0.698797 (29.850874587160444 steps/sec)\n",
            "Step #14300\tEpoch   4 Batch 1799/3125   Loss: 0.888091 mae: 0.702283 (30.70285407509488 steps/sec)\n",
            "Step #14350\tEpoch   4 Batch 1849/3125   Loss: 0.756484 mae: 0.702843 (30.295232147657824 steps/sec)\n",
            "Step #14400\tEpoch   4 Batch 1899/3125   Loss: 0.864119 mae: 0.699306 (30.197138319363788 steps/sec)\n",
            "Step #14450\tEpoch   4 Batch 1949/3125   Loss: 0.797574 mae: 0.694379 (30.1229248001287 steps/sec)\n",
            "Step #14500\tEpoch   4 Batch 1999/3125   Loss: 0.878040 mae: 0.702517 (29.826475684864096 steps/sec)\n",
            "Step #14550\tEpoch   4 Batch 2049/3125   Loss: 0.741098 mae: 0.706890 (30.263313594097507 steps/sec)\n",
            "Step #14600\tEpoch   4 Batch 2099/3125   Loss: 0.834145 mae: 0.691736 (29.425002479967684 steps/sec)\n",
            "Step #14650\tEpoch   4 Batch 2149/3125   Loss: 0.741923 mae: 0.691336 (29.95001694473987 steps/sec)\n",
            "Step #14700\tEpoch   4 Batch 2199/3125   Loss: 0.708842 mae: 0.696007 (29.48435502270426 steps/sec)\n",
            "Step #14750\tEpoch   4 Batch 2249/3125   Loss: 0.745521 mae: 0.696293 (29.755784718241514 steps/sec)\n",
            "Step #14800\tEpoch   4 Batch 2299/3125   Loss: 0.865498 mae: 0.707670 (29.392653322313407 steps/sec)\n",
            "Step #14850\tEpoch   4 Batch 2349/3125   Loss: 0.790625 mae: 0.702038 (29.727638306778616 steps/sec)\n",
            "Step #14900\tEpoch   4 Batch 2399/3125   Loss: 0.719087 mae: 0.701077 (30.241375595897217 steps/sec)\n",
            "Step #14950\tEpoch   4 Batch 2449/3125   Loss: 0.758933 mae: 0.692588 (29.940580037062524 steps/sec)\n",
            "Step #15000\tEpoch   4 Batch 2499/3125   Loss: 0.906340 mae: 0.702713 (30.331564915764055 steps/sec)\n",
            "Step #15050\tEpoch   4 Batch 2549/3125   Loss: 0.625534 mae: 0.705487 (29.788811638981795 steps/sec)\n",
            "Step #15100\tEpoch   4 Batch 2599/3125   Loss: 0.917844 mae: 0.703639 (30.041684292374793 steps/sec)\n",
            "Step #15150\tEpoch   4 Batch 2649/3125   Loss: 0.866283 mae: 0.701470 (30.058016222985938 steps/sec)\n",
            "Step #15200\tEpoch   4 Batch 2699/3125   Loss: 0.746625 mae: 0.695184 (29.933644047825844 steps/sec)\n",
            "Step #15250\tEpoch   4 Batch 2749/3125   Loss: 0.909098 mae: 0.696649 (29.875083621378355 steps/sec)\n",
            "Step #15300\tEpoch   4 Batch 2799/3125   Loss: 0.813048 mae: 0.705431 (30.03819458469369 steps/sec)\n",
            "Step #15350\tEpoch   4 Batch 2849/3125   Loss: 0.795310 mae: 0.696460 (30.275262601182714 steps/sec)\n",
            "Step #15400\tEpoch   4 Batch 2899/3125   Loss: 0.823115 mae: 0.695794 (30.329963774445886 steps/sec)\n",
            "Step #15450\tEpoch   4 Batch 2949/3125   Loss: 0.829516 mae: 0.699591 (30.35225896632938 steps/sec)\n",
            "Step #15500\tEpoch   4 Batch 2999/3125   Loss: 0.732186 mae: 0.697636 (29.92809073619337 steps/sec)\n",
            "Step #15550\tEpoch   4 Batch 3049/3125   Loss: 0.747025 mae: 0.688185 (29.86768024272959 steps/sec)\n",
            "Step #15600\tEpoch   4 Batch 3099/3125   Loss: 0.855183 mae: 0.695103 (29.897413825917283 steps/sec)\n",
            "\n",
            "Train time for epoch #5 (15625 total steps): 104.78588080406189\n",
            "Model test set loss: 0.804106 mae: 0.706976\n",
            "best loss = 0.8041055202484131\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: ./models/export/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeE5X1jF0mQQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0OkH0KsXOr"
      },
      "source": [
        "train_step:优化的是loss值，解决的其实是一个极小化loss问题。这里的含义是使用Adam下降算法（在tensorflow中已经写好了各种优化算法，这里只需要声明和调用即可），使loss值最小，也就是使网络的输出与样本的输出接近。这里的Loss损失函数，可以是均方误差，自定义函数或者交叉熵。train_step在后面调用sess.run()会话计算时，会喂入输入数据。每喂入一组，就计算一次会话，更新一轮参数，所以train_step的含义我理解应该是每次喂入训练数据后执行的结果，可以翻译成“训练步骤”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koOz8Vbwfzp3"
      },
      "source": [
        "Tensorflow的Checkpoint机制将可追踪变量以二进制的方式储存成一个.ckpt文件，储存了变量的名称及对应张量的值。\n",
        "\n",
        "  Checkpoint 只保存模型的参数，不保存模型的计算过程，因此一般用于在具有模型源代码的时候恢复之前训练好的模型参数。如果需要导出模型（无需源代码也能运行模型）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAY1Qa204rJy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knIa8av81ZmU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}